{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How does the field data circuit breaker in Elasticsearch estimate memory requirements, and what can happen if limits are exceeded?",
          "timestamp": "2025-10-03T04:07:20.998285+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\": \"field data circuit breaker memory requirements\"}",
          "tool_call_id": "call_M4WdqyAkPZjxctcnpQhdDhJ7",
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"field data circuit breaker limits exceeded consequences\"}",
          "tool_call_id": "call_Z1VF1LMjdShn9gzq1x7lfgST",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 223,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 53,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:07:21+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMRMfxho6X9fMJx9j1kT1UijRBEK2",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "CSV",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/csv-processor.html"
              ],
              "content": "# CSV processor [csv-processor]\n\n\nExtracts fields from CSV line out of a single text field within a document. Any empty field in CSV will be skipped.\n\n$$$csv-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `field` | yes | - | The field to extract data from |\n| `target_fields` | yes | - | The array of fields to assign extracted values to |\n| `separator` | no | , | Separator used in CSV, has to be single character string |\n| `quote` | no | \" | Quote used in CSV, has to be single character string |\n| `ignore_missing` | no | `false` | If `true` and `field` does not exist, the processor quietly exits without modifying the document |\n| `trim` | no | `false` | Trim whitespaces in unquoted fields |\n| `empty_value` | no | - | Value used to fill empty fields, empty fields will be skipped if this is not provided.                                             Empty field is one with no value (2 consecutive separators) or empty quotes (`\"\"`) |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n```js\n{\n  \"csv\": {\n    \"field\": \"my_field\",\n    \"target_fields\": [\"field1\", \"field2\"]\n  }\n}\n```\n\nIf the `trim` option is enabled then any whitespace in the beginning and in the end of each unquoted field will be trimmed. For example with configuration above, a value of `A, B` will result in field `field2` having value `{{nbsp}}B` (with space at the beginning). If `trim` is enabled `A, B` will result in field `field2` having value `B` (no whitespace). Quoted fields will be left untouched.",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/csv-processor.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-fielddata.html"
              ],
              "applies_to": {
                "deployment": {
                  "self": null
                }
              },
              "content": "# Field data cache settings [modules-fielddata]\n\nThe field data cache contains [field data](/reference/elasticsearch/mapping-reference/text.md#fielddata-mapping-param) and [global ordinals](/reference/elasticsearch/mapping-reference/eager-global-ordinals.md), which are both used to support aggregations on certain field types. Since these are on-heap data structures, it is important to monitor the cache\u2019s use.\n\nThe entries in the cache are expensive to build, so the default behavior is to keep the cache loaded in memory. The default cache size is unlimited, causing the cache to grow until it reaches the limit set by the [field data circuit breaker](/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md#fielddata-circuit-breaker). This behavior can be configured.\n\nIf the cache size limit is set, the cache will begin clearing the least-recently-updated entries in the cache. This setting can automatically avoid the circuit breaker limit, at the cost of rebuilding the cache as needed.\n\nIf the circuit breaker limit is reached, further requests that increase the cache size will be prevented. In this case you should manually [clear the cache](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-clear-cache).\n\n::::{tip}\nYou can monitor memory usage for field data as well as the field data circuit breaker using the [nodes stats API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-nodes-stats) or the [cat fielddata API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-cat-fielddata).\n::::\n\n\n`indices.fielddata.cache.size`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) The max size of the field data cache, eg `38%` of node heap space, or an absolute value, eg `12GB`. Defaults to unbounded. If you choose to set it, it should be smaller than [Field data circuit breaker](/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md#fielddata-circuit-breaker) limit.",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/configuration-reference/field-data-cache-settings.md"
            },
            {
              "navigation_title": "Append",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/append-processor.html"
              ],
              "content": "# Append processor [append-processor]\n\n\nAppends one or more values to an existing array if the field already exists and it is an array. Converts a scalar to an array and appends one or more values to it if the field exists and it is a scalar. Creates an array containing the provided values if the field doesn\u2019t exist. Accepts a single value or an array of values.\n\n$$$append-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `field` | yes | - | The field to be appended to. Supports [template snippets](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#template-snippets). |\n| `value` | yes* | - | The value to be appended. Supports [template snippets](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#template-snippets). May specify only one of `value` or `copy_from`. |\n| `copy_from` {applies_to}`stack: ga 9.2` | no | - | The origin field which will be appended to `field`, cannot set `value` simultaneously. |\n| `allow_duplicates` | no | true | If `false`, the processor does not appendvalues already present in the field. |\n| `ignore_empty_values` {applies_to}`stack: ga 9.2` | no | false | If `true`, the processor does not append values that resolve to `null` or an empty string.\n| `media_type` | no | `application/json` | The media type for encoding `value`. Applies only when `value` is a [template snippet](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#template-snippets). Must be one of `application/json`, `text/plain`, or`application/x-www-form-urlencoded`. |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n## Examples [append-processor-examples]\n\n### Simple example [append-processor-simple-example]\n\nHere is an `append` processor definition that adds the string `\"production\"` as well as the values of the `app` and `owner` fields to the `tags` field:\n\n```js\n{\n  \"append\": {\n    \"field\": \"tags\",\n    \"value\": [\"production\", \"{{{app}}}\", \"{{{owner}}}\"]\n  }\n}\n```\n\n### Example using `allow_duplicates` and `ignore_empty_values` [append-processor-example-using-allow-duplicates-and-ignore-empty-values]\n\n```{applies_to}\nstack: ga 9.2\n```\n\nBy using `allow_duplicates` and `ignore_empty_values`, it is possible to only append the `host.name` to the `related.hosts` if the `host.name` is not empty and if the value is not already present in `related.hosts`:\n\n```js\n{\n  \"append\": {\n    \"field\": \"related.hosts\",\n    \"copy_from\": \"host.name\",\n    \"allow_duplicates\": false,\n    \"ignore_empty_values\": true\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/append-processor.md"
            },
            {
              "navigation_title": "{{infer-cap}}",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-processor.html"
              ],
              "content": "# {{infer-cap}} processor [inference-processor]\n\nUses a pre-trained {{dfanalytics}} model or a model deployed for natural language processing tasks to infer against the data that is being ingested in the pipeline.\n\n$$$inference-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `model_id`  | yes | - | (String) An inference ID, a model deployment ID, a trained model ID or an alias. |\n| `input_output` | no | - | (List) Input fields for {{infer}} and output (destination) fields for the {{infer}} results. This option is incompatible with the `target_field` and `field_map` options. |\n| `target_field` | no | `ml.inference.<processor_tag>` | (String) Field added to incoming documents to contain results objects. |\n| `field_map` | no | If defined the model\u2019s default field map | (Object) Maps the document field names to the known field names of the model. This mapping takes precedence over any default mappings provided in the model configuration. |\n| `inference_config` | no | The default settings defined in the model | (Object) Contains the inference type and its options. |\n| `ignore_missing` | no | `false` | (Boolean) If `true` and any of the input fields defined in `input_ouput` are missing then those missing fields are quietly ignored, otherwise a missing field causes a failure. Only applies when using `input_output` configurations to explicitly list the input fields. |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n::::{important}\n\n* You cannot use the `input_output` field with the `target_field` and `field_map` fields. For NLP models, use the `input_output` option. For {{dfanalytics}} models, use the `target_field` and `field_map` option.\n* Each {{infer}} input field must be single strings, not arrays of strings.\n* The `input_field` is processed as is and ignores any [index mapping](docs-content://manage-data/data-store/mapping.md)'s [analyzers](docs-content://manage-data/data-store/text-analysis.md) at time of {{infer}} run.\n\n::::\n\n## Configuring input and output fields [inference-input-output-example]\n\nSelect the `content` field for inference and write the result to `content_embedding`.\n\n::::{important}\nIf the specified `output_field` already exists in the ingest document, it won\u2019t be overwritten. The {{infer}} results will be appended to the existing fields within `output_field`, which could lead to duplicate fields and potential errors. To avoid this, use an unique `output_field` field name that does not clash with any existing fields.\n::::\n\n\n```js\n{\n  \"inference\": {\n    \"model_id\": \"model_deployment_for_inference\",\n    \"input_output\": [\n        {\n            \"input_field\": \"content\",\n            \"output_field\": \"content_embedding\"\n        }\n    ]\n  }\n}\n```\n\n## Configuring multiple inputs [_configuring_multiple_inputs]\n\nThe `content` and `title` fields will be read from the incoming document and sent to the model for the inference. The inference output is written to `content_embedding` and `title_embedding` respectively.\n\n```js\n{\n  \"inference\": {\n    \"model_id\": \"model_deployment_for_inference\",\n    \"input_output\": [\n        {\n            \"input_field\": \"content\",\n            \"output_field\": \"content_embedding\"\n        },\n        {\n            \"input_field\": \"title\",\n            \"output_field\": \"title_embedding\"\n        }\n    ]\n  }\n}\n```\n\nSelecting the input fields with `input_output` is incompatible with the `target_field` and `field_map` options.\n\n{{dfanalytics-cap}} models must use the `target_field` to specify the root location results are written to and optionally a `field_map` to map field names in the input document to the model input fields.\n\n```js\n{\n  \"inference\": {\n    \"model_id\": \"model_deployment_for_inference\",\n    \"target_field\": \"FlightDelayMin_prediction_infer\",\n    \"field_map\": {\n      \"your_field\": \"my_field\"\n    },\n    \"inference_config\": { \"regression\": {} }\n  }\n}\n```\n\n## {{classification-cap}} configuration options [inference-processor-classification-opt]\n\nClassification configuration for inference.\n\n`num_top_classes`\n:   (Optional, integer) Specifies the number of top class predictions to return. Defaults to 0.\n\n`num_top_feature_importance_values`\n:   (Optional, integer) Specifies the maximum number of [{{feat-imp}}](docs-content://explore-analyze/machine-learning/data-frame-analytics/ml-feature-importance.md) values per document. Defaults to 0 which means no {{feat-imp}} calculation occurs.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`top_classes_results_field`\n:   (Optional, string) Specifies the field to which the top classes are written. Defaults to `top_classes`.\n\n`prediction_field_type`\n:   (Optional, string) Specifies the type of the predicted field to write. Valid values are: `string`, `number`, `boolean`. When `boolean` is provided `1.0` is transformed to `true` and `0.0` to `false`.\n\n## Fill mask configuration options [inference-processor-fill-mask-opt]\n\n`num_top_classes`\n:   (Optional, integer) Specifies the number of top class predictions to return. Defaults to 0.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    ::::{dropdown} Properties of deberta_v2\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## NER configuration options [inference-processor-ner-opt]\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    ::::{dropdown} Properties of deberta_v2\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## {{regression-cap}} configuration options [inference-processor-regression-opt]\n\nRegression configuration for inference.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`num_top_feature_importance_values`\n:   (Optional, integer) Specifies the maximum number of [{{feat-imp}}](docs-content://explore-analyze/machine-learning/data-frame-analytics/ml-feature-importance.md) values per document. By default, it is zero and no {{feat-imp}} calculation occurs.\n\n## Text classification configuration options [inference-processor-text-classification-opt]\n\n`classification_labels`\n:   (Optional, string) An array of classification labels.\n\n`num_top_classes`\n:   (Optional, integer) Specifies the number of top class predictions to return. Defaults to 0.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of deberta_v2\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    :::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## Text embedding configuration options [inference-processor-text-embedding-opt]\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    ::::{dropdown} Properties of deberta_v2\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## Text expansion configuration options [inference-processor-text-expansion-opt]\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of deberta_v2\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    :::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## Text similarity configuration options [inference-processor-text-similarity-opt]\n\n`text_similarity`\n:   (Object, optional) Text similarity takes an input sequence and compares it with another input sequence. This is commonly referred to as cross-encoding. This task is useful for ranking document text when comparing it to another provided text input.\n\n::::{dropdown} Properties of text_similarity inference\n`span_score_combination_function`\n:   (Optional, string) Identifies how to combine the resulting similarity score when a provided text passage is longer than `max_sequence_length` and must be automatically separated for multiple calls. This only is applicable when `truncate` is `none` and `span` is a non-negative number. The default value is `max`. Available options are:\n* `max`: The maximum score from all the spans is returned.\n* `mean`: The mean score over all the spans is returned.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n* `bert`: Use for BERT-style models\n* `deberta_v2`: Use for DeBERTa v2 and v3-style models\n* `mpnet`: Use for MPNet-style models\n* `roberta`: Use for RoBERTa-style and BART-style models\n* {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n* {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\nRefer to [Properties of `tokenizaton`](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-ml-put-trained-model) to review the properties of the `tokenization` object.\n::::\n\n## Zero shot classification configuration options [inference-processor-zero-shot-opt]\n\n`labels`\n:   (Optional, array) The labels to classify. Can be set at creation for default labels, and then updated during inference.\n\n`multi_label`\n:   (Optional, boolean) Indicates if more than one `true` label is possible given the input. This is useful when labeling text that could pertain to more than one of the input labels. Defaults to `false`.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n    ::::{dropdown} Properties of tokenization\n    `bert`\n    :   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n        :::::{dropdown} Properties of bert\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::{note}\n        For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n        ::::\n        :::::\n    `deberta_v2`\n    :   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n        ::::{dropdown} Properties of deberta_v2\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::\n    `roberta`\n    :   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n        :::::{dropdown} Properties of roberta\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::{note}\n        For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n        ::::\n        :::::\n    `mpnet`\n    :   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n        :::::{dropdown} Properties of mpnet\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::{note}\n        For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n        ::::\n        :::::\n    ::::\n\n## {{infer-cap}} processor examples [inference-processor-config-example]\n\n```js\n\"inference\":{\n  \"model_id\": \"my_model_id\",\n  \"field_map\": {\n    \"original_fieldname\": \"expected_fieldname\"\n  },\n  \"inference_config\": {\n    \"regression\": {\n      \"results_field\": \"my_regression\"\n    }\n  }\n}\n```\n\nThis configuration specifies a `regression` inference and the results are written to the `my_regression` field contained in the `target_field` results object. The `field_map` configuration maps the field `original_fieldname` from the source document to the field expected by the model.\n\n```js\n\"inference\":{\n  \"model_id\":\"my_model_id\"\n  \"inference_config\": {\n    \"classification\": {\n      \"num_top_classes\": 2,\n      \"results_field\": \"prediction\",\n      \"top_classes_results_field\": \"probabilities\"\n    }\n  }\n}\n```\n\nThis configuration specifies a `classification` inference. The number of categories for which the predicted probabilities are reported is 2 (`num_top_classes`). The result is written to the `prediction` field and the top classes to the `probabilities` field. Both fields are contained in the `target_field` results object.\n\nFor an example that uses {{nlp}} trained models, refer to [Add NLP inference to ingest pipelines](docs-content://explore-analyze/machine-learning/nlp/ml-nlp-inference.md).\n\n\n### {{feat-imp-cap}} object mapping [inference-processor-feature-importance]\n\nTo get the full benefit of aggregating and searching for [{{feat-imp}}](docs-content://explore-analyze/machine-learning/data-frame-analytics/ml-feature-importance.md), update your index mapping of the {{feat-imp}} result field as you can see below:\n\n```js\n\"ml.inference.feature_importance\": {\n  \"type\": \"nested\",\n  \"dynamic\": true,\n  \"properties\": {\n    \"feature_name\": {\n      \"type\": \"keyword\"\n    },\n    \"importance\": {\n      \"type\": \"double\"\n    }\n  }\n}\n```\n\nThe mapping field name for {{feat-imp}} (in the example above, it is `ml.inference.feature_importance`) is compounded as follows:\n\n`<ml.inference.target_field>`.`<inference.tag>`.`feature_importance`\n\n* `<ml.inference.target_field>`: defaults to `ml.inference`.\n* `<inference.tag>`: if is not provided in the processor definition, then it is not part of the field path.\n\nFor example, if you provide a tag `foo` in the definition as you can see below:\n\n```js\n{\n  \"tag\": \"foo\",\n  ...\n}\n```\n\nThen, the {{feat-imp}} value is written to the `ml.inference.foo.feature_importance` field.\n\nYou can also specify the target field as follows:\n\n```js\n{\n  \"tag\": \"foo\",\n  \"target_field\": \"my_field\"\n}\n```\n\nIn this case, {{feat-imp}} is exposed in the `my_field.foo.feature_importance` field.\n\n\n### {{infer-cap}} processor examples [inference-processor-examples]\n\nThe following example uses an [{{infer}} endpoint](https://www.elastic.co/docs/api/doc/elasticsearch/group/endpoint-inference) in an {{infer}} processor named `query_helper_pipeline` to perform a chat completion task. The processor generates an {{es}} query from natural language input using a prompt designed for a completion task type. Refer to [this list](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put) for the {{infer}} service you use and check the corresponding examples of setting up an endpoint with the chat completion task type.\n\n```console\nPUT _ingest/pipeline/query_helper_pipeline\n{\n  \"processors\": [\n    {\n      \"script\": {\n        \"source\": \"ctx.prompt = 'Please generate an elasticsearch search query on index `articles_index` for the following natural language query. Dates are in the field `@timestamp`, document types are in the field `type` (options are `news`, `publication`), categories in the field `category` and can be multiple (options are `medicine`, `pharmaceuticals`, `technology`), and document names are in the field `title` which should use a fuzzy match. Ignore fields which cannot be determined from the natural language query context: ' + ctx.content\" <1>\n      }\n    },\n    {\n      \"inference\": {\n        \"model_id\": \"openai_chat_completions\", <2>\n        \"input_output\": {\n          \"input_field\": \"prompt\",\n          \"output_field\": \"query\"\n        }\n      }\n    },\n    {\n      \"remove\": {\n        \"field\": \"prompt\"\n      }\n    }\n  ]\n}\n```\n\n1. The `prompt` field contains the prompt used for the completion task, created with [Painless](docs-content://explore-analyze/scripting/modules-scripting-painless.md). `+ ctx.content` appends the natural language input to the prompt.\n2. The ID of the pre-configured {{infer}} endpoint, which utilizes the [`openai` service](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put) with the `completion` task type.\n\n\nThe following API request will simulate running a document through the ingest pipeline created previously:\n\n```console\nPOST _ingest/pipeline/query_helper_pipeline/_simulate\n{\n  \"docs\": [\n    {\n      \"_source\": {\n        \"content\": \"artificial intelligence in medicine articles published in the last 12 months\" <1>\n      }\n    }\n  ]\n}\n```\n\n1. The natural language query used to generate an {{es}} query within the prompt created by the {{infer}} processor.\n\n\n\n### Further readings [infer-proc-readings]\n\n* [Which job is the best for you? Using LLMs and semantic_text to match resumes to jobs](https://www.elastic.co/search-labs/blog/openwebcrawler-llms-semantic-text-resume-job-search)",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/inference-processor.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/circuit-breaker.html"
              ],
              "applies_to": {
                "deployment": {
                  "ess": null,
                  "self": null
                }
              },
              "content": "# Circuit breaker settings [circuit-breaker]\n\n$$$circuit-breaker-description$$$\n{{es}} contains multiple circuit breakers used to prevent operations from using an excessive amount of memory. Each breaker tracks the memory used by certain operations and specifies a limit for how much memory it may track. Additionally, there is a parent-level breaker that specifies the total amount of memory that may be tracked across all breakers.\n\nWhen a circuit breaker reaches its limit, {{es}} will reject further operations. See [Circuit breaker errors](docs-content://troubleshoot/elasticsearch/circuit-breaker-errors.md) for information about errors raised by circuit breakers.\n\nCircuit breakers do not track all memory usage in {{es}} and therefore provide only incomplete protection against excessive memory usage. If {{es}} uses too much memory then it may suffer from performance issues and nodes may even fail with an `OutOfMemoryError`. See [High JVM memory pressure](docs-content://troubleshoot/elasticsearch/high-jvm-memory-pressure.md) for help with troubleshooting high heap usage.\n\nExcept where noted otherwise, these settings can be dynamically updated on a live cluster with the [cluster-update-settings](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-cluster-put-settings) API.\n\nFor information about circuit breaker errors, see [Circuit breaker errors](docs-content://troubleshoot/elasticsearch/circuit-breaker-errors.md).\n\n\n### Parent circuit breaker [parent-circuit-breaker]\n\nThe parent-level breaker can be configured with the following settings:\n\n`indices.breaker.total.use_real_memory`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Determines whether the parent breaker should take real memory usage into account (`true`) or only consider the amount that is reserved by child circuit breakers (`false`). Defaults to `true`.\n\n$$$indices-breaker-total-limit$$$\n\n`indices.breaker.total.limit` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Starting limit for overall parent breaker. Defaults to 70% of JVM heap if `indices.breaker.total.use_real_memory` is `false`. If `indices.breaker.total.use_real_memory` is `true`, defaults to 95% of the JVM heap.\n\n\n### Field data circuit breaker [fielddata-circuit-breaker]\n\nThe field data circuit breaker estimates the heap memory required to load a field into the [field data cache](/reference/elasticsearch/configuration-reference/field-data-cache-settings.md). If loading the field would cause the cache to exceed a predefined memory limit, the circuit breaker stops the operation and returns an error.\n\n$$$fielddata-circuit-breaker-limit$$$\n\n`indices.breaker.fielddata.limit` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for fielddata breaker. Defaults to 40% of JVM heap.\n\n$$$fielddata-circuit-breaker-overhead$$$\n\n`indices.breaker.fielddata.overhead` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all field data estimations are multiplied with to determine a final estimation. Defaults to `1.03`.\n\n\n### Request circuit breaker [request-circuit-breaker]\n\nThe request circuit breaker allows Elasticsearch to prevent per-request data structures (for example, memory used for calculating aggregations during a request) from exceeding a certain amount of memory.\n\n$$$request-breaker-limit$$$\n\n`indices.breaker.request.limit` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for request breaker, defaults to 60% of JVM heap.\n\n$$$request-breaker-overhead$$$\n\n`indices.breaker.request.overhead` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all request estimations are multiplied with to determine a final estimation. Defaults to `1`.\n\n\n### In flight requests circuit breaker [in-flight-circuit-breaker]\n\nThe in flight requests circuit breaker allows Elasticsearch to limit the memory usage of all currently active incoming requests on transport or HTTP level from exceeding a certain amount of memory on a node. The memory usage is based on the content length of the request itself. This circuit breaker also considers that memory is not only needed for representing the raw request but also as a structured object which is reflected by default overhead.\n\n`network.breaker.inflight_requests.limit`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for in flight requests breaker, defaults to 100% of JVM heap. This means that it is bound by the limit configured for the parent circuit breaker.\n\n`network.breaker.inflight_requests.overhead`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all in flight requests estimations are multiplied with to determine a final estimation. Defaults to 2.\n\n\n### Script compilation circuit breaker [script-compilation-circuit-breaker]\n\nSlightly different than the previous memory-based circuit breaker, the script compilation circuit breaker limits the number of inline script compilations within a period of time.\n\nSee the \"prefer-parameters\" section of the [scripting](docs-content://explore-analyze/scripting/modules-scripting-using.md) documentation for more information.\n\n`script.max_compilations_rate`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for the number of unique dynamic scripts within a certain interval that are allowed to be compiled. Defaults to `150/5m`, meaning 150 every 5 minutes.\n\nIf the cluster regularly hits the given `max_compilation_rate`, it\u2019s possible the script cache is undersized, use [Nodes Stats](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-nodes-stats) to inspect the number of recent cache evictions, `script.cache_evictions_history` and compilations `script.compilations_history`.  If there are a large number of recent cache evictions or compilations, the script cache may be undersized, consider doubling the size of the script cache via the setting `script.cache.max_size`.\n\n\n### Regex circuit breaker [regex-circuit-breaker]\n\nPoorly written regular expressions can degrade cluster stability and performance. The regex circuit breaker limits the use and complexity of [regex in Painless scripts](/reference/scripting-languages/painless/painless-regexes.md).\n\n$$$script-painless-regex-enabled$$$\n\n`script.painless.regex.enabled` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Enables regex in Painless scripts. Accepts:\n\n    `limited` (Default)\n    :   Enables regex but limits complexity using the [`script.painless.regex.limit-factor`](#script-painless-regex-limit-factor) cluster setting.\n\n    `true`\n    :   Enables regex with no complexity limits. Disables the regex circuit breaker.\n\n    `false`\n    :   Disables regex. Any Painless script containing a regular expression returns an error.\n\n\n$$$script-painless-regex-limit-factor$$$\n\n`script.painless.regex.limit-factor`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Limits the number of characters a regular expression in a Painless script can consider. {{es}} calculates this limit by multiplying the setting value by the script input\u2019s character length.\n\n    For example, the input `foobarbaz` has a character length of `9`. If `script.painless.regex.limit-factor` is `6`, a regular expression on `foobarbaz` can consider up to 54 (9 * 6) characters. If the expression exceeds this limit, it triggers the regex circuit breaker and returns an error.\n\n    {{es}} only applies this limit if [`script.painless.regex.enabled`](#script-painless-regex-enabled) is `limited`.\n\n\n\n## EQL circuit breaker [circuit-breakers-page-eql]\n\nWhen a [sequence](/reference/query-languages/eql/eql-syntax.md#eql-sequences) query is executed, the node handling the query needs to keep some structures in memory, which are needed by the algorithm implementing the sequence matching. When large amounts of data need to be processed, and/or a large amount of matched sequences is requested by the user (by setting the [size](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-eql-search) query param), the memory occupied by those structures could potentially exceed the available memory of the JVM. This would cause an `OutOfMemory` exception which would bring down the node.\n\nTo prevent this from happening, a special circuit breaker is used, which limits the memory allocation during the execution of a [sequence](/reference/query-languages/eql/eql-syntax.md#eql-sequences) query. When the breaker is triggered, an `org.elasticsearch.common.breaker.CircuitBreakingException` is thrown and a descriptive error message including `circuit_breaking_exception` is returned to the user.\n\nThis circuit breaker can be configured using the following settings:\n\n`breaker.eql_sequence.limit`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) The limit for circuit breaker used to restrict the memory utilisation during the execution of an EQL sequence query. This value is defined as a percentage of the JVM heap. Defaults to `50%`. If the [parent circuit breaker](#parent-circuit-breaker) is set to a value less than `50%`, this setting uses that value as its default instead.\n\n`breaker.eql_sequence.overhead`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that sequence query memory estimates are multiplied by to determine a final estimate. Defaults to `1`.\n\n`breaker.eql_sequence.type`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Circuit breaker type. Valid values are:\n\n    `memory` (Default)\n    :   The breaker limits memory usage for EQL sequence queries.\n\n    `noop`\n    :   Disables the breaker.\n\n\n\n### {{ml-cap}} circuit breaker [circuit-breakers-page-model-inference]\n\n`breaker.model_inference.limit`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) The limit for the trained model circuit breaker. This value is defined as a percentage of the JVM heap. Defaults to `50%`. If the [parent circuit breaker](#parent-circuit-breaker) is set to a value less than `50%`, this setting uses that value as its default instead.\n\n`breaker.model_inference.overhead`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all trained model estimations are multiplied by to determine a final estimation. Defaults to `1`.\n\n`breaker.model_inference.type`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) The underlying type of the circuit breaker. There are two valid options: `noop` and `memory`. `noop` means the circuit breaker does nothing to prevent too much memory usage. `memory` means the circuit breaker tracks the memory used by trained models and can potentially break and prevent `OutOfMemory` errors. The default value is `memory`.",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md"
            }
          ],
          "tool_call_id": "call_M4WdqyAkPZjxctcnpQhdDhJ7",
          "metadata": null,
          "timestamp": "2025-10-03T04:07:23.538360+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-fielddata.html"
              ],
              "applies_to": {
                "deployment": {
                  "self": null
                }
              },
              "content": "# Field data cache settings [modules-fielddata]\n\nThe field data cache contains [field data](/reference/elasticsearch/mapping-reference/text.md#fielddata-mapping-param) and [global ordinals](/reference/elasticsearch/mapping-reference/eager-global-ordinals.md), which are both used to support aggregations on certain field types. Since these are on-heap data structures, it is important to monitor the cache\u2019s use.\n\nThe entries in the cache are expensive to build, so the default behavior is to keep the cache loaded in memory. The default cache size is unlimited, causing the cache to grow until it reaches the limit set by the [field data circuit breaker](/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md#fielddata-circuit-breaker). This behavior can be configured.\n\nIf the cache size limit is set, the cache will begin clearing the least-recently-updated entries in the cache. This setting can automatically avoid the circuit breaker limit, at the cost of rebuilding the cache as needed.\n\nIf the circuit breaker limit is reached, further requests that increase the cache size will be prevented. In this case you should manually [clear the cache](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-clear-cache).\n\n::::{tip}\nYou can monitor memory usage for field data as well as the field data circuit breaker using the [nodes stats API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-nodes-stats) or the [cat fielddata API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-cat-fielddata).\n::::\n\n\n`indices.fielddata.cache.size`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) The max size of the field data cache, eg `38%` of node heap space, or an absolute value, eg `12GB`. Defaults to unbounded. If you choose to set it, it should be smaller than [Field data circuit breaker](/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md#fielddata-circuit-breaker) limit.",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/configuration-reference/field-data-cache-settings.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/circuit-breaker.html"
              ],
              "applies_to": {
                "deployment": {
                  "ess": null,
                  "self": null
                }
              },
              "content": "# Circuit breaker settings [circuit-breaker]\n\n$$$circuit-breaker-description$$$\n{{es}} contains multiple circuit breakers used to prevent operations from using an excessive amount of memory. Each breaker tracks the memory used by certain operations and specifies a limit for how much memory it may track. Additionally, there is a parent-level breaker that specifies the total amount of memory that may be tracked across all breakers.\n\nWhen a circuit breaker reaches its limit, {{es}} will reject further operations. See [Circuit breaker errors](docs-content://troubleshoot/elasticsearch/circuit-breaker-errors.md) for information about errors raised by circuit breakers.\n\nCircuit breakers do not track all memory usage in {{es}} and therefore provide only incomplete protection against excessive memory usage. If {{es}} uses too much memory then it may suffer from performance issues and nodes may even fail with an `OutOfMemoryError`. See [High JVM memory pressure](docs-content://troubleshoot/elasticsearch/high-jvm-memory-pressure.md) for help with troubleshooting high heap usage.\n\nExcept where noted otherwise, these settings can be dynamically updated on a live cluster with the [cluster-update-settings](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-cluster-put-settings) API.\n\nFor information about circuit breaker errors, see [Circuit breaker errors](docs-content://troubleshoot/elasticsearch/circuit-breaker-errors.md).\n\n\n### Parent circuit breaker [parent-circuit-breaker]\n\nThe parent-level breaker can be configured with the following settings:\n\n`indices.breaker.total.use_real_memory`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Determines whether the parent breaker should take real memory usage into account (`true`) or only consider the amount that is reserved by child circuit breakers (`false`). Defaults to `true`.\n\n$$$indices-breaker-total-limit$$$\n\n`indices.breaker.total.limit` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Starting limit for overall parent breaker. Defaults to 70% of JVM heap if `indices.breaker.total.use_real_memory` is `false`. If `indices.breaker.total.use_real_memory` is `true`, defaults to 95% of the JVM heap.\n\n\n### Field data circuit breaker [fielddata-circuit-breaker]\n\nThe field data circuit breaker estimates the heap memory required to load a field into the [field data cache](/reference/elasticsearch/configuration-reference/field-data-cache-settings.md). If loading the field would cause the cache to exceed a predefined memory limit, the circuit breaker stops the operation and returns an error.\n\n$$$fielddata-circuit-breaker-limit$$$\n\n`indices.breaker.fielddata.limit` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for fielddata breaker. Defaults to 40% of JVM heap.\n\n$$$fielddata-circuit-breaker-overhead$$$\n\n`indices.breaker.fielddata.overhead` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all field data estimations are multiplied with to determine a final estimation. Defaults to `1.03`.\n\n\n### Request circuit breaker [request-circuit-breaker]\n\nThe request circuit breaker allows Elasticsearch to prevent per-request data structures (for example, memory used for calculating aggregations during a request) from exceeding a certain amount of memory.\n\n$$$request-breaker-limit$$$\n\n`indices.breaker.request.limit` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for request breaker, defaults to 60% of JVM heap.\n\n$$$request-breaker-overhead$$$\n\n`indices.breaker.request.overhead` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all request estimations are multiplied with to determine a final estimation. Defaults to `1`.\n\n\n### In flight requests circuit breaker [in-flight-circuit-breaker]\n\nThe in flight requests circuit breaker allows Elasticsearch to limit the memory usage of all currently active incoming requests on transport or HTTP level from exceeding a certain amount of memory on a node. The memory usage is based on the content length of the request itself. This circuit breaker also considers that memory is not only needed for representing the raw request but also as a structured object which is reflected by default overhead.\n\n`network.breaker.inflight_requests.limit`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for in flight requests breaker, defaults to 100% of JVM heap. This means that it is bound by the limit configured for the parent circuit breaker.\n\n`network.breaker.inflight_requests.overhead`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all in flight requests estimations are multiplied with to determine a final estimation. Defaults to 2.\n\n\n### Script compilation circuit breaker [script-compilation-circuit-breaker]\n\nSlightly different than the previous memory-based circuit breaker, the script compilation circuit breaker limits the number of inline script compilations within a period of time.\n\nSee the \"prefer-parameters\" section of the [scripting](docs-content://explore-analyze/scripting/modules-scripting-using.md) documentation for more information.\n\n`script.max_compilations_rate`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) Limit for the number of unique dynamic scripts within a certain interval that are allowed to be compiled. Defaults to `150/5m`, meaning 150 every 5 minutes.\n\nIf the cluster regularly hits the given `max_compilation_rate`, it\u2019s possible the script cache is undersized, use [Nodes Stats](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-nodes-stats) to inspect the number of recent cache evictions, `script.cache_evictions_history` and compilations `script.compilations_history`.  If there are a large number of recent cache evictions or compilations, the script cache may be undersized, consider doubling the size of the script cache via the setting `script.cache.max_size`.\n\n\n### Regex circuit breaker [regex-circuit-breaker]\n\nPoorly written regular expressions can degrade cluster stability and performance. The regex circuit breaker limits the use and complexity of [regex in Painless scripts](/reference/scripting-languages/painless/painless-regexes.md).\n\n$$$script-painless-regex-enabled$$$\n\n`script.painless.regex.enabled` ![logo cloud](https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg \"Supported on Elastic Cloud Hosted\")\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Enables regex in Painless scripts. Accepts:\n\n    `limited` (Default)\n    :   Enables regex but limits complexity using the [`script.painless.regex.limit-factor`](#script-painless-regex-limit-factor) cluster setting.\n\n    `true`\n    :   Enables regex with no complexity limits. Disables the regex circuit breaker.\n\n    `false`\n    :   Disables regex. Any Painless script containing a regular expression returns an error.\n\n\n$$$script-painless-regex-limit-factor$$$\n\n`script.painless.regex.limit-factor`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Limits the number of characters a regular expression in a Painless script can consider. {{es}} calculates this limit by multiplying the setting value by the script input\u2019s character length.\n\n    For example, the input `foobarbaz` has a character length of `9`. If `script.painless.regex.limit-factor` is `6`, a regular expression on `foobarbaz` can consider up to 54 (9 * 6) characters. If the expression exceeds this limit, it triggers the regex circuit breaker and returns an error.\n\n    {{es}} only applies this limit if [`script.painless.regex.enabled`](#script-painless-regex-enabled) is `limited`.\n\n\n\n## EQL circuit breaker [circuit-breakers-page-eql]\n\nWhen a [sequence](/reference/query-languages/eql/eql-syntax.md#eql-sequences) query is executed, the node handling the query needs to keep some structures in memory, which are needed by the algorithm implementing the sequence matching. When large amounts of data need to be processed, and/or a large amount of matched sequences is requested by the user (by setting the [size](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-eql-search) query param), the memory occupied by those structures could potentially exceed the available memory of the JVM. This would cause an `OutOfMemory` exception which would bring down the node.\n\nTo prevent this from happening, a special circuit breaker is used, which limits the memory allocation during the execution of a [sequence](/reference/query-languages/eql/eql-syntax.md#eql-sequences) query. When the breaker is triggered, an `org.elasticsearch.common.breaker.CircuitBreakingException` is thrown and a descriptive error message including `circuit_breaking_exception` is returned to the user.\n\nThis circuit breaker can be configured using the following settings:\n\n`breaker.eql_sequence.limit`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) The limit for circuit breaker used to restrict the memory utilisation during the execution of an EQL sequence query. This value is defined as a percentage of the JVM heap. Defaults to `50%`. If the [parent circuit breaker](#parent-circuit-breaker) is set to a value less than `50%`, this setting uses that value as its default instead.\n\n`breaker.eql_sequence.overhead`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that sequence query memory estimates are multiplied by to determine a final estimate. Defaults to `1`.\n\n`breaker.eql_sequence.type`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) Circuit breaker type. Valid values are:\n\n    `memory` (Default)\n    :   The breaker limits memory usage for EQL sequence queries.\n\n    `noop`\n    :   Disables the breaker.\n\n\n\n### {{ml-cap}} circuit breaker [circuit-breakers-page-model-inference]\n\n`breaker.model_inference.limit`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) The limit for the trained model circuit breaker. This value is defined as a percentage of the JVM heap. Defaults to `50%`. If the [parent circuit breaker](#parent-circuit-breaker) is set to a value less than `50%`, this setting uses that value as its default instead.\n\n`breaker.model_inference.overhead`\n:   ([Dynamic](docs-content://deploy-manage/stack-settings.md#dynamic-cluster-setting)) A constant that all trained model estimations are multiplied by to determine a final estimation. Defaults to `1`.\n\n`breaker.model_inference.type`\n:   ([Static](docs-content://deploy-manage/stack-settings.md#static-cluster-setting)) The underlying type of the circuit breaker. There are two valid options: `noop` and `memory`. `noop` means the circuit breaker does nothing to prevent too much memory usage. `memory` means the circuit breaker tracks the memory used by trained models and can potentially break and prevent `OutOfMemory` errors. The default value is `memory`.",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-settings-limit.html"
              ],
              "navigation_title": "Mapping limit",
              "applies_to": {
                "stack": "all"
              },
              "content": "# Mapping limit settings [mapping-settings-limit]\n\n:::{include} _snippets/serverless-availability.md\n:::\n\nUse the following settings to limit the number of field mappings (created manually or dynamically) and prevent documents from causing a mapping explosion:\n\n$$$total-fields-limit$$$\n`index.mapping.total_fields.limit` {applies_to}`serverless: all`\n:   The maximum number of fields in an index. Field and object mappings, as well as field aliases count towards this limit. Mapped runtime fields count towards this limit as well. The default value is `1000`.\n\n    ::::{important}\n    The limit is in place to prevent mappings and searches from becoming too large. Higher values can lead to performance degradations and memory issues, especially in clusters with a high load or few resources.\n\n    If you increase this setting, we recommend you also increase the [`indices.query.bool.max_clause_count`](/reference/elasticsearch/configuration-reference/search-settings.md) setting, which limits the maximum number of clauses in a query.\n\n    ::::\n\n\n    ::::{tip}\n    If your field mappings contain a large, arbitrary set of keys, consider using the [flattened](/reference/elasticsearch/mapping-reference/flattened.md) data type, or setting the index setting `index.mapping.total_fields.ignore_dynamic_beyond_limit` to `true`.\n\n    ::::\n\n$$$ignore-dynamic-beyond-limit$$$\n`index.mapping.total_fields.ignore_dynamic_beyond_limit` {applies_to}`serverless: all`\n:   This setting determines what happens when a dynamically mapped field would exceed the total fields limit. When set to `false` (the default), the index request of the document that tries to add a dynamic field to the mapping will fail with the message `Limit of total fields [X] has been exceeded`. When set to `true`, the index request will not fail. Instead, fields that would exceed the limit are not added to the mapping, similar to [`dynamic: false`](/reference/elasticsearch/mapping-reference/dynamic.md). The fields that were not added to the mapping will be added to the [`_ignored` field](/reference/elasticsearch/mapping-reference/mapping-ignored-field.md). The default value is `false`.\n\n`index.mapping.depth.limit`\n:   The maximum depth for a field, which is measured as the number of inner objects. For instance, if all fields are defined at the root object level, then the depth is `1`. If there is one object mapping, then the depth is `2`, etc. Default is `20`.\n\n`index.mapping.nested_fields.limit`\n:   The maximum number of distinct `nested` mappings in an index. The `nested` type should only be used in special cases, when arrays of objects need to be queried independently of each other. To safeguard against poorly designed mappings, this setting limits the number of unique `nested` types per index. Default is `50`.\n\n`index.mapping.nested_objects.limit`\n:   The maximum number of nested JSON objects that a single document can contain across all `nested` types. This limit helps to prevent out of memory errors when a document contains too many nested objects. Default is `10000`.\n\n`index.mapping.field_name_length.limit`\n:   Setting for the maximum length of a field name. This setting isn\u2019t really something that addresses mappings explosion but might still be useful if you want to limit the field length. It usually shouldn\u2019t be necessary to set this setting. The default is okay unless a user starts to add a huge number of fields with really long names. Default is `Long.MAX_VALUE` (no limit).\n\n`index.mapping.dimension_fields.limit`\n:   (Dynamic, integer) Maximum number of [time series dimensions](docs-content://manage-data/data-store/data-streams/time-series-data-stream-tsds.md#time-series-dimension) for the index. Defaults to `32768`.",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/index-settings/mapping-limit.md"
            },
            {
              "content": "For instance,\n```esql\nFROM index | WHERE field = \"value\"\n```\nis equivalent to:\n```esql\nFROM index | WHERE field = \"value\" | LIMIT 1000\n```\n\nQueries do not return more than 10,000 rows, regardless of the `LIMIT` command\u2019s value. This is a configurable upper limit.\n\nTo overcome this limitation:\n\n* Reduce the result set size by modifying the query to only return relevant data. Use [`WHERE`](/reference/query-languages/esql/commands/where.md) to select a smaller subset of the data.\n* Shift any post-query processing to the query itself. You can use the {{esql}} [`STATS`](/reference/query-languages/esql/commands/stats-by.md) command to aggregate data in the query.\n\nThe upper limit only applies to the number of rows that are output by the query, not to the number of documents it processes: the query runs on the full data set.\n\nConsider the following two queries:\n```esql\nFROM index | WHERE field0 == \"value\" | LIMIT 20000\n```\nand\n```esql\nFROM index | STATS AVG(field1) BY field2 | LIMIT 20000\n```\n\nIn both cases, the filtering by `field0` in the first query or the grouping by `field2` in the second is applied over all the documents present in the `index`, irrespective of their number or indexes size. However, both queries will return at most 10,000 rows, even if there were more rows available to return.\n\nThe default and maximum limits can be changed using these dynamic cluster settings:\n\n* `esql.query.result_truncation_default_size`\n* `esql.query.result_truncation_max_size`\n\nHowever, doing so involves trade-offs. A larger result-set involves a higher memory pressure and increased processing times; the internode traffic within and across clusters can also increase.\n\nThese limitations are similar to those enforced by the [search API for pagination](/reference/elasticsearch/rest-apis/paginate-search-results.md).\n\n| Functionality                    | Search                  | {{esql}}                                  |\n|----------------------------------|-------------------------|-------------------------------------------|\n| Results returned by default      | 10                      | 1.000                                     |\n| Default upper limit              | 10,000                  | 10,000                                    |\n| Specify number of results        | `size`                  | `LIMIT`                                   |\n| Change default number of results | n/a                     | esql.query.result_truncation_default_size |\n| Change default upper limit       | index-max-result-window | esql.query.result_truncation_max_size     |",
              "filename": "elasticsearch-main/docs/reference/query-languages/esql/_snippets/common/result-set-size-limitation.md"
            },
            {
              "navigation_title": "Rename",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/rename-processor.html"
              ],
              "content": "# Rename processor [rename-processor]\n\n\nRenames an existing field. If the field doesn\u2019t exist or the new name is already used, an exception will be thrown.\n\n$$$rename-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `field` | yes | - | The field to be renamed. Supports [template snippets](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#template-snippets). |\n| `target_field` | yes | - | The new name of the field. Supports [template snippets](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#template-snippets). |\n| `ignore_missing` | no | `false` | If `true` and `field` does not exist, the processor quietly exits without modifying the document. |\n| `override` | no | `false` | If `true`, the processor will update pre-existing non-null-valued fields. When set to `false`, such fields will not be touched. |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n```js\n{\n  \"rename\": {\n    \"field\": \"provider\",\n    \"target_field\": \"cloud.provider\"\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/rename-processor.md"
            }
          ],
          "tool_call_id": "call_Z1VF1LMjdShn9gzq1x7lfgST",
          "metadata": null,
          "timestamp": "2025-10-03T04:07:23.538389+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "In Elasticsearch, the **field data circuit breaker** is designed to manage memory consumption related to loading fields into the field data cache. Here\u2019s how it works and the consequences of exceeding its limits:\n\n### Memory Requirement Estimation\n\n1. **Estimation Process**: The field data circuit breaker estimates the heap memory needed to load a particular field into the field data cache. This is crucial because the cache uses on-heap data structures, and improper handling can lead to excessive memory usage.\n  \n2. **Overhead Factor**: The estimation calculation multiplies the expected memory usage by a predefined overhead factor (default is `1.03`). This overhead is intended to provide a buffer, taking into account variability in memory usage during actual operations ([source](https://github.com/elastic/elasticsearch/blob/main/elasticsearch-main/docs/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md#fielddata-circuit-breaker)).\n\n### What Happens If Limits Are Exceeded\n\n1. **Prevention of Operations**: If the estimation indicates that loading the field would exceed the limit set for the field data circuit breaker (defaults to 40% of the JVM heap), the breaker will prevent further operations that could increase cache size. This essentially stops problematic requests from being executed ([source](https://github.com/elastic/elasticsearch/blob/main/elasticsearch-main/docs/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md#fielddata-circuit-breaker)).\n\n2. **Error Messages**: When triggered, it will return an error rather than performing the action. This mechanism prevents the system from becoming unstable due to high memory pressure, which could potentially lead to an `OutOfMemoryError` if not checked ([source](https://github.com/elastic/elasticsearch/blob/main/elasticsearch-main/docs/reference/elasticsearch/configuration-reference/circuit-breaker-settings.md)).\n\n3. **Manual Cache Clearing**: If the limit is reached, and operations are halted, you might need to manually clear the cache to free up memory ([source](https://github.com/elastic/elasticsearch/blob/main/elasticsearch-main/docs/reference/elasticsearch/configuration-reference/field-data-cache-settings.md)).\n\nOverall, the field data circuit breaker is a critical safety feature in Elasticsearch that helps maintain stability and performance by managing memory usage during operations that involve loading field data.",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 20491,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 467,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:07:23+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMRMht1BA29kmEQqv1JQyXX9XU7ws",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}