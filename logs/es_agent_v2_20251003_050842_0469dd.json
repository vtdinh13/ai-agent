{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How can I deploy the GraphQL connector using Docker and what are the preliminary steps involved?",
          "timestamp": "2025-10-03T05:08:39.981411+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"GraphQL connector Docker deployment\"}",
          "tool_call_id": "call_OzbhGIthdUkp9LdQ0AZwCy8s",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 219,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 17,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T05:08:40+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMSK0D2Th5BTJ1ceqdL2VTcIZ0TnE",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "GraphQL",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/es-connectors-graphql.html"
              ],
              "content": "# Elastic GraphQL connector reference [es-connectors-graphql]\n\n\nThe Elastic GraphQL connector is written in Python using the [Elastic connector framework](https://github.com/elastic/connectors/tree/main). View the [source code for this connector](https://github.com/elastic/connectors/blob/main/connectors/sources/graphql.py).\n\n\n## Availability and prerequisites [es-connectors-graphql-connector-availability-and-prerequisites]\n\nThis connector was introduced in Elastic **8.14.0**, available as a **self-managed** self-managed connector.\n\nTo use this connector, satisfy all [self-managed connector prerequisites](/reference/search-connectors/self-managed-connectors.md). Importantly, you must deploy the connectors service on your own infrastructure. You have two deployment options:\n\n* [Run connectors service from source](/reference/search-connectors/es-connectors-run-from-source.md). Use this option if you\u2019re comfortable working with Python and want to iterate quickly locally.\n* [Run connectors service in Docker](/reference/search-connectors/es-connectors-run-from-docker.md). Use this option if you want to deploy the connectors to a server, or use a container orchestration platform.\n\n::::{note}\nThis connector is in **technical preview** and is subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Technical preview features are not subject to the support SLA of official GA features.\n\n::::\n\n\n\n## Usage [es-connectors-graphql-connector-usage]\n\nTo set up this connector in the UI, select the **GraphQL** tile when creating a new connector under **Search \u2192 Connectors**.\n\nIf you\u2019re already familiar with how connectors work, you can also use the [Connector APIs](https://www.elastic.co/docs/api/doc/elasticsearch/group/endpoint-connector).\n\nFor additional operations, see [*Connectors UI in {{kib}}*](/reference/search-connectors/connectors-ui-in-kibana.md).\n\n\n## Deploy with Docker [es-connectors-graphql-connector-docker]\n\nYou can deploy the GraphQL connector as a self-managed connector using Docker. Follow these instructions.\n\n::::{dropdown} Step 1: Download sample configuration file\nDownload the sample configuration file. You can either download it manually or run the following command:\n\n```sh\ncurl https://raw.githubusercontent.com/elastic/connectors/main/config.yml.example --output ~/connectors-config/config.yml\n```\n\nRemember to update the `--output` argument value if your directory name is different, or you want to use a different config file name.\n\n::::\n\n\n::::{dropdown} Step 2: Update the configuration file for your self-managed connector\nUpdate the configuration file with the following settings to match your environment:\n\n* `elasticsearch.host`\n* `elasticsearch.api_key`\n* `connectors`\n\nIf you\u2019re running the connector service against a Dockerized version of Elasticsearch and Kibana, your config file will look like this:\n\n```yaml\n# When connecting to your cloud deployment you should edit the host value\nelasticsearch.host: http://host.docker.internal:9200\nelasticsearch.api_key: <ELASTICSEARCH_API_KEY>\n\nconnectors:\n  -\n    connector_id: <CONNECTOR_ID_FROM_KIBANA>\n    service_type: graphql\n    api_key: <CONNECTOR_API_KEY_FROM_KIBANA> # Optional. If not provided, the connector will use the elasticsearch.api_key instead\n```\n\nUsing the `elasticsearch.api_key` is the recommended authentication method. However, you can also use `elasticsearch.username` and `elasticsearch.password` to authenticate with your Elasticsearch instance.\n\nNote: You can change other default configurations by simply uncommenting specific settings in the configuration file and modifying their values.\n\n::::\n\n\n::::{dropdown} Step 3: Run the Docker image\nRun the Docker image with the Connector Service using the following command:\n\n```sh subs=true\ndocker run \\\n-v ~/connectors-config:/config \\\n--network \"elastic\" \\\n--tty \\\n--rm \\\ndocker.elastic.co/integrations/elastic-connectors:{{version.stack}} \\\n/app/bin/elastic-ingest \\\n-c /config/config.yml\n```\n\n::::\n\n\nRefer to [`DOCKER.md`](https://github.com/elastic/connectors/tree/main/docs/DOCKER.md) in the `elastic/connectors` repo for more details.\n\nFind all available Docker images in the [official registry](https://www.docker.elastic.co/r/integrations/elastic-connectors).\n\n::::{tip}\nWe also have a quickstart self-managed option using Docker Compose, so you can spin up all required services at once: Elasticsearch, Kibana, and the connectors service. Refer to this [README](https://github.com/elastic/connectors/tree/main/scripts/stack#readme) in the `elastic/connectors` repo for more information.\n\n::::\n\n\n\n## Configuration [es-connectors-graphql-connector-configuration]\n\n\n### Configure GraphQL connector [es-connectors-graphql-connector-configure-graphql-connector]\n\nNote the following configuration fields:\n\n`http_endpoint` (required)\n:   Base URL of the GraphQL endpoint. **Example**: `https://api.xyz.com/graphql`\n\n`http_method` (required)\n:   `GET` or `POST`.\n\n`authentication_method`(required)\n:   Select from `No Auth`, `Basic Auth`, and `Bearer Token`.\n\n`username`\n:   Required when using basic authentication.\n\n`password`\n:   Required when using basic authentication.\n\n`token`\n:   Required when using bearer token authentication.\n\n`graphql_query` (required)\n:   Query used to fetch data from the source. Can contain variables provided in the `graphql_variables` field. The connector will substitute the variables in the query with values from `graphql_variables` and make a GraphQL query to the source.\n\n    **Example**:\n\n    ```js\n    query getUser($id: ID!) {\n        user(id: $id) {\n            name\n            email\n        }\n    }\n    ```\n\n\n`graphql_variables`\n:   A JSON object of key/value pairs containing variables used in the GraphQL query. The connector will substitute the variables in the query with the values provided here and make a GraphQL query to the source.\n\n    **Example**:\n\n    For the GraphQL query `query getUser($id: ID!) { user(id: $id) { name } }`\n\n    * Where the value of `graphql_variables` is `{\"id\": \"123\"}`\n    * The connector will execute `query getUser { user(id: \"123\") { name } }` to fetch data from the source\n\n\n`graphql_object_to_id_map` (required)\n:   A JSON mapping between GraphQL response objects to index and their ID fields. The connector will fetch data for each object (JSON key) and use the provided ID field (JSON value) to index the object into Elasticsearch. The connector will index all fields for each object specified in the mapping. Use dot `(.)` notation to specify the full path from the root of the GraphQL response to the desired object.\n\n    **Example**:\n\n    The GraphQL query `query getUser { organization { users{ user_id name email} } }` fetches all available users from the source. To index every user as a separate document configure this field as below.\n\n    ```js\n    {\n        \"organization.users\": \"user_id\"\n    }\n    ```\n\n    In this example `user_id` is unique in every user document. Therefore, we set `user_id` as the value for `organization.users`.\n\n    ::::{note}\n    The path provided in this field should only contain JSON objects and not lists.\n\n    ::::\n\n\n`headers`\n:   JSON object containing custom headers to be sent with each GraphQL request:\n\n    ```js\n    {\n        \"content-type\": \"Application/json\"\n    }\n    ```\n\n\n`pagination_model` (required)\n:   This field specifies the pagination model to be used by the connector. The connector supports `No pagination` and `Cursor-based pagination` pagination models.\n\n    For cursor-based pagination, add `pageInfo {endCursor hasNextPage}` and an `after` argument variable in your query at the desired node (`Pagination key`). Use the `after` query argument with a variable to iterate through pages. The default value for this field is `No pagination`. Example:\n\n    For `Cursor-based pagination`, the query should look like this example:\n\n    ```js\n    query getUsers($cursor: String!) {\n        sampleData {\n            users(after: $cursor) {\n                pageInfo {\n                    endCursor\n                    hasNextPage\n                }\n                nodes {\n                    first_name\n                    last_name\n                    address\n                }\n            }\n        }\n    }\n    ```\n\n    The value of `pagination_key` is `sampleData.users` so it must contain:\n\n    * `pageInfo {endCursor hasNextPage}`\n    * the `after` argument with a variable when using cursor-based pagination\n\n\n`pagination_key` (required)\n:   Specifies which GraphQL object is used for pagination. Use `.` to provide the full path of the object from the root of the response.\n\n    **Example**:\n\n    * `organization.users`\n\n\n`connection_timeout`\n:   Specifies the maximum time in seconds to wait for a response from the GraphQL source. Default value is **30 seconds**.\n\n\n## Documents and syncs [es-connectors-graphql-connector-documents-and-syncs]\n\nThe connector syncs the objects and entities based on GraphQL Query and GraphQL Object List.\n\n\n## Sync types [es-connectors-graphql-connector-sync-types]\n\n[Full syncs](/reference/search-connectors/content-syncs.md#es-connectors-sync-types-full) are supported by default for all connectors.\n\nThis connector currently does not support [incremental syncs](/reference/search-connectors/content-syncs.md#es-connectors-sync-types-incremental).\n\n\n## Sync rules [es-connectors-graphql-connector-sync-rules]\n\n[Basic sync rules](/reference/search-connectors/es-sync-rules.md#es-sync-rules-basic) are identical for all connectors and are available by default.\n\n\n## Advanced Sync Rules [es-connectors-graphql-connector-advanced-sync-rules]\n\nAdvanced sync rules are not available for this connector in the present version.\n\n\n## Connector Client operations [es-connectors-graphql-connector-connector-client-operations]\n\n\n### End-to-end Testing [es-connectors-graphql-connector-end-to-end-testing]\n\nThe connector framework enables operators to run functional tests against a real data source, using Docker Compose. You don\u2019t need a running Elasticsearch instance or GraphQL source to run this test.\n\nRefer to [Connector testing](/reference/search-connectors/self-managed-connectors.md#es-build-connector-testing) for more details.\n\nTo perform E2E testing for the GraphQL connector, run the following command:\n\n```shell\n$ make ftest NAME=graphql\n```\n\nFor faster tests, add the `DATA_SIZE=small` flag:\n\n```shell\nmake ftest NAME=graphql DATA_SIZE=small\n```\n\nBy default, `DATA_SIZE=MEDIUM`.\n\n\n## Known issues [es-connectors-graphql-connector-known-issues]\n\n* Every document will be updated in every sync.\n* If the same field name exists with different types across different objects, the connector might raise a mapping parser exception.\n\nRefer to [Known issues](/release-notes/known-issues.md) for a list of known issues for all connectors.\n\n\n## Troubleshooting [es-connectors-graphql-connector-troubleshooting]\n\nSee [Troubleshooting](/reference/search-connectors/es-connectors-troubleshooting.md).\n\n\n## Security [es-connectors-graphql-connector-security]\n\nSee [Security](/reference/search-connectors/es-connectors-security.md).",
              "filename": "elasticsearch-main/docs/reference/search-connectors/es-connectors-graphql.md"
            },
            {
              "navigation_title": "API tutorial",
              "applies_to": {
                "stack": "ga",
                "serverless": {
                  "elasticsearch": "ga"
                }
              },
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/es-connectors-tutorial-api.html"
              ],
              "description": "Use APIs to synchronize data from a PostgreSQL data source into Elasticsearch.",
              "content": "# Connector API tutorial [es-connectors-tutorial-api]\n\nLearn how to set up a self-managed connector using the [{{es}} connector APIs]({{es-apis}}group/endpoint-connector).\n\nFor this example we\u2019ll use the [PostgreSQL connector](/reference/search-connectors/es-connectors-postgresql.md) to sync data from a PostgreSQL database to {{es}}. We\u2019ll spin up a simple PostgreSQL instance in Docker with some example data, create a connector, and sync the data to {{es}}. You can follow the same steps to set up a connector for another data source.\n\n::::{tip}\nThis tutorial focuses on running a self-managed connector on your own infrastructure, and managing syncs using the connector APIs.\n\nIf you\u2019re just getting started with {{es}}, this tutorial might be a bit advanced. Refer to [quickstart](docs-content://solutions/search/get-started.md) for a more beginner-friendly introduction to {{es}}.\n\nIf you\u2019re just getting started with connectors, you might want to start in the UI first. Check out this tutorial that focuses on managing connectors using the UI: [](/reference/search-connectors/es-postgresql-connector-client-tutorial.md).\n\n::::\n\n## Prerequisites [es-connectors-tutorial-api-prerequisites]\n\n* You should be familiar with how connectors, connectors work, to understand how the API calls relate to the overall connector setup.\n* You need to have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed.\n* You need to have {{es}} running, and an API key to access it. Refer to the next section for details, if you don\u2019t have an {{es}} deployment yet.\n\n## Set up {{es}} [es-connectors-tutorial-api-setup-es]\n\nIf you already have an {{es}} deployment on Elastic Cloud (*Hosted deployment* or *Serverless project*), you\u2019re good to go. To spin up {{es}} in local dev mode in Docker for testing purposes, open the collapsible section below.\n\n:::::{dropdown} Run local {{es}} in Docker\n```sh subs=true\ndocker run -p 9200:9200 -d --name elasticsearch \\\n  -e \"discovery.type=single-node\" \\\n  -e \"xpack.security.enabled=false\" \\\n  -e \"xpack.security.http.ssl.enabled=false\" \\\n  -e \"xpack.license.self_generated.type=trial\" \\\n  docker.elastic.co/elasticsearch/elasticsearch:{{version.stack}}\n```\n\n::::{warning}\nThis {{es}} setup is for development purposes only. Never use this configuration in production. Refer to [Set up {{es}}](docs-content://deploy-manage/deploy/self-managed/installing-elasticsearch.md) for production-grade installation instructions, including Docker.\n\n::::\n\n\nWe will use the default password `changeme` for the `elastic` user. For production environments, always ensure your cluster runs with security enabled.\n\n```sh\nexport ELASTIC_PASSWORD=\"changeme\"\n```\n\nSince we run our cluster locally with security disabled, we won\u2019t use API keys to authenticate against the {{es}}. Instead, in each cURL request, we will use the `-u` flag for authentication.\n\nLet\u2019s test that we can access {{es}}:\n\n```sh\ncurl -s -X GET -u elastic:$ELASTIC_PASSWORD http://localhost:9200\n```\n\nNote: With {{es}} running locally, you will need to pass the username and password to authenticate against {{es}} in the configuration file for the connector service.\n\n:::::\n\n\n::::{admonition} Running API calls\nYou can run API calls using the [Dev Tools Console](docs-content://explore-analyze/query-filter/tools/console.md) in Kibana, using `curl` in your terminal, or with our programming language clients.\nTo use curl, you\u2019ll need to add authentication headers to your request.\n\nHere\u2019s an example of how to do that. Note that if you want the connector ID to be auto-generated, use the `POST _connector` endpoint.\n\n```sh\ncurl -s -X PUT http://localhost:9200/_connector/my-connector-id \\\n-H \"Authorization: APIKey $APIKEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"name\": \"Music catalog\",\n  \"index_name\":  \"music\",\n  \"service_type\": \"postgresql\"\n}'\n```\n\nRefer to [](/reference/search-connectors/es-postgresql-connector-client-tutorial.md) for instructions on creating an API key.\n\n::::\n\n## Run PostgreSQL instance in Docker (optional) [es-connectors-tutorial-api-setup-postgres]\n\nFor this tutorial, we\u2019ll set up a PostgreSQL instance in Docker with some example data. Of course, you can **skip this step and use your own existing PostgreSQL instance** if you have one. Keep in mind that using a different instance might require adjustments to the connector configuration described in the next steps.\n\n::::{dropdown} Expand to run simple PostgreSQL instance in Docker and import example data\nLet\u2019s launch a PostgreSQL container with a user and password, exposed at port `5432`:\n\n```sh\ndocker run --name postgres -e POSTGRES_USER=myuser -e POSTGRES_PASSWORD=mypassword -p 5432:5432 -d postgres\n```\n\n### Download and import example data\n\nNext we need to create a directory to store our example dataset for this tutorial. In your terminal, run the following command:\n\n```sh\nmkdir -p ~/data\n```\n\nWe will use the [Chinook dataset](https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_PostgreSql.sql) example data.\n\nRun the following command to download the file to the `~/data` directory:\n\n```sh\ncurl -L https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_PostgreSql.sql -o ~/data/Chinook_PostgreSql.sql\n```\n\nNow we need to import the example data into the PostgreSQL container and create the tables.\n\nRun the following Docker commands to copy our sample data into the container and execute the `psql` script:\n\n```sh\ndocker cp ~/data/Chinook_PostgreSql.sql postgres:/\ndocker exec -it postgres psql -U myuser -f /Chinook_PostgreSql.sql\n```\n\nLet\u2019s verify that the tables are created correctly in the `chinook` database:\n\n```sh\ndocker exec -it postgres psql -U myuser -d chinook -c \"\\dt\"\n```\n\nThe `album` table should contain **347** entries and the `artist` table should contain **275** entries.\n\n::::\n\n\nThis tutorial uses a very basic setup. To use advanced functionality such as filtering rules and incremental syncs, enable `track_commit_timestamp` on your PostgreSQL database. Refer to postgresql-connector-client-tutorial for more details.\n\nNow it\u2019s time for the real fun! We\u2019ll set up a connector to create a searchable mirror of our PostgreSQL data in {{es}}.\n\n## Create a connector [es-connectors-tutorial-api-create-connector]\n\nWe\u2019ll use the [create connector API]({{es-apis}}operation/operation-connector-put) to create a PostgreSQL connector instance.\n\nRun the following API call, using the [Dev Tools Console](docs-content://explore-analyze/query-filter/tools/console.md) or `curl`:\n\n```console\nPUT _connector/my-connector-id\n{\n  \"name\": \"Music catalog\",\n  \"index_name\":  \"music\",\n  \"service_type\": \"postgresql\"\n}\n```\n\n::::{tip}\n`service_type` refers to the third-party data source you\u2019re connecting to.\n\n::::\n\n\nNote that we specified the `my-connector-id` ID as a part of the `PUT` request. We\u2019ll need the connector ID to set up and run the connector service locally.\n\nIf you\u2019d prefer to use an autogenerated ID, replace `PUT _connector/my-connector-id` with `POST _connector`.\n\n## Set up the connector service [es-connectors-tutorial-api-deploy-connector]\n\nNow we\u2019ll run the connector service so we can start syncing data from our PostgreSQL instance to {{es}}. We\u2019ll use the steps outlined in [](/reference/search-connectors/es-connectors-run-from-docker.md).\n\nWhen running the connectors service on your own infrastructure, you need to provide a configuration file with the following details:\n\n* Your {{es}} endpoint (`elasticsearch.host`)\n* An {{es}} API key (`elasticsearch.api_key`)\n* Your third-party data source type (`service_type`)\n* Your connector ID (`connector_id`)\n\n### Create an API key [es-connectors-tutorial-api-create-api-key]\n\nIf you haven\u2019t already created an API key to access {{es}}, you can use the [_security/api_key]({{es-apis}}operation/operation-security-create-api-key) endpoint.\n\nHere, we assume your target {{es}} index name is `music`. If you use a different index name, adjust the request body accordingly.\n\n```console\nPOST /_security/api_key\n{\n  \"name\": \"music-connector\",\n  \"role_descriptors\": {\n    \"music-connector-role\": {\n      \"cluster\": [\n        \"monitor\",\n        \"manage_connector\"\n      ],\n      \"indices\": [\n        {\n          \"names\": [\n            \"music\",\n            \".search-acl-filter-music\",\n            \".elastic-connectors*\"\n          ],\n          \"privileges\": [\n            \"all\"\n          ],\n          \"allow_restricted_indices\": false\n        }\n      ]\n    }\n  }\n}\n```\n\nYou\u2019ll need to use the `encoded` value from the response as the `elasticsearch.api_key` in your configuration file.\n\n::::{tip}\nYou can also create an API key in the {{kib}} and Serverless UIs.\n\n::::\n\n### Prepare the configuration file [es-connectors-tutorial-api-prepare-configuration-file]\n\nLet\u2019s create a directory and a `config.yml` file to store the connector configuration:\n\n```sh\nmkdir -p ~/connectors-config\ntouch ~/connectors-config/config.yml\n```\n\nNow, let\u2019s add our connector details to the config file. Open `config.yml` and paste the following configuration, replacing placeholders with your own values:\n\n```yaml\nelasticsearch.host: <ELASTICSEARCH_ENDPOINT> # Your Elasticsearch endpoint\nelasticsearch.api_key: <ELASTICSEARCH_API_KEY> # Your Elasticsearch API key\n\nconnectors:\n  - connector_id: \"my-connector-id\"\n    service_type: \"postgresql\"\n```\n\nWe provide an [example configuration file](https://raw.githubusercontent.com/elastic/connectors/main/config.yml.example) in the `elastic/connectors` repository for reference.\n\n### Run the service [es-connectors-tutorial-api-run-connector-service]\n\nNow that we have the configuration file set up, we can run the connector service locally. This will point your connector instance at your {{es}} deployment.\n\nRun the following Docker command to start the connector service:\n\n```sh subs=true\ndocker run \\\n-v \"$HOME/connectors-config:/config\" \\\n--rm \\\n--tty -i \\\n--network host \\\ndocker.elastic.co/integrations/elastic-connectors:{{version.stack}} \\\n/app/bin/elastic-ingest \\\n-c /config/config.yml\n```\n\nVerify your connector is connected by getting the connector status (should be `needs_configuration`) and `last_seen` field (note that time is reported in UTC). The `last_seen` field indicates that the connector successfully connected to {{es}}.\n\n```console\nGET _connector/my-connector-id\n```\n\n## Configure the connector [es-connectors-tutorial-api-update-connector-configuration]\n\nNow our connector instance is up and running, but it doesn\u2019t yet know *where* to sync data from. The final piece of the puzzle is to configure our connector with details about our PostgreSQL instance. When setting up a connector in the Elastic Cloud or Serverless UIs, you\u2019re prompted to add these details in the user interface.\n\nBut because this tutorial is all about working with connectors *programmatically*, we\u2019ll use the [update connector configuration API]({{es-apis}}operation/operation-connector-update-configuration) to add our configuration details.\n\n::::{tip}\nBefore configuring the connector, ensure that the configuration schema is registered by the service. For self-managed connectors, the schema registers on service startup (once the `config.yml` is populated).\n\nConfiguration updates via the API are possible only *after schema registration*. Verify this by checking the configuration property returned by the `GET _connector/my-connector-id` request. It should be non-empty.\n\n::::\n\n\nRun the following API call to configure the connector with our connectors-postgresql-client-configuration,PostgreSQL configuration details:\n\n```console\nPUT _connector/my-connector-id/_configuration\n{\n  \"values\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": 5432,\n    \"username\": \"myuser\",\n    \"password\": \"mypassword\",\n    \"database\": \"chinook\",\n    \"schema\": \"public\",\n    \"tables\": \"album,artist\"\n  }\n}\n```\n\n::::{note}\nConfiguration details are specific to the connector type. The keys and values will differ depending on which third-party data source you\u2019re connecting to. Refer to the individual connectors-references,connector references for these configuration details.\n\n::::\n\n## Sync your data [es-connectors-tutorial-api-sync]\n\nWe\u2019re now ready to sync our PostgreSQL data to {{es}}. Run the following API call to start a full sync job:\n\n```console\nPOST _connector/_sync_job\n{\n    \"id\": \"my-connector-id\",\n    \"job_type\": \"full\"\n}\n```\n\nTo store data in {{es}}, the connector needs to create an index. When we created the connector, we specified the `music` index. The connector will create and configure this {{es}} index before launching the sync job.\n\n::::{tip}\nIn the approach we\u2019ve used here, the connector will use [dynamic mappings](docs-content://manage-data/data-store/mapping.md#mapping-dynamic) to automatically infer the data types of your fields. In a real-world scenario you would use the {{es}} [create index API]({{es-apis}}operation/operation-indices-create) to first create the index with the desired field mappings and index settings. Defining your own mappings upfront gives you more control over how your data is indexed.\n\n::::\n\n### Check sync status [es-connectors-tutorial-api-check-sync-status]\n\nUse the [get sync job API]({{es-apis}}operation/operation-connector-sync-job-get) to track the status and progress of the sync job. By default, the most recent job statuses are returned first. Run the following API call to check the status of the sync job:\n\n```console\nGET _connector/_sync_job?connector_id=my-connector-id&size=1\n```\n\nThe job document will be updated as the sync progresses, you can check it as often as you\u2019d like to poll for updates.\n\nOnce the job completes, the status should be `completed` and `indexed_document_count` should be **622**.\n\n## Query your data\n\nVerify that data is present in the `music` index with the following API call:\n\n```console\nGET music/_count\n```\n\n{{es}} stores data in documents, which are JSON objects. List the individual documents with the following API call:\n\n```console\nGET music/_search\n```\n\n## Troubleshoot [es-connectors-tutorial-api-troubleshooting]\n\nUse the following command to inspect the latest sync job\u2019s status:\n\n```console\nGET _connector/_sync_job?connector_id=my-connector-id&size=1\n```\n\nIf the connector encountered any errors during the sync, you\u2019ll find these in the `error` field.\n\n\n## Clean up [es-connectors-tutorial-api-cleanup]\n\nTo delete the connector and its associated sync jobs run this command:\n\n```console\nDELETE _connector/my-connector-id&delete_sync_jobs=true\n```\n\nThis won\u2019t delete the Elasticsearch index that was created by the connector to store the data. Delete the `music` index by running the following command:\n\n```console\nDELETE music\n```\n\nTo remove the PostgreSQL container, run the following commands:\n\n```sh\ndocker stop postgres\ndocker rm postgres\n```\n\nTo remove the connector service, run the following commands:\n\n```sh\ndocker stop <container_id>\ndocker rm <container_id>\n```\n\n## Next steps [es-connectors-tutorial-api-next-steps]\n\nCongratulations! You\u2019ve successfully set up a self-managed connector using the connector APIs.\n\nHere are some next steps to explore:\n\n* Learn more about the [connector APIs]({{es-apis}}group/endpoint-connector).\n* Learn how to deploy {{es}}, {{kib}}, and the connectors service using Docker Compose in our [quickstart guide](https://github.com/elastic/connectors/tree/main/scripts/stack#readme).",
              "filename": "elasticsearch-main/docs/reference/search-connectors/api-tutorial.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/es-connectors-docker-compose-quickstart.html"
              ],
              "content": "# Docker Compose quickstart [es-connectors-docker-compose-quickstart]\n\nUse our Docker Compose quickstart to easily set up a full self-managed stack and try out Elastic Connectors.\n\nWe\u2019ve provided a script to start up and run Elasticsearch, Kibana, and Connectors instances using Docker Compose. The script prompts you to configure your Connectors before starting. Additionally, you can use the same set of scripts to manually configure your Connectors and run the stack.\n\nRefer to the [README](https://github.com/elastic/connectors/tree/main/scripts/stack#readme) in the [elastic/connectors](https://github.com/elastic/connectors) source repository for more information.",
              "filename": "elasticsearch-main/docs/reference/search-connectors/es-connectors-docker-compose-quickstart.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/es-connectors-run-from-docker.html"
              ],
              "content": "# Running from a Docker container [es-connectors-run-from-docker]\n\n::::{tip}\nUse our [Docker Compose quickstart](/reference/search-connectors/es-connectors-docker-compose-quickstart.md) to quickly get started with a full Elastic Stack deployment using Connectors.\n::::\n\nInstead of running the Connectors Service from source, you can use the official Docker image to run the service in a container.\n\nAs a prerequisite, you need to have an Elasticsearch and Kibana instance running. From inside your Kibana UI, You will need to [follow the initial setup](/reference/search-connectors/es-connectors-run-from-source.md#es-connectors-run-from-source-setup-kibana) in the same manner as if you are running the service from source.\n\nWhen you are ready to run Connectors:\n\n**Step 1: Download sample configuration file**\n\nDownload the sample configuration file. You can either download it manually or run the following command:\n\n```sh\ncurl https://raw.githubusercontent.com/elastic/connectors/main/config.yml.example --output </absolute/path/to>/connectors-config/config.yml\n```\n\nDon\u2019t forget to change the `--output` argument value to the path where you want to save the `config.yml` file on your local system. But keep note of where you wrote this file, as it is required in the `docker run` step below.\n\n**Step 2: Update the configuration file for your self-managed connector**\n\nUpdate the configuration file with the following settings to match your environment:\n\n* `elasticsearch.host`\n* `elasticsearch.api_key`\n* `connectors`\n\nIf you\u2019re running the connector service against a Dockerized version of Elasticsearch and Kibana, your config file will look like this:\n\n```yaml\n# When connecting to your cloud deployment you should edit the host value\nelasticsearch.host: http://host.docker.internal:9200\nelasticsearch.api_key: <ELASTICSEARCH_API_KEY>\n\nconnectors:\n  -\n    connector_id: <CONNECTOR_ID_FROM_KIBANA>\n    service_type: sharepoint_online # Example value \u2014 update this for service type you are connecting to\n    api_key: <CONNECTOR_API_KEY_FROM_KIBANA> # Optional. If not provided, the connector will use the elasticsearch.api_key instead\n```\n\n**Step 3: Run the Docker image**\n\nRun the Docker image with the Connector Service using the following command:\n\n```sh subs=true\ndocker run \\\n-v \"</absolute/path/to>/connectors-config:/config\" \\ # NOTE: you must change this path to match where the config.yml is located\n--rm \\\n--tty -i \\\n--network host \\\ndocker.elastic.co/integrations/elastic-connectors:{{version.stack}} \\\n/app/bin/elastic-ingest \\\n-c /config/config.yml\n```\n\n::::{tip}\nFor unreleased versions, append the `-SNAPSHOT` suffix to the version number. For example, `docker.elastic.co/integrations/elastic-connectors:{{version.stack}}-SNAPSHOT`.\n\n::::\n\n\nFind all available Docker images in the [official registry](https://www.docker.elastic.co/r/integrations/elastic-connectors).\n\n\n## Enter data source details in Kibana [es-build-connector-finalizes-kibana]\n\nOnce the connector service is running, it\u2019s time to head back to the Kibana UI to finalize the connector configuration. In this step, you need to add the specific connection details about your data source instance, like URL, authorization credentials, etc. As mentioned above, these details will vary based on the third-party data source you\u2019re connecting to.\n\nFor example, the PostgreSQL connector requires the following details:\n\n* **Host**\n* **Port**\n* **Username**\n* **Password**\n* **Database**\n* **Comma-separated list of tables**\n\nYou\u2019re now ready to run a sync. Select the **Full sync** button in the Kibana UI to start ingesting documents into Elasticsearch.",
              "filename": "elasticsearch-main/docs/reference/search-connectors/es-connectors-run-from-docker.md"
            },
            {
              "navigation_title": "Tutorial",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/es-postgresql-connector-client-tutorial.html",
                "https://www.elastic.co/guide/en/starting-with-the-elasticsearch-platform-and-its-solutions/current/getting-started-appsearch.html"
              ],
              "applies_to": {
                "stack": "ga",
                "serverless": {
                  "elasticsearch": "ga"
                }
              },
              "description": "Synchronize data from a PostgreSQL data source into Elasticsearch.",
              "content": "# Set up a self-managed PostgreSQL connector\n\nElastic connectors enable you to create searchable, read-only replicas of your data sources in {{es}}.\nThis tutorial walks you through the process of creating a self-managed connector for a PostgreSQL data source.\n\nIf you want to deploy a self-managed connector for another data source, use this tutorial as a blueprint. Refer to the list of available [connectors](/reference/search-connectors/index.md).\n\n::::{tip}\nWant to get started quickly testing a self-managed connector and a self-managed cluster using Docker Compose? Refer to the [readme](https://github.com/elastic/connectors/tree/main/scripts/stack#readme) in the `elastic/connectors` repo for more information.\n::::\n\n## Prerequisites [es-postgresql-connector-client-tutorial-prerequisites]\n\n### Elastic prerequisites [es-postgresql-connector-client-tutorial-prerequisites-elastic]\n\nYou must satisfy the [prerequisites](/reference/search-connectors/self-managed-connectors.md#es-build-connector-prerequisites) for self-managed connectors.\n\n<!--\nTBD: If you're using {{es-serverless}}, you must have a `developer` or `admin` predefined role or an equivalent custom role to add the connector. \n-->\n\n### PostgreSQL prerequisites [es-postgresql-connector-client-tutorial-postgresql-prerequisites]\n\nYou need:\n\n* PostgreSQL version 11+.\n* Tables must be owned by a PostgreSQL user.\n* Database `superuser` privileges are required to index all database tables.\n\n::::{tip}\nYou should enable recording of the commit time of PostgreSQL transactions. Otherwise, *all* data will be indexed in every sync. By default, `track_commit_timestamp` is `off`.\n\nEnable this by running the following command on the PosgreSQL server command line:\n\n```shell\nALTER SYSTEM SET track_commit_timestamp = on;\n```\n\nThen restart the PostgreSQL server.\n\n::::\n\n## Set up the connector\n\n:::::{stepper}\n::::{step} Create an Elasticsearch index\nTo store data in {{es}}, the connector needs to create an index.\nBy default, connectors use [dynamic mappings](docs-content://manage-data/data-store/mapping.md#mapping-dynamic) to automatically infer the data types of your fields.\nIf you [use APIs](/reference/search-connectors/api-tutorial.md) or {{es-serverless}}, you can create an index with the desired field mappings and index settings before you create the connector.\nDefining your own mappings upfront gives you more control over how your data is indexed.\n\nNavigate to **{{index-manage-app}}** or use the [global search field](docs-content://explore-analyze/find-and-organize/find-apps-and-objects.md).\nFollow the index creation workflow then optionally define field mappings.\nFor example, to add semantic search capabilities, you could add an extra field that stores your vectors for semantic search.\n::::\n::::{step} Create the connector\nNavigate to **{{connectors-app}}** or use the global search field.\nFollow the connector creation process in the UI. For example:\n\n1. Select **PostgreSQL** from the list of connectors.\n1. Edit the name and description for the connector. This will help your team identify the connector.\n1. Gather configuration details.\n   Before you can proceed to the next step, you need to gather some details about your Elastic deployment:\n\n    * Elasticsearch endpoint:\n      * If you\u2019re an Elastic Cloud user, find your deployment\u2019s Elasticsearch endpoint in the Cloud UI under **Cloud > Deployments > <your-deployment> > Elasticsearch**.\n      * If you\u2019re running your Elastic deployment and the connector service in Docker, the default Elasticsearch endpoint is `http://host.docker.internal:9200`.\n    * API key: You\u2019ll need this key to configure the connector. Use an existing key or create a new one.\n    * Connector ID: Your unique connector ID is automatically generated when you create the connector.\n::::\n::::{step} Run the connector service\nYou must run the connector code on your own infrastructure and link it to {{es}}.\nYou have two options: [Run with Docker](/reference/search-connectors/es-connectors-run-from-docker.md) and [Run from source](/reference/search-connectors/es-connectors-run-from-source.md).\nFor this example, we\u2019ll use the latter method:\n\n1. Clone or fork the repository locally with the following command: `git clone https://github.com/elastic/connectors`.\n1. Open the `config.yml` configuration file in your editor of choice.\n1. Replace the values for `host`, `api_key`, and `connector_id` with the values you gathered earlier. Use the `service_type` value `postgresql` for this connector.\n\n    :::{dropdown} Expand to see an example config.yml file\n    Replace the values for `host`, `api_key`, and `connector_id` with your own values.\n    Use the `service_type` value `postgresql` for this connector.\n\n    ```yaml\n    elasticsearch:\n      host: \"<YOUR-ELASTIC-URL>>\" # Your Elasticsearch endpoint\n      api_key: \"<YOUR-API-KEY>\" # Your top-level Elasticsearch API key\n    ...\n    connectors:\n      -\n        connector_id: \"<YOUR-CONNECTOR-ID>\"\n        api_key: \"<YOUR-API-KEY>\" # Your scoped connector index API key (optional). If not provided, the top-level API key is used.\n        service_type: \"postgresql\"\n    \n    sources:\n      # mongodb: connectors.sources.mongo:MongoDataSource\n      # s3: connectors.sources.s3:S3DataSource\n      # dir: connectors.sources.directory:DirectoryDataSource\n      # mysql: connectors.sources.mysql:MySqlDataSource\n      # network_drive: connectors.sources.network_drive:NASDataSource\n      # google_cloud_storage: connectors.sources.google_cloud_storage:GoogleCloudStorageDataSource\n      # azure_blob_storage: connectors.sources.azure_blob_storage:AzureBlobStorageDataSource\n      postgresql: connectors.sources.postgresql:PostgreSQLDataSource\n      # oracle: connectors.sources.oracle:OracleDataSource\n      # sharepoint: connectors.sources.sharepoint:SharepointDataSource\n      # mssql: connectors.sources.mssql:MSSQLDataSource\n      # jira: connectors.sources.jira:JiraDataSource\n    ```\n\n1. Now that you\u2019ve configured the connector code, you can run the connector service. In your terminal or IDE:\n\n    1. `cd` into the root of your `connectors` clone/fork.\n    1. Run the following command: `make run`.\n\nThe connector service should now be running.\nThe UI will let you know that the connector has successfully connected to {{es}}.\n\n:::{tip}\nHere we\u2019re working locally. In production setups, you\u2019ll deploy the connector service to your own infrastructure.\n:::\n::::\n::::{step} Add your data source details\n\nNow your connector instance is up and running, but it doesn\u2019t yet know where to sync data from.\nThe final piece of the puzzle is to configure your connector with details about the PostgreSQL instance.\n\nReturn to **{{connectors-app}}** to complete the connector creation process in the UI.\nEnter the following PostgreSQL instance details:\n\n* **Host**: The server host address for your PostgreSQL instance.\n* **Port**: The port number for your PostgreSQL instance.\n* **Username**: The username of the PostgreSQL account.\n* **Password**: The password for that user.\n* **Database**: The name of the PostgreSQL database.\n* **Schema**: The schema of the PostgreSQL database.\n* **Comma-separated list of tables**: `*` will fetch data from all tables in the configured database.\n\n:::{note}\nConfiguration details are specific to the connector type.\nThe keys and values will differ depending on which third-party data source you\u2019re connecting to.\nRefer to the [](/reference/search-connectors/es-connectors-postgresql.md) for more details.\n:::\n::::\n::::{step} Link your index\nIf you [use APIs](/reference/search-connectors/api-tutorial.md) or {{es-serverless}}, you can create an index or choose an existing index for use by the connector.\nOtherwise, the index is created for you and uses dynamic mappings for the fields.\n::::\n:::::\n\n## Sync your data [es-postgresql-connector-client-tutorial-sync-data-source]\n\nIn the **{{connectors-app}}** page, you can launch a sync on-demand or on a schedule.\nThe connector will traverse the database and synchronize documents to your index.\n\nIf you navigate to the terminal window where you\u2019re running the connector service, after a sync occurs you should see output like the following:\n\n```shell\n[FMWK][13:22:26][INFO] Fetcher <create: 499 update: 0 |delete: 0>\n[FMWK][13:22:26][INF0] Fetcher <create: 599 update: 0 |delete: 0>\n[FMWK][13:22:26][INFO] Fetcher <create: 699 update: 0 |delete: 0>\n...\n[FMWK][23:22:28][INF0] [oRXQwYYBLhXTs-qYpJ9i] Sync done: 3864 indexed, 0 deleted.\n(27 seconds)\n```\n\nThis confirms the connector has fetched records from your PostgreSQL tables and transformed them into documents in your {{es}} index.\n\nIf you verify your {{es}} documents and you\u2019re happy with the results, set a recurring sync schedule.\nThis will ensure your searchable data in {{es}} is always up to date with changes to your PostgreSQL data source.\n\nIn **{{connectors-app}}**, click on the connector, and then click **Scheduling**.\nFor example, you can schedule your content to be synchronized at the top of every hour, as long as the connector is up and running.\n\n## Next steps\n\nYou just learned how to synchronize data from an external database to {{es}}.\nFor an overview of how to start searching and analyzing your data in Kibana, go to [Explore and analyze](docs-content://explore-analyze/index.md).\n\nLearn more:\n\n* [Overview of self-managed connectors and frameworks](/reference/search-connectors/self-managed-connectors.md)\n* [Elastic connector framework repository](https://github.com/elastic/connectors/tree/main)\n* [Elastic PostgreSQL connector reference](/reference/search-connectors/es-connectors-postgresql.md)\n* [Overview of all Elastic connectors](/reference/search-connectors/index.md)",
              "filename": "elasticsearch-main/docs/reference/search-connectors/es-postgresql-connector-client-tutorial.md"
            }
          ],
          "tool_call_id": "call_OzbhGIthdUkp9LdQ0AZwCy8s",
          "metadata": null,
          "timestamp": "2025-10-03T05:08:41.827051+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "To deploy the GraphQL connector using Docker, you'll need to follow a series of well-defined steps. Here\u2019s an overview of the process along with the preliminary steps involved:\n\n### Preliminary Steps:\n\n1. **Pre-requisites**:\n   - Ensure you have Docker installed on your machine.\n   - Install and run Elasticsearch and Kibana. This can be done using Docker or other methods.\n\n2. **Download Sample Configuration File**:\n   - You need to download a sample configuration file for the GraphQL connector. You can either do this manually or use the following command:\n     ```bash\n     curl https://raw.githubusercontent.com/elastic/connectors/main/config.yml.example --output ~/connectors-config/config.yml\n     ```\n   - Ensure the output path is correct, or modify it accordingly.\n\n3. **Update the Configuration File**:\n   - Edit the `config.yml` file that you downloaded to include your specific settings:\n     ```yaml\n     elasticsearch.host: http://host.docker.internal:9200\n     elasticsearch.api_key: <YOUR_ELASTICSEARCH_API_KEY>\n\n     connectors:\n       -\n         connector_id: <YOUR_CONNECTOR_ID_FROM_KIBANA>\n         service_type: graphql\n         api_key: <YOUR_CONNECTOR_API_KEY_FROM_KIBANA> # Optional\n     ```\n   - Here, make sure to replace the placeholders with your actual Elasticsearch host, API key, and connector ID.\n\n### Deployment Steps:\n\n4. **Run the Docker Image**:\n   - Start the connector service with the following command:\n     ```bash\n     docker run \\\n     -v ~/connectors-config:/config \\\n     --network \"elastic\" \\\n     --tty \\\n     --rm \\\n     docker.elastic.co/integrations/elastic-connectors:{{version.stack}} \\\n     /app/bin/elastic-ingest \\\n     -c /config/config.yml\n     ```\n   - Replace `{{version.stack}}` with the appropriate version tag based on the image you are using.\n\n### Additional Information:\n- For further details, you may refer to the official documentation on deploying the GraphQL connector with Docker [here](https://github.com/elastic/connectors/tree/main/docs/DOCKER.md).\n\nThe entire deployment process ensures that your GraphQL connector is set up correctly and is ready to sync data with Elasticsearch.\n\nFor full instructions, see the source: [GraphQL Connector Reference](https://github.com/elastic/elasticsearch/blob/main/docs/reference/search-connectors/es-connectors-graphql.md).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 10575,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 504,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T05:08:42+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMSK2jDPKQf2tpNLRgZvgFir3tkr8",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}