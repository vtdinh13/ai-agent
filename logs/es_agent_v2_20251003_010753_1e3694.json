{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How do character filters work in Elasticsearch and what are some examples of built-in filters?",
          "timestamp": "2025-10-03T01:07:49.923624+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\": \"character filters Elasticsearch\"}",
          "tool_call_id": "call_5bcYp93dc5zDDGV2JltoC0wJ",
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"built-in character filters Elasticsearch\"}",
          "tool_call_id": "call_ZxKL0yVUJCKSoysh8CupIrht",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 218,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 48,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T01:07:50+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMOYwodLU1Ez2QCIQPR4wpaJBkDN0",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "content": "# Reference",
              "filename": "elasticsearch-main/docs/reference/index.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html"
              ],
              "content": "# Character filter reference [analysis-charfilters]\n\n*Character filters* are used to preprocess the stream of characters before it is passed to the [tokenizer](/reference/text-analysis/tokenizer-reference.md).\n\nA character filter receives the original text as a stream of characters and can transform the stream by adding, removing, or changing characters. For instance, a character filter could be used to convert Hindu-Arabic numerals (\u0660\u200e\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u200e\u0669\u200e) into their Arabic-Latin equivalents (0123456789), or to strip HTML elements like `<b>` from the stream.\n\nElasticsearch has a number of built in character filters which can be used to build [custom analyzers](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md).\n\n[HTML Strip Character Filter](/reference/text-analysis/analysis-htmlstrip-charfilter.md)\n:   The `html_strip` character filter strips out HTML elements like `<b>` and decodes HTML entities like `&amp;`.\n\n[Mapping Character Filter](/reference/text-analysis/analysis-mapping-charfilter.md)\n:   The `mapping` character filter replaces any occurrences of the specified strings with the specified replacements.\n\n[Pattern Replace Character Filter](/reference/text-analysis/analysis-pattern-replace-charfilter.md)\n:   The `pattern_replace` character filter replaces any characters matching a regular expression with the specified replacement.",
              "filename": "elasticsearch-main/docs/reference/text-analysis/character-filter-reference.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html"
              ],
              "content": "# Analyzer reference [analysis-analyzers]\n\nElasticsearch ships with a wide range of built-in analyzers, which can be used in any index without further configuration:\n\n[Standard Analyzer](/reference/text-analysis/analysis-standard-analyzer.md)\n:   The `standard` analyzer divides text into terms on word boundaries, as defined by the Unicode Text Segmentation algorithm. It removes most punctuation, lowercases terms, and supports removing stop words.\n\n[Simple Analyzer](/reference/text-analysis/analysis-simple-analyzer.md)\n:   The `simple` analyzer divides text into terms whenever it encounters a character which is not a letter. It lowercases all terms.\n\n[Whitespace Analyzer](/reference/text-analysis/analysis-whitespace-analyzer.md)\n:   The `whitespace` analyzer divides text into terms whenever it encounters any whitespace character. It does not lowercase terms.\n\n[Stop Analyzer](/reference/text-analysis/analysis-stop-analyzer.md)\n:   The `stop` analyzer is like the `simple` analyzer, but also supports removal of stop words.\n\n[Keyword Analyzer](/reference/text-analysis/analysis-keyword-analyzer.md)\n:   The `keyword` analyzer is a noop analyzer that accepts whatever text it is given and outputs the exact same text as a single term.\n\n[Pattern Analyzer](/reference/text-analysis/analysis-pattern-analyzer.md)\n:   The `pattern` analyzer uses a regular expression to split the text into terms. It supports lower-casing and stop words.\n\n[Language Analyzers](/reference/text-analysis/analysis-lang-analyzer.md)\n:   Elasticsearch provides many language-specific analyzers like `english` or `french`.\n\n[Fingerprint Analyzer](/reference/text-analysis/analysis-fingerprint-analyzer.md)\n:   The `fingerprint` analyzer is a specialist analyzer which creates a fingerprint which can be used for duplicate detection.\n\n\n## Custom analyzers [_custom_analyzers]\n\nIf you do not find an analyzer suitable for your needs, you can create a [`custom`](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md) analyzer which combines the appropriate [character filters](/reference/text-analysis/character-filter-reference.md), [tokenizer](/reference/text-analysis/tokenizer-reference.md), and [token filters](/reference/text-analysis/token-filter-reference.md).",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analyzer-reference.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html"
              ],
              "content": "# Full text queries [full-text-queries]\n\nThe full text queries enable you to search [analyzed text fields](docs-content://manage-data/data-store/text-analysis.md) such as the body of an email. The query string is processed using the same analyzer that was applied to the field during indexing.\n\nThe queries in this group are:\n\n[`intervals` query](/reference/query-languages/query-dsl/query-dsl-intervals-query.md)\n:   A full text query that allows fine-grained control of the ordering and proximity of matching terms.\n\n[`match` query](/reference/query-languages/query-dsl/query-dsl-match-query.md)\n:   The standard query for performing full text queries, including fuzzy matching and phrase or proximity queries.\n\n[`match_bool_prefix` query](/reference/query-languages/query-dsl/query-dsl-match-bool-prefix-query.md)\n:   Creates a `bool` query that matches each term as a `term` query, except for the last term, which is matched as a `prefix` query\n\n[`match_phrase` query](/reference/query-languages/query-dsl/query-dsl-match-query-phrase.md)\n:   Like the `match` query but used for matching exact phrases or word proximity matches.\n\n[`match_phrase_prefix` query](/reference/query-languages/query-dsl/query-dsl-match-query-phrase-prefix.md)\n:   Like the `match_phrase` query, but does a wildcard search on the final word.\n\n[`multi_match` query](/reference/query-languages/query-dsl/query-dsl-multi-match-query.md)\n:   The multi-field version of the `match` query.\n\n[`combined_fields` query](/reference/query-languages/query-dsl/query-dsl-combined-fields-query.md)\n:   Matches over multiple fields as if they had been indexed into one combined field.\n\n[`query_string` query](/reference/query-languages/query-dsl/query-dsl-query-string-query.md)\n:   Supports the compact Lucene [query string syntax](/reference/query-languages/query-dsl/query-dsl-query-string-query.md#query-string-syntax), allowing you to specify AND|OR|NOT conditions and multi-field search within a single query string. For expert users only.\n\n[`simple_query_string` query](/reference/query-languages/query-dsl/query-dsl-simple-query-string-query.md)\n:   A simpler, more robust version of the `query_string` syntax suitable for exposing directly to users.",
              "filename": "elasticsearch-main/docs/reference/query-languages/query-dsl/full-text-queries.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis.html"
              ],
              "content": "# Analysis plugins [analysis]\n\nAnalysis plugins extend Elasticsearch by adding new analyzers, tokenizers, token filters, or character filters to Elasticsearch.\n\n\n## Core analysis plugins [_core_analysis_plugins]\n\nThe core analysis plugins are:\n\n[ICU](/reference/elasticsearch-plugins/analysis-icu.md)\n:   Adds extended Unicode support using the [ICU](http://site.icu-project.org/) libraries, including better analysis of Asian languages, Unicode normalization, Unicode-aware case folding, collation support, and transliteration.\n\n[Kuromoji](/reference/elasticsearch-plugins/analysis-kuromoji.md)\n:   Advanced analysis of Japanese using the [Kuromoji analyzer](https://www.atilika.org/).\n\n[Nori](/reference/elasticsearch-plugins/analysis-nori.md)\n:   Morphological analysis of Korean using the Lucene Nori analyzer.\n\n[Phonetic](/reference/elasticsearch-plugins/analysis-phonetic.md)\n:   Analyzes tokens into their phonetic equivalent using Soundex, Metaphone, Caverphone, and other codecs.\n\n[SmartCN](/reference/elasticsearch-plugins/analysis-smartcn.md)\n:   An analyzer for Chinese or mixed Chinese-English text. This analyzer uses probabilistic knowledge to find the optimal word segmentation for Simplified Chinese text. The text is first broken into sentences, then each sentence is segmented into words.\n\n[Stempel](/reference/elasticsearch-plugins/analysis-stempel.md)\n:   Provides high quality stemming for Polish.\n\n[Ukrainian](/reference/elasticsearch-plugins/analysis-ukrainian.md)\n:   Provides stemming for Ukrainian.\n\n\n## Community contributed analysis plugins [_community_contributed_analysis_plugins]\n\nA number of analysis plugins have been contributed by our community:\n\n* [IK Analysis Plugin](https://github.com/medcl/elasticsearch-analysis-ik) (by Medcl)\n* [Pinyin Analysis Plugin](https://github.com/medcl/elasticsearch-analysis-pinyin) (by Medcl)\n* [Vietnamese Analysis Plugin](https://github.com/duydo/elasticsearch-analysis-vietnamese) (by Duy Do)\n* [STConvert Analysis Plugin](https://github.com/medcl/elasticsearch-analysis-stconvert) (by Medcl)",
              "filename": "elasticsearch-main/docs/reference/elasticsearch-plugins/analysis-plugins.md"
            }
          ],
          "tool_call_id": "call_5bcYp93dc5zDDGV2JltoC0wJ",
          "metadata": null,
          "timestamp": "2025-10-03T01:07:52.946172+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html"
              ],
              "content": "# Character filter reference [analysis-charfilters]\n\n*Character filters* are used to preprocess the stream of characters before it is passed to the [tokenizer](/reference/text-analysis/tokenizer-reference.md).\n\nA character filter receives the original text as a stream of characters and can transform the stream by adding, removing, or changing characters. For instance, a character filter could be used to convert Hindu-Arabic numerals (\u0660\u200e\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u200e\u0669\u200e) into their Arabic-Latin equivalents (0123456789), or to strip HTML elements like `<b>` from the stream.\n\nElasticsearch has a number of built in character filters which can be used to build [custom analyzers](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md).\n\n[HTML Strip Character Filter](/reference/text-analysis/analysis-htmlstrip-charfilter.md)\n:   The `html_strip` character filter strips out HTML elements like `<b>` and decodes HTML entities like `&amp;`.\n\n[Mapping Character Filter](/reference/text-analysis/analysis-mapping-charfilter.md)\n:   The `mapping` character filter replaces any occurrences of the specified strings with the specified replacements.\n\n[Pattern Replace Character Filter](/reference/text-analysis/analysis-pattern-replace-charfilter.md)\n:   The `pattern_replace` character filter replaces any characters matching a regular expression with the specified replacement.",
              "filename": "elasticsearch-main/docs/reference/text-analysis/character-filter-reference.md"
            },
            {
              "navigation_title": "Mapping",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html"
              ],
              "content": "# Mapping character filter [analysis-mapping-charfilter]\n\n\nThe `mapping` character filter accepts a map of keys and values. Whenever it encounters a string of characters that is the same as a key, it replaces them with the value associated with that key.\n\nMatching is greedy; the longest pattern matching at a given point wins. Replacements are allowed to be the empty string.\n\nThe `mapping` filter uses Lucene\u2019s [MappingCharFilter](https://lucene.apache.org/core/10_0_0/analysis/common/org/apache/lucene/analysis/charfilter/MappingCharFilter.md).\n\n## Example [analysis-mapping-charfilter-analyze-ex]\n\nThe following [analyze API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-analyze) request uses the `mapping` filter to convert Hindu-Arabic numerals (\u0660\u200e\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u200e\u0669\u200e) into their Arabic-Latin equivalents (0123456789), changing the text `My license plate is \u0662\u0665\u0660\u0661\u0665` to `My license plate is 25015`.\n\n```console\nGET /_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [\n    {\n      \"type\": \"mapping\",\n      \"mappings\": [\n        \"\u0660 => 0\",\n        \"\u0661 => 1\",\n        \"\u0662 => 2\",\n        \"\u0663 => 3\",\n        \"\u0664 => 4\",\n        \"\u0665 => 5\",\n        \"\u0666 => 6\",\n        \"\u0667 => 7\",\n        \"\u0668 => 8\",\n        \"\u0669 => 9\"\n      ]\n    }\n  ],\n  \"text\": \"My license plate is \u0662\u0665\u0660\u0661\u0665\"\n}\n```\n\nThe filter produces the following text:\n\n```text\n[ My license plate is 25015 ]\n```\n\n\n## Configurable parameters [analysis-mapping-charfilter-configure-parms]\n\n`mappings`\n:   (Required*, array of strings) Array of mappings, with each element having the form `key => value`.\n\n    Either this or the `mappings_path` parameter must be specified.\n\n\n`mappings_path`\n:   (Required*, string) Path to a file containing `key => value` mappings.\n\n    This path must be absolute or relative to the `config` location, and the file must be UTF-8 encoded. Each mapping in the file must be separated by a line break.\n\n    Either this or the `mappings` parameter must be specified.\n\n\n\n## Customize and add to an analyzer [analysis-mapping-charfilter-customize]\n\nTo customize the `mappings` filter, duplicate it to create the basis for a new custom character filter. You can modify the filter using its configurable parameters.\n\nThe following [create index API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-create) request configures a new [custom analyzer](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md) using a custom `mappings` filter, `my_mappings_char_filter`.\n\nThe `my_mappings_char_filter` filter replaces the `:)` and `:(` emoticons with a text equivalent.\n\n```console\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"my_mappings_char_filter\"\n          ]\n        }\n      },\n      \"char_filter\": {\n        \"my_mappings_char_filter\": {\n          \"type\": \"mapping\",\n          \"mappings\": [\n            \":) => _happy_\",\n            \":( => _sad_\"\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\nThe following [analyze API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-analyze) request uses the custom `my_mappings_char_filter` to replace `:(` with `_sad_` in the text `I'm delighted about it :(`.\n\n```console\nGET /my-index-000001/_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [ \"my_mappings_char_filter\" ],\n  \"text\": \"I'm delighted about it :(\"\n}\n```\n\nThe filter produces the following text:\n\n```text\n[ I'm delighted about it _sad_ ]\n```",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analysis-mapping-charfilter.md"
            },
            {
              "navigation_title": "ASCII folding",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-asciifolding-tokenfilter.html"
              ],
              "content": "# ASCII folding token filter [analysis-asciifolding-tokenfilter]\n\n\nConverts alphabetic, numeric, and symbolic characters that are not in the Basic Latin Unicode block (first 127 ASCII characters) to their ASCII equivalent, if one exists. For example, the filter changes `\u00e0` to `a`.\n\nThis filter uses Lucene\u2019s [ASCIIFoldingFilter](https://lucene.apache.org/core/10_0_0/analysis/common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html).\n\n## Example [analysis-asciifolding-tokenfilter-analyze-ex]\n\nThe following [analyze API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-analyze) request uses the `asciifolding` filter to drop the diacritical marks in `a\u00e7a\u00ed \u00e0 la carte`:\n\n```console\nGET /_analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"asciifolding\"],\n  \"text\" : \"a\u00e7a\u00ed \u00e0 la carte\"\n}\n```\n\nThe filter produces the following tokens:\n\n```text\n[ acai, a, la, carte ]\n```\n\n\n## Add to an analyzer [analysis-asciifolding-tokenfilter-analyzer-ex]\n\nThe following [create index API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-create) request uses the `asciifolding` filter to configure a new [custom analyzer](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md).\n\n```console\nPUT /asciifold_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_asciifolding\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"asciifolding\" ]\n        }\n      }\n    }\n  }\n}\n```\n\n\n## Configurable parameters [analysis-asciifolding-tokenfilter-configure-parms]\n\n`preserve_original`\n:   (Optional, Boolean) If `true`, emit both original tokens and folded tokens. Defaults to `false`.\n\n\n## Customize [analysis-asciifolding-tokenfilter-customize]\n\nTo customize the `asciifolding` filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters.\n\nFor example, the following request creates a custom `asciifolding` filter with `preserve_original` set to true:\n\n```console\nPUT /asciifold_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_asciifolding\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"my_ascii_folding\" ]\n        }\n      },\n      \"filter\": {\n        \"my_ascii_folding\": {\n          \"type\": \"asciifolding\",\n          \"preserve_original\": true\n        }\n      }\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analysis-asciifolding-tokenfilter.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-normalizers.html"
              ],
              "content": "# Normalizers [analysis-normalizers]\n\nNormalizers are similar to analyzers except that they may only emit a single token. As a consequence, they do not have a tokenizer and only accept a subset of the available char filters and token filters. Only the filters that work on a per-character basis are allowed. For instance a lowercasing filter would be allowed, but not a stemming filter, which needs to look at the keyword as a whole. The current list of filters that can be used in a normalizer definition are: `arabic_normalization`, `asciifolding`, `bengali_normalization`, `cjk_width`, `decimal_digit`, `elision`, `german_normalization`, `hindi_normalization`, `indic_normalization`, `lowercase`, `pattern_replace`, `persian_normalization`, `scandinavian_folding`, `serbian_normalization`, `sorani_normalization`, `trim`, `uppercase`.\n\nElasticsearch ships with a `lowercase` built-in normalizer. For other forms of normalization, a custom configuration is required.\n\n\n## Custom normalizers [_custom_normalizers]\n\nCustom normalizers take a list of [character filters](/reference/text-analysis/character-filter-reference.md) and a list of [token filters](/reference/text-analysis/token-filter-reference.md).\n\n```console\nPUT index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"char_filter\": {\n        \"quote\": {\n          \"type\": \"mapping\",\n          \"mappings\": [\n            \"\u00ab => \\\"\",\n            \"\u00bb => \\\"\"\n          ]\n        }\n      },\n      \"normalizer\": {\n        \"my_normalizer\": {\n          \"type\": \"custom\",\n          \"char_filter\": [\"quote\"],\n          \"filter\": [\"lowercase\", \"asciifolding\"]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"foo\": {\n        \"type\": \"keyword\",\n        \"normalizer\": \"my_normalizer\"\n      }\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/text-analysis/normalizers.md"
            },
            {
              "navigation_title": "Uppercase",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-uppercase-tokenfilter.html"
              ],
              "content": "# Uppercase token filter [analysis-uppercase-tokenfilter]\n\n\nChanges token text to uppercase. For example, you can use the `uppercase` filter to change `the Lazy DoG` to `THE LAZY DOG`.\n\nThis filter uses Lucene\u2019s [UpperCaseFilter](https://lucene.apache.org/core/10_0_0/analysis/common/org/apache/lucene/analysis/core/UpperCaseFilter.md).\n\n::::{warning}\nDepending on the language, an uppercase character can map to multiple lowercase characters. Using the `uppercase` filter could result in the loss of lowercase character information.\n\nTo avoid this loss but still have a consistent letter case, use the [`lowercase`](/reference/text-analysis/analysis-lowercase-tokenfilter.md) filter instead.\n\n::::\n\n\n## Example [analysis-uppercase-tokenfilter-analyze-ex]\n\nThe following [analyze API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-analyze) request uses the default `uppercase` filter to change the `the Quick FoX JUMPs` to uppercase:\n\n```console\nGET _analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"uppercase\"],\n  \"text\" : \"the Quick FoX JUMPs\"\n}\n```\n\nThe filter produces the following tokens:\n\n```text\n[ THE, QUICK, FOX, JUMPS ]\n```\n\n\n## Add to an analyzer [analysis-uppercase-tokenfilter-analyzer-ex]\n\nThe following [create index API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-create) request uses the `uppercase` filter to configure a new [custom analyzer](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md).\n\n```console\nPUT uppercase_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_uppercase\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"uppercase\" ]\n        }\n      }\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analysis-uppercase-tokenfilter.md"
            }
          ],
          "tool_call_id": "call_ZxKL0yVUJCKSoysh8CupIrht",
          "metadata": null,
          "timestamp": "2025-10-03T01:07:52.946193+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "In Elasticsearch, **character filters** are used to preprocess a stream of characters before it is sent to the tokenizer. This means they can add, remove, or transform characters in the input text. For instance, a character filter could replace certain numeric characters with their Arabic-Latin equivalents or strip out HTML tags from a text. They are useful for cleaning or modifying input data to enhance the indexing process.\n\n### Examples of Built-In Character Filters:\n1. **HTML Strip Character Filter**:\n   - Strips out HTML elements and decodes HTML entities. For instance, it will convert `<b>` tags to plain text and decode `&amp;` to `&`.\n   - [HTML Strip Character Filter Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-htmlstrip-charfilter.html)\n\n2. **Mapping Character Filter**:\n   - Replaces specific character sequences with other specified character sequences. For example, it can convert Hindu-Arabic numerals (like \\u0660, \\u0661) to their Latin equivalents (0, 1).\n   - [Mapping Character Filter Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html)\n\n3. **Pattern Replace Character Filter**:\n   - Uses regular expressions to find and replace patterns in the text. This can be useful for more complex transformations based on text patterns.\n   - [Pattern Replace Character Filter Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-replace-charfilter.html)\n\nThese character filters can be combined with tokenizers and token filters to create custom analyzers tailored to specific requirements in analyzing and indexing text.\n\nFor more detail, you can refer to the character filter reference documentation here: [Character Filter Reference](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 5657,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 378,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T01:07:53+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMOYzococUsdL6m02gb9Wt1WEXxiI",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}