{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "What methods are available to order buckets in an Elasticsearch aggregation, and can you give an example?",
          "timestamp": "2025-10-03T04:06:59.319151+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"order buckets in Elasticsearch aggregation methods example\"}",
          "tool_call_id": "call_Y7Yl0wVN7xWev5PA6BxUTOjd",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 220,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 19,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:06:59+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMRMJ8TRtj87I1h4tpVbqNrWCrs45",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "Bucket sort",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-bucket-sort-aggregation.html"
              ],
              "content": "# Bucket sort aggregation [search-aggregations-pipeline-bucket-sort-aggregation]\n\n\nA parent pipeline aggregation which sorts the buckets of its parent multi-bucket aggregation. Zero or more sort fields may be specified together with the corresponding sort order. Each bucket may be sorted based on its `_key`, `_count` or its sub-aggregations. In addition, parameters `from` and `size` may be set in order to truncate the result buckets.\n\n::::{note}\nThe `bucket_sort` aggregation, like all pipeline aggregations, is executed after all other non-pipeline aggregations. This means the sorting only applies to whatever buckets are already returned from the parent aggregation. For example, if the parent aggregation is `terms` and its `size` is set to `10`, the `bucket_sort` will only sort over those 10 returned term buckets.\n::::\n\n\n## Syntax [_syntax_10]\n\nA `bucket_sort` aggregation looks like this in isolation:\n\n```js\n{\n  \"bucket_sort\": {\n    \"sort\": [\n      { \"sort_field_1\": { \"order\": \"asc\" } },   <1>\n      { \"sort_field_2\": { \"order\": \"desc\" } },\n      \"sort_field_3\"\n    ],\n    \"from\": 1,\n    \"size\": 3\n  }\n}\n```\n\n1. Here, `sort_field_1` is the bucket path to the variable to be used as the primary sort and its order is ascending.\n\n\n$$$bucket-sort-params$$$\n\n| Parameter Name | Description | Required | Default Value |\n| --- | --- | --- | --- |\n| `sort` | The list of fields to sort on. See [`sort`](/reference/elasticsearch/rest-apis/sort-search-results.md) for more details. | Optional |  |\n| `from` | Buckets in positions prior to the set value will be truncated. | Optional | `0` |\n| `size` | The number of buckets to return. Defaults to all buckets of the parent aggregation. | Optional |  |\n| `gap_policy` | The policy to apply when gaps are found in the data (see [Dealing with gaps in the data](/reference/aggregations/pipeline.md#gap-policy) for more details) | Optional | `skip` |\n\nThe following snippet returns the buckets corresponding to the 3 months with the highest total sales in descending order:\n\n```console\nPOST /sales/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"sales_per_month\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"month\"\n      },\n      \"aggs\": {\n        \"total_sales\": {\n          \"sum\": {\n            \"field\": \"price\"\n          }\n        },\n        \"sales_bucket_sort\": {\n          \"bucket_sort\": {\n            \"sort\": [\n              { \"total_sales\": { \"order\": \"desc\" } } <1>\n            ],\n            \"size\": 3                                <2>\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n1. `sort` is set to use the values of `total_sales` in descending order\n2. `size` is set to `3` meaning only the top 3 months in `total_sales` will be returned\n\n\nAnd the following may be the response:\n\n```console-result\n{\n   \"took\": 82,\n   \"timed_out\": false,\n   \"_shards\": ...,\n   \"hits\": ...,\n   \"aggregations\": {\n      \"sales_per_month\": {\n         \"buckets\": [\n            {\n               \"key_as_string\": \"2015/01/01 00:00:00\",\n               \"key\": 1420070400000,\n               \"doc_count\": 3,\n               \"total_sales\": {\n                   \"value\": 550.0\n               }\n            },\n            {\n               \"key_as_string\": \"2015/03/01 00:00:00\",\n               \"key\": 1425168000000,\n               \"doc_count\": 2,\n               \"total_sales\": {\n                   \"value\": 375.0\n               }\n            },\n            {\n               \"key_as_string\": \"2015/02/01 00:00:00\",\n               \"key\": 1422748800000,\n               \"doc_count\": 2,\n               \"total_sales\": {\n                   \"value\": 60.0\n               }\n            }\n         ]\n      }\n   }\n}\n```\n\n\n## Truncating without sorting [_truncating_without_sorting]\n\nIt is also possible to use this aggregation in order to truncate the result buckets without doing any sorting. To do so, just use the `from` and/or `size` parameters without specifying `sort`.\n\nThe following example simply truncates the result so that only the second bucket is returned:\n\n```console\nPOST /sales/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"sales_per_month\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"month\"\n      },\n      \"aggs\": {\n        \"bucket_truncate\": {\n          \"bucket_sort\": {\n            \"from\": 1,\n            \"size\": 1\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n   \"took\": 11,\n   \"timed_out\": false,\n   \"_shards\": ...,\n   \"hits\": ...,\n   \"aggregations\": {\n      \"sales_per_month\": {\n         \"buckets\": [\n            {\n               \"key_as_string\": \"2015/02/01 00:00:00\",\n               \"key\": 1422748800000,\n               \"doc_count\": 2\n            }\n         ]\n      }\n   }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-pipeline-bucket-sort-aggregation.md"
            },
            {
              "navigation_title": "Terms",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html"
              ],
              "content": "# Terms aggregation [search-aggregations-bucket-terms-aggregation]\n\n\nA multi-bucket value source based aggregation where buckets are dynamically built - one per unique value.\n\nExample:\n\n$$$terms-aggregation-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": { \"field\": \"genre\" }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"genres\": {\n      \"doc_count_error_upper_bound\": 0,   <1>\n      \"sum_other_doc_count\": 0,           <2>\n      \"buckets\": [                        <3>\n        {\n          \"key\": \"electronic\",\n          \"doc_count\": 6\n        },\n        {\n          \"key\": \"rock\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n```\n\n1. an upper bound of the error on the document counts for each term, see [below](#terms-agg-doc-count-error)\n2. when there are lots of unique terms, Elasticsearch only returns the top terms; this number is the sum of the document counts for all buckets that are not part of the response\n3. the list of the top buckets, the meaning of `top` being defined by the [order](#search-aggregations-bucket-terms-aggregation-order)\n\n\n$$$search-aggregations-bucket-terms-aggregation-types$$$\nThe `field` can be [Keyword](/reference/elasticsearch/mapping-reference/keyword.md), [Numeric](/reference/elasticsearch/mapping-reference/number.md), [`ip`](/reference/elasticsearch/mapping-reference/ip.md), [`boolean`](/reference/elasticsearch/mapping-reference/boolean.md), or [`binary`](/reference/elasticsearch/mapping-reference/binary.md).\n\n::::{note}\nBy default, you cannot run a `terms` aggregation on a `text` field. Use a `keyword` [sub-field](/reference/elasticsearch/mapping-reference/multi-fields.md) instead. Alternatively, you can enable [`fielddata`](/reference/elasticsearch/mapping-reference/text.md#fielddata-mapping-param) on the `text` field to create buckets for the field\u2019s [analyzed](docs-content://manage-data/data-store/text-analysis.md) terms. Enabling `fielddata` can significantly increase memory usage.\n::::\n\n\n## Size [search-aggregations-bucket-terms-aggregation-size]\n\nBy default, the `terms` aggregation returns the top ten terms with the most documents. Use the `size` parameter to return more terms, up to the [search.max_buckets](/reference/elasticsearch/configuration-reference/search-settings.md#search-settings-max-buckets) limit.\n\nIf your data contains 100 or 1000 unique terms, you can increase the `size` of the `terms` aggregation to return them all. If you have more unique terms and you need them all, use the [composite aggregation](/reference/aggregations/search-aggregations-bucket-composite-aggregation.md) instead.\n\nLarger values of `size` use more memory to compute and, push the whole aggregation close to the `max_buckets` limit. You\u2019ll know you\u2019ve gone too large if the request fails with a message about `max_buckets`.\n\n\n## Shard size [search-aggregations-bucket-terms-aggregation-shard-size]\n\nTo get more accurate results, the `terms` agg fetches more than the top `size` terms from each shard. It fetches the top `shard_size` terms, which defaults to `size * 1.5 + 10`.\n\nThis is to handle the case when one term has many documents on one shard but is just below the `size` threshold on all other shards. If each shard only returned `size` terms, the aggregation would return an partial doc count for the term. So `terms` returns more terms in an attempt to catch the missing terms. This helps, but it\u2019s still quite possible to return a partial doc count for a term. It just takes a term with more disparate per-shard doc counts.\n\nYou can increase `shard_size` to better account for these disparate doc counts and improve the accuracy of the selection of top terms. It is much cheaper to increase the `shard_size` than to increase the `size`. However, it still takes more bytes over the wire and waiting in memory on the coordinating node.\n\n::::{important}\nThis guidance only applies if you\u2019re using the `terms` aggregation\u2019s default sort `order`. If you\u2019re sorting by anything other than document count in descending order, see [Order](#search-aggregations-bucket-terms-aggregation-order).\n::::\n\n\n::::{note}\n`shard_size` cannot be smaller than `size` (as it doesn\u2019t make much sense). When it is, Elasticsearch will override it and reset it to be equal to `size`.\n::::\n\n\n\n## Document count error [terms-agg-doc-count-error]\n\nEven with a larger `shard_size` value, `doc_count` values for a `terms` aggregation may be approximate. As a result, any sub-aggregations on the `terms` aggregation may also be approximate.\n\n`sum_other_doc_count` is the number of documents that didn\u2019t make it into the the top `size` terms. If this is greater than `0`, you can be sure that the `terms` agg had to throw away some buckets, either because they didn\u2019t fit into `size` on the coordinating node or they didn\u2019t fit into `shard_size` on the data node.\n\n\n## Per bucket document count error [_per_bucket_document_count_error]\n\nIf you set the `show_term_doc_count_error` parameter to `true`, the `terms` aggregation will include `doc_count_error_upper_bound`, which is an upper bound to the error on the `doc_count` returned by each shard. It\u2019s the sum of the size of the largest bucket on each shard that didn\u2019t fit into `shard_size`.\n\nIn more concrete terms, imagine there is one bucket that is very large on one shard and just outside the `shard_size` on all the other shards. In that case, the `terms` agg will return the bucket because it is large, but it\u2019ll be missing data from many documents on the shards where the term fell below the `shard_size` threshold. `doc_count_error_upper_bound` is the maximum number of those missing documents.\n\n$$$terms-aggregation-doc-count-error-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"products\": {\n      \"terms\": {\n        \"field\": \"product\",\n        \"size\": 5,\n        \"show_term_doc_count_error\": true\n      }\n    }\n  }\n}\n```\n\nThese errors can only be calculated in this way when the terms are ordered by descending document count. When the aggregation is ordered by the terms values themselves (either ascending or descending) there is no error in the document count since if a shard does not return a particular term which appears in the results from another shard, it must not have that term in its index. When the aggregation is either sorted by a sub aggregation or in order of ascending document count, the error in the document counts cannot be determined and is given a value of -1 to indicate this.\n\n\n## Order [search-aggregations-bucket-terms-aggregation-order]\n\nBy default, the `terms` aggregation orders terms by descending document `_count`.  This produces a bounded [document count](#terms-agg-doc-count-error) error that {{es}} can report.\n\nYou can use the `order` parameter to specify a different sort order, but we don\u2019t recommend it.  It is extremely easy to create a terms ordering that will just return wrong results, and not obvious to see when you have done so. Change this only with caution.\n\n::::{warning}\nEspecially avoid using `\"order\": { \"_count\": \"asc\" }`. If you need to find rare terms, use the [`rare_terms`](/reference/aggregations/search-aggregations-bucket-rare-terms-aggregation.md) aggregation instead. Due to the way the `terms` aggregation [gets terms from shards](#search-aggregations-bucket-terms-aggregation-shard-size), sorting by ascending doc count often produces inaccurate results.\n::::\n\n\n### Ordering by the term value [_ordering_by_the_term_value]\n\nIn this case, the buckets are ordered by the actual term values, such as lexicographic order for keywords or numerically for numbers. This sorting is safe in both ascending and descending directions, and produces accurate results.\n\nExample of ordering the buckets alphabetically by their terms in an ascending manner:\n\n$$$terms-aggregation-asc-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"_key\": \"asc\" }\n      }\n    }\n  }\n}\n```\n\n\n### Ordering by a sub aggregation [_ordering_by_a_sub_aggregation]\n\n::::{warning}\nSorting by a sub aggregation generally produces incorrect ordering, due to the way the `terms` aggregation [gets results from shards](#search-aggregations-bucket-terms-aggregation-shard-size).\n::::\n\n\nThere are two cases when sub-aggregation ordering is safe and returns correct results: sorting by a maximum in descending order, or sorting by a minimum in ascending order. These approaches work because they align with the behavior of sub aggregations. That is, if you\u2019re looking for the largest maximum or the smallest minimum, the global answer (from combined shards) must be included in one of the local shard answers. Conversely, the smallest maximum and largest minimum wouldn\u2019t be accurately computed.\n\nNote also that in these cases, the ordering is correct but the doc counts and non-ordering sub aggregations may still have errors (and {{es}} does not calculate a bound for those errors).\n\nOrdering the buckets by single value metrics sub-aggregation (identified by the aggregation name):\n\n$$$terms-aggregation-subaggregation-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"max_play_count\": \"desc\" }\n      },\n      \"aggs\": {\n        \"max_play_count\": { \"max\": { \"field\": \"play_count\" } }\n      }\n    }\n  }\n}\n```\n\nOrdering the buckets by multi value metrics sub-aggregation (identified by the aggregation name):\n\n$$$terms-aggregation-multivalue-subaggregation-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"playback_stats.max\": \"desc\" }\n      },\n      \"aggs\": {\n        \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n      }\n    }\n  }\n}\n```\n\n::::{admonition} Pipeline aggs cannot be used for sorting\n:class: note\n\n[Pipeline aggregations](/reference/aggregations/pipeline.md) are run during the reduce phase after all other aggregations have already completed. For this reason, they cannot be used for ordering.\n\n::::\n\n\nIt is also possible to order the buckets based on a \"deeper\" aggregation in the hierarchy. This is supported as long as the aggregations path are of a single-bucket type, where the last aggregation in the path may either be a single-bucket one or a metrics one. If it\u2019s a single-bucket type, the order will be defined by the number of docs in the bucket (i.e. `doc_count`), in case it\u2019s a metrics one, the same rules as above apply (where the path must indicate the metric name to sort by in case of a multi-value metrics aggregation, and in case of a single-value metrics aggregation the sort will be applied on that value).\n\nThe path must be defined in the following form:\n\n```ebnf\nAGG_SEPARATOR       =  '>' ;\nMETRIC_SEPARATOR    =  '.' ;\nAGG_NAME            =  <the name of the aggregation> ;\nMETRIC              =  <the name of the metric (in case of multi-value metrics aggregation)> ;\nPATH                =  <AGG_NAME> [ <AGG_SEPARATOR>, <AGG_NAME> ]* [ <METRIC_SEPARATOR>, <METRIC> ] ;\n```\n\n$$$terms-aggregation-hierarchy-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"countries\": {\n      \"terms\": {\n        \"field\": \"artist.country\",\n        \"order\": { \"rock>playback_stats.avg\": \"desc\" }\n      },\n      \"aggs\": {\n        \"rock\": {\n          \"filter\": { \"term\": { \"genre\": \"rock\" } },\n          \"aggs\": {\n            \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe above will sort the artist\u2019s countries buckets based on the average play count among the rock songs.\n\nMultiple criteria can be used to order the buckets by providing an array of order criteria such as the following:\n\n$$$terms-aggregation-multicriteria-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"countries\": {\n      \"terms\": {\n        \"field\": \"artist.country\",\n        \"order\": [ { \"rock>playback_stats.avg\": \"desc\" }, { \"_count\": \"desc\" } ]\n      },\n      \"aggs\": {\n        \"rock\": {\n          \"filter\": { \"term\": { \"genre\": \"rock\" } },\n          \"aggs\": {\n            \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe above will sort the artist\u2019s countries buckets based on the average play count among the rock songs and then by their `doc_count` in descending order.\n\n::::{note}\nIn the event that two buckets share the same values for all order criteria the bucket\u2019s term value is used as a tie-breaker in ascending alphabetical order to prevent non-deterministic ordering of buckets.\n::::\n\n\n\n### Ordering by count ascending [_ordering_by_count_ascending]\n\nOrdering terms by ascending document `_count` produces an unbounded error that {{es}} can\u2019t accurately report. We therefore strongly recommend against using `\"order\": { \"_count\": \"asc\" }` as shown in the following example:\n\n$$$terms-aggregation-count-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"_count\": \"asc\" }\n      }\n    }\n  }\n}\n```\n\n\n\n## Minimum document count [_minimum_document_count_4]\n\nIt is possible to only return terms that match more than a configured number of hits using the `min_doc_count` option:\n\n$$$terms-aggregation-min-doc-count-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"min_doc_count\": 10\n      }\n    }\n  }\n}\n```\n\nThe above aggregation would only return tags which have been found in 10 hits or more. Default value is `1`.\n\nTerms are collected and ordered on a shard level and merged with the terms collected from other shards in a second step. However, the shard does not have the information about the global document count available. The decision if a term is added to a candidate list depends only on the order computed on the shard using local shard frequencies. The `min_doc_count` criterion is only applied after merging local terms statistics of all shards. In a way the decision to add the term as a candidate is made without being very *certain* about if the term will actually reach the required `min_doc_count`. This might cause many (globally) high frequent terms to be missing in the final result if low frequent terms populated the candidate lists. To avoid this, the `shard_size` parameter can be increased to allow more candidate terms on the shards. However, this increases memory consumption and network traffic.\n\n### `shard_min_doc_count` [search-aggregations-bucket-terms-shard-min-doc-count]\n\nThe parameter `shard_min_doc_count` regulates the *certainty* a shard has if the term should actually be added to the candidate list or not with respect to the `min_doc_count`. Terms will only be considered if their local shard frequency within the set is higher than the `shard_min_doc_count`. If your dictionary contains many low frequent terms and you are not interested in those (for example misspellings), then you can set the `shard_min_doc_count` parameter to filter out candidate terms on a shard level that will with a reasonable certainty not reach the required `min_doc_count` even after merging the local counts. `shard_min_doc_count` is set to `0` per default and has no effect unless you explicitly set it.\n\n::::{note}\nSetting `min_doc_count`=`0` will also return buckets for terms that didn\u2019t match any hit. However, some of the returned terms which have a document count of zero might only belong to deleted documents or documents from other types, so there is no warranty that a `match_all` query would find a positive document count for those terms.\n::::\n\n\n::::{warning}\nWhen NOT sorting on `doc_count` descending, high values of `min_doc_count` may return a number of buckets which is less than `size` because not enough data was gathered from the shards. Missing buckets can be back by increasing `shard_size`. Setting `shard_min_doc_count` too high will cause terms to be filtered out on a shard level. This value should be set much lower than `min_doc_count/#shards`.\n::::\n\n\n\n\n## Script [search-aggregations-bucket-terms-aggregation-script]\n\nUse a [runtime field](docs-content://manage-data/data-store/mapping/runtime-fields.md) if the data in your documents doesn\u2019t exactly match what you\u2019d like to aggregate. If, for example, \"anthologies\" need to be in a special category then you could run this:\n\n$$$terms-aggregation-script-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"runtime_mappings\": {\n    \"normalized_genre\": {\n      \"type\": \"keyword\",\n      \"script\": \"\"\"\n        String genre = doc['genre'].value;\n        if (doc['product'].value.startsWith('Anthology')) {\n          emit(genre + ' anthology');\n        } else {\n          emit(genre);\n        }\n      \"\"\"\n    }\n  },\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"normalized_genre\"\n      }\n    }\n  }\n}\n```\n\nWhich will look like:\n\n```console-result\n{\n  \"aggregations\": {\n    \"genres\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"electronic\",\n          \"doc_count\": 4\n        },\n        {\n          \"key\": \"rock\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"electronic anthology\",\n          \"doc_count\": 2\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  },\n  ...\n}\n```\n\nThis is a little slower because the runtime field has to access two fields instead of one and because there are some optimizations that work on non-runtime `keyword` fields that we have to give up for for runtime `keyword` fields. If you need the speed, you can index the `normalized_genre` field.\n\n\n## Filtering Values [_filtering_values_4]\n\nIt is possible to filter the values for which buckets will be created. This can be done using the `include` and `exclude` parameters which are based on regular expression strings or arrays of exact values. Additionally, `include` clauses can filter using `partition` expressions.\n\n### Filtering Values with regular expressions [_filtering_values_with_regular_expressions_2]\n\n$$$terms-aggregation-regex-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"include\": \".*sport.*\",\n        \"exclude\": \"water_.*\"\n      }\n    }\n  }\n}\n```\n\nIn the above example, buckets will be created for all the tags that has the word `sport` in them, except those starting with `water_` (so the tag `water_sports` will not be aggregated). The `include` regular expression will determine what values are \"allowed\" to be aggregated, while the `exclude` determines the values that should not be aggregated. When both are defined, the `exclude` has precedence, meaning, the `include` is evaluated first and only then the `exclude`.\n\nThe syntax is the same as [regexp queries](/reference/query-languages/query-dsl/regexp-syntax.md).\n\n\n### Filtering Values with exact values [_filtering_values_with_exact_values_2]\n\nFor matching based on exact values the `include` and `exclude` parameters can simply take an array of strings that represent the terms as they are found in the index:\n\n$$$terms-aggregation-exact-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"JapaneseCars\": {\n      \"terms\": {\n        \"field\": \"make\",\n        \"include\": [ \"mazda\", \"honda\" ]\n      }\n    },\n    \"ActiveCarManufacturers\": {\n      \"terms\": {\n        \"field\": \"make\",\n        \"exclude\": [ \"rover\", \"jensen\" ]\n      }\n    }\n  }\n}\n```\n\n\n### Filtering Values with partitions [_filtering_values_with_partitions]\n\nSometimes there are too many unique terms to process in a single request/response pair so it can be useful to break the analysis up into multiple requests. This can be achieved by grouping the field\u2019s values into a number of partitions at query-time and processing only one partition in each request. Consider this request which is looking for accounts that have not logged any access recently:\n\n$$$terms-aggregation-partitions-example$$$\n\n```console\nGET /_search\n{\n   \"size\": 0,\n   \"aggs\": {\n      \"expired_sessions\": {\n         \"terms\": {\n            \"field\": \"account_id\",\n            \"include\": {\n               \"partition\": 0,\n               \"num_partitions\": 20\n            },\n            \"size\": 10000,\n            \"order\": {\n               \"last_access\": \"asc\"\n            }\n         },\n         \"aggs\": {\n            \"last_access\": {\n               \"max\": {\n                  \"field\": \"access_date\"\n               }\n            }\n         }\n      }\n   }\n}\n```\n\nThis request is finding the last logged access date for a subset of customer accounts because we might want to expire some customer accounts who haven\u2019t been seen for a long while. The `num_partitions` setting has requested that the unique account_ids are organized evenly into twenty partitions (0 to 19). and the `partition` setting in this request filters to only consider account_ids falling into partition 0. Subsequent requests should ask for partitions 1 then 2 etc to complete the expired-account analysis.\n\nNote that the `size` setting for the number of results returned needs to be tuned with the `num_partitions`. For this particular account-expiration example the process for balancing values for `size` and `num_partitions` would be as follows:\n\n1. Use the `cardinality` aggregation to estimate the total number of unique account_id values\n2. Pick a value for `num_partitions` to break the number from 1) up into more manageable chunks\n3. Pick a `size` value for the number of responses we want from each partition\n4. Run a test request\n\nIf we have a circuit-breaker error we are trying to do too much in one request and must increase `num_partitions`. If the request was successful but the last account ID in the date-sorted test response was still an account we might want to expire then we may be missing accounts of interest and have set our numbers too low. We must either\n\n* increase the `size` parameter to return more results per partition (could be heavy on memory) or\n* increase the `num_partitions` to consider less accounts per request (could increase overall processing time as we need to make more requests)\n\nUltimately this is a balancing act between managing the Elasticsearch resources required to process a single request and the volume of requests that the client application must issue to complete a task.\n\n::::{warning}\nPartitions cannot be used together with an `exclude` parameter.\n::::\n\n\n\n\n## Multi-field terms aggregation [_multi_field_terms_aggregation]\n\nThe `terms` aggregation does not support collecting terms from multiple fields in the same document. The reason is that the `terms` agg doesn\u2019t collect the string term values themselves, but rather uses [global ordinals](#search-aggregations-bucket-terms-aggregation-execution-hint) to produce a list of all of the unique values in the field. Global ordinals results in an important performance boost which would not be possible across multiple fields.\n\nThere are three approaches that you can use to perform a `terms` agg across multiple fields:\n\n[Script](#search-aggregations-bucket-terms-aggregation-script)\n:   Use a script to retrieve terms from multiple fields. This disables the global ordinals optimization and will be slower than collecting terms from a single field, but it gives you the flexibility to implement this option at search time.\n\n[`copy_to` field](/reference/elasticsearch/mapping-reference/copy-to.md)\n:   If you know ahead of time that you want to collect the terms from two or more fields, then use `copy_to` in your mapping to create a new dedicated field at index time which contains the values from both fields. You can aggregate on this single field, which will benefit from the global ordinals optimization.\n\n[`multi_terms` aggregation](/reference/aggregations/search-aggregations-bucket-multi-terms-aggregation.md)\n:   Use multi_terms aggregation to combine terms from multiple fields into a compound key. This also disables the global ordinals and will be slower than collecting terms from a single field. It is faster but less flexible than using a script.\n\n\n## Collect mode [search-aggregations-bucket-terms-aggregation-collect]\n\nDeferring calculation of child aggregations\n\nFor fields with many unique terms and a small number of required results it can be more efficient to delay the calculation of child aggregations until the top parent-level aggs have been pruned. Ordinarily, all branches of the aggregation tree are expanded in one depth-first pass and only then any pruning occurs. In some scenarios this can be very wasteful and can hit memory constraints. An example problem scenario is querying a movie database for the 10 most popular actors and their 5 most common co-stars:\n\n$$$terms-aggregation-collect-mode-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"actors\": {\n      \"terms\": {\n        \"field\": \"actors\",\n        \"size\": 10\n      },\n      \"aggs\": {\n        \"costars\": {\n          \"terms\": {\n            \"field\": \"actors\",\n            \"size\": 5\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nEven though the number of actors may be comparatively small and we want only 50 result buckets there is a combinatorial explosion of buckets during calculation - a single actor can produce n\u00b2 buckets where n is the number of actors. The sane option would be to first determine the 10 most popular actors and only then examine the top co-stars for these 10 actors. This alternative strategy is what we call the `breadth_first` collection mode as opposed to the `depth_first` mode.\n\n::::{note}\nThe `breadth_first` is the default mode for fields with a cardinality bigger than the requested size or when the cardinality is unknown (numeric fields or scripts for instance). It is possible to override the default heuristic and to provide a collect mode directly in the request:\n::::\n\n\n$$$terms-aggregation-breadth-first-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"actors\": {\n      \"terms\": {\n        \"field\": \"actors\",\n        \"size\": 10,\n        \"collect_mode\": \"breadth_first\" <1>\n      },\n      \"aggs\": {\n        \"costars\": {\n          \"terms\": {\n            \"field\": \"actors\",\n            \"size\": 5\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n1. the possible values are `breadth_first` and `depth_first`\n\n\nWhen using `breadth_first` mode the set of documents that fall into the uppermost buckets are cached for subsequent replay so there is a memory overhead in doing this which is linear with the number of matching documents. Note that the `order` parameter can still be used to refer to data from a child aggregation when using the `breadth_first` setting - the parent aggregation understands that this child aggregation will need to be called first before any of the other child aggregations.\n\n::::{warning}\nNested aggregations such as `top_hits` which require access to score information under an aggregation that uses the `breadth_first` collection mode need to replay the query on the second pass but only for the documents belonging to the top buckets.\n::::\n\n\n\n## Execution hint [search-aggregations-bucket-terms-aggregation-execution-hint]\n\nThere are different mechanisms by which terms aggregations can be executed:\n\n* by using field values directly in order to aggregate data per-bucket (`map`)\n* by using global ordinals of the field and allocating one bucket per global ordinal (`global_ordinals`)\n\nElasticsearch tries to have sensible defaults so this is something that generally doesn\u2019t need to be configured.\n\n`global_ordinals` is the default option for `keyword` field, it uses global ordinals to allocates buckets dynamically so memory usage is linear to the number of values of the documents that are part of the aggregation scope.\n\n`map` should only be considered when very few documents match a query. Otherwise the ordinals-based execution mode is significantly faster. By default, `map` is only used when running an aggregation on scripts, since they don\u2019t have ordinals.\n\n$$$terms-aggregation-execution-hint-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"execution_hint\": \"map\" <1>\n      }\n    }\n  }\n}\n```\n\n1. The possible values are `map`, `global_ordinals`\n\n\nPlease note that Elasticsearch will ignore this execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.\n\n\n## Missing value [_missing_value_5]\n\nThe `missing` parameter defines how documents that are missing a value should be treated. By default they will be ignored but it is also possible to treat them as if they had a value.\n\n$$$terms-aggregation-missing-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"missing\": \"N/A\" <1>\n      }\n    }\n  }\n}\n```\n\n1. Documents without a value in the `tags` field will fall into the same bucket as documents that have the value `N/A`.\n\n\n\n## Mixing field types [_mixing_field_types_2]\n\n::::{warning}\nWhen aggregating on multiple indices the type of the aggregated field may not be the same in all indices. Some types are compatible with each other (`integer` and `long` or `float` and `double`) but when the types are a mix of decimal and non-decimal number the terms aggregation will promote the non-decimal numbers to decimal numbers. This can result in a loss of precision in the bucket values.\n::::\n\n\n\n### Troubleshooting [search-aggregations-bucket-terms-aggregation-troubleshooting]\n\n### Failed Trying to Format Bytes [_failed_trying_to_format_bytes]\n\nWhen running a terms aggregation (or other aggregation, but in practice usually terms) over multiple indices, you may get an error that starts with \"Failed trying to format bytes\u2026 \".  This is usually caused by two of the indices not having the same mapping type for the field being aggregated.\n\n**Use an explicit `value_type`** Although it\u2019s best to correct the mappings, you can work around this issue if the field is unmapped in one of the indices.  Setting the `value_type` parameter can resolve the issue by coercing the unmapped field into the correct type.\n\n$$$terms-aggregation-value_type-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"ip_addresses\": {\n      \"terms\": {\n        \"field\": \"destination_ip\",\n        \"missing\": \"0.0.0.0\",\n        \"value_type\": \"ip\"\n      }\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-terms-aggregation.md"
            },
            {
              "navigation_title": "Composite",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-composite-aggregation.html"
              ],
              "content": "# Composite aggregation [search-aggregations-bucket-composite-aggregation]\n\n\n::::{warning}\nThe composite aggregation is expensive. Load test your application before deploying a composite aggregation in production.\n::::\n\n\nA multi-bucket aggregation that creates composite buckets from different sources.\n\nUnlike the other `multi-bucket` aggregations, you can use the `composite` aggregation to paginate **all** buckets from a multi-level aggregation efficiently. This aggregation provides a way to stream **all** buckets of a specific aggregation, similar to what [scroll](/reference/elasticsearch/rest-apis/paginate-search-results.md#scroll-search-results) does for documents.\n\nThe composite buckets are built from the combinations of the values extracted/created for each document and each combination is considered as a composite bucket.\n\nFor example, consider the following document:\n\n```js\n{\n  \"keyword\": [\"foo\", \"bar\"],\n  \"number\": [23, 65, 76]\n}\n```\n\nUsing `keyword` and `number` as source fields for the aggregation results in the following composite buckets:\n\n```js\n{ \"keyword\": \"foo\", \"number\": 23 }\n{ \"keyword\": \"foo\", \"number\": 65 }\n{ \"keyword\": \"foo\", \"number\": 76 }\n{ \"keyword\": \"bar\", \"number\": 23 }\n{ \"keyword\": \"bar\", \"number\": 65 }\n{ \"keyword\": \"bar\", \"number\": 76 }\n```\n\n## Value sources [_value_sources]\n\nThe `sources` parameter defines the source fields to use when building composite buckets. The order that the `sources` are defined controls the order that the keys are returned.\n\n::::{note}\nYou must use a unique name when defining `sources`.\n::::\n\n\nThe `sources` parameter can be any of the following types:\n\n* [Terms](#_terms)\n* [Histogram](#_histogram)\n* [Date histogram](#_date_histogram)\n* [GeoTile grid](#_geotile_grid)\n\n### Terms [_terms]\n\nThe `terms` value source is similar to a simple `terms` aggregation. The values are extracted from a field exactly like the `terms` aggregation.\n\nExample:\n\n$$$composite-aggregation-terms-field-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"product\": { \"terms\": { \"field\": \"product\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\nLike the `terms` aggregation, it\u2019s possible to use a [runtime field](docs-content://manage-data/data-store/mapping/runtime-fields.md) to create values for the composite buckets:\n\n$$$composite-aggregation-terms-runtime-field-example$$$\n\n```console\nGET /_search\n{\n  \"runtime_mappings\": {\n    \"day_of_week\": {\n      \"type\": \"keyword\",\n      \"script\": \"\"\"\n        emit(doc['timestamp'].value.dayOfWeekEnum\n          .getDisplayName(TextStyle.FULL, Locale.ENGLISH))\n      \"\"\"\n    }\n  },\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          {\n            \"dow\": {\n              \"terms\": { \"field\": \"day_of_week\" }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\nAlthough similar, the `terms` value source doesn\u2019t support the same set of parameters as the `terms` aggregation. For other supported value source parameters, see:\n\n* [Order](#_order)\n* [Missing bucket](#_missing_bucket)\n\n\n### Histogram [_histogram]\n\nThe `histogram` value source can be applied on numeric values to build fixed size interval over the values. The `interval` parameter defines how the numeric values should be transformed. For instance an `interval` set to 5 will translate any numeric values to its closest interval, a value of `101` would be translated to `100` which is the key for the interval between 100 and 105.\n\nExample:\n\n$$$composite-aggregation-histogram-field-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"histo\": { \"histogram\": { \"field\": \"price\", \"interval\": 5 } } }\n        ]\n      }\n    }\n  }\n}\n```\n\nLike the `histogram` aggregation it\u2019s possible to use a [runtime field](docs-content://manage-data/data-store/mapping/runtime-fields.md) to create values for the composite buckets:\n\n$$$composite-aggregation-histogram-runtime-field-example$$$\n\n```console\nGET /_search\n{\n  \"runtime_mappings\": {\n    \"price.discounted\": {\n      \"type\": \"double\",\n      \"script\": \"\"\"\n        double price = doc['price'].value;\n        if (doc['product'].value == 'mad max') {\n          price *= 0.8;\n        }\n        emit(price);\n      \"\"\"\n    }\n  },\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          {\n            \"price\": {\n              \"histogram\": {\n                \"interval\": 5,\n                \"field\": \"price.discounted\"\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n\n### Date histogram [_date_histogram]\n\nThe `date_histogram` is similar to the `histogram` value source except that the interval is specified by date/time expression:\n\n$$$composite-aggregation-datehistogram-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\nThe example above creates an interval per day and translates all `timestamp` values to the start of its closest intervals. Available expressions for interval: `year`, `quarter`, `month`, `week`, `day`, `hour`, `minute`, `second`\n\nTime values can also be specified via abbreviations supported by [time units](/reference/elasticsearch/rest-apis/api-conventions.md#time-units) parsing. Note that fractional time values are not supported, but you can address this by shifting to another time unit (e.g., `1.5h` could instead be specified as `90m`).\n\n**Format**\n\nInternally, a date is represented as a 64 bit number representing a timestamp in milliseconds-since-the-epoch. These timestamps are returned as the bucket keys. It is possible to return a formatted date string instead using the format specified with the format parameter:\n\n$$$composite-aggregation-datehistogram-format-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          {\n            \"date\": {\n              \"date_histogram\": {\n                \"field\": \"timestamp\",\n                \"calendar_interval\": \"1d\",\n                \"format\": \"yyyy-MM-dd\"         <1>\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n1. Supports expressive date [format pattern](/reference/aggregations/search-aggregations-bucket-daterange-aggregation.md#date-format-pattern)\n\n\n**Time Zone**\n\nDate-times are stored in Elasticsearch in UTC. By default, all bucketing and rounding is also done in UTC. The `time_zone` parameter can be used to indicate that bucketing should use a different time zone.\n\nTime zones may either be specified as an ISO 8601 UTC offset (e.g. `+01:00` or `-08:00`)  or as a timezone id, an identifier used in the TZ database like `America/Los_Angeles`.\n\n**Offset**\n\nUse the `offset` parameter to change the start value of each bucket by the specified positive (`+`) or negative offset (`-`) duration, such as `1h` for an hour, or `1d` for a day. See [Time units](/reference/elasticsearch/rest-apis/api-conventions.md#time-units) for more possible time duration options.\n\nFor example, when using an interval of `day`, each bucket runs from midnight to midnight. Setting the `offset` parameter to `+6h` changes each bucket to run from 6am to 6am:\n\n$$$composite-aggregation-datehistogram-offset-example$$$\n\n```console\nPUT my-index-000001/_doc/1?refresh\n{\n  \"date\": \"2015-10-01T05:30:00Z\"\n}\n\nPUT my-index-000001/_doc/2?refresh\n{\n  \"date\": \"2015-10-01T06:30:00Z\"\n}\n\nGET my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\" : {\n        \"sources\" : [\n          {\n            \"date\": {\n              \"date_histogram\" : {\n                \"field\": \"date\",\n                \"calendar_interval\": \"day\",\n                \"offset\": \"+6h\",\n                \"format\": \"iso8601\"\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\nInstead of a single bucket starting at midnight, the above request groups the documents into buckets starting at 6am:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"my_buckets\": {\n      \"after_key\": { \"date\": \"2015-10-01T06:00:00.000Z\" },\n      \"buckets\": [\n        {\n          \"key\": { \"date\": \"2015-09-30T06:00:00.000Z\" },\n          \"doc_count\": 1\n        },\n        {\n          \"key\": { \"date\": \"2015-10-01T06:00:00.000Z\" },\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n```\n\n::::{note}\nThe start `offset` of each bucket is calculated after `time_zone` adjustments have been made.\n::::\n\n\n\n### GeoTile grid [_geotile_grid]\n\nThe `geotile_grid` value source works on `geo_point` fields and groups points into buckets that represent cells in a grid. The resulting grid can be sparse and only contains cells that have matching data. Each cell corresponds to a [map tile](https://en.wikipedia.org/wiki/Tiled_web_map) as used by many online map sites. Each cell is labeled using a `\"{{zoom}}/{x}/{{y}}\"` format, where zoom is equal to the user-specified precision.\n\n$$$composite-aggregation-geotilegrid-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"tile\": { \"geotile_grid\": { \"field\": \"location\", \"precision\": 8 } } }\n        ]\n      }\n    }\n  }\n}\n```\n\n**Precision**\n\nThe highest-precision geotile of length 29 produces cells that cover less than 10cm by 10cm of land. This precision is uniquely suited for composite aggregations as each tile does not have to be generated and loaded in memory.\n\nSee [Zoom level documentation](https://wiki.openstreetmap.org/wiki/Zoom_levels) on how precision (zoom) correlates to size on the ground. Precision for this aggregation can be between 0 and 29, inclusive.\n\n**Bounding box filtering**\n\nThe geotile source can optionally be constrained to a specific geo bounding box, which reduces the range of tiles used. These bounds are useful when only a specific part of a geographical area needs high precision tiling.\n\n$$$composite-aggregation-geotilegrid-boundingbox-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          {\n            \"tile\": {\n              \"geotile_grid\": {\n                \"field\": \"location\",\n                \"precision\": 22,\n                \"bounds\": {\n                  \"top_left\": \"POINT (4.9 52.4)\",\n                  \"bottom_right\": \"POINT (5.0 52.3)\"\n                }\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n\n### Mixing different value sources [_mixing_different_value_sources]\n\nThe `sources` parameter accepts an array of value sources. It is possible to mix different value sources to create composite buckets. For example:\n\n$$$composite-aggregation-mixing-sources-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\" } } },\n          { \"product\": { \"terms\": { \"field\": \"product\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\nThis will create composite buckets from the values created by two value sources, a `date_histogram` and a `terms`. Each bucket is composed of two values, one for each value source defined in the aggregation. Any type of combinations is allowed and the order in the array is preserved in the composite buckets.\n\n$$$composite-aggregation-mixing-three-sources-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"shop\": { \"terms\": { \"field\": \"shop\" } } },\n          { \"product\": { \"terms\": { \"field\": \"product\" } } },\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\n\n\n## Order [_order]\n\nBy default the composite buckets are sorted by their natural ordering. Values are sorted in ascending order of their values. When multiple value sources are requested, the ordering is done per value source, the first value of the composite bucket is compared to the first value of the other composite bucket and if they are equals the next values in the composite bucket are used for tie-breaking. This means that the composite bucket `[foo, 100]` is considered smaller than `[foobar, 0]` because `foo` is considered smaller than `foobar`. It is possible to define the direction of the sort for each value source by setting `order` to `asc` (default value) or `desc` (descending order) directly in the value source definition. For example:\n\n$$$composite-aggregation-order-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\", \"order\": \"desc\" } } },\n          { \"product\": { \"terms\": { \"field\": \"product\", \"order\": \"asc\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\n... will sort the composite bucket in descending order when comparing values from the `date_histogram` source and in ascending order when comparing values from the `terms` source.\n\n\n## Missing bucket [_missing_bucket]\n\nBy default documents without a value for a given source are ignored. It is possible to include them in the response by setting `missing_bucket` to `true` (defaults to `false`):\n\n$$$composite-aggregation-missing-bucket-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [{\n          \"product_name\": {\n            \"terms\": {\n              \"field\": \"product\",\n              \"missing_bucket\": true,\n              \"missing_order\": \"last\"\n            }\n          }\n        }]\n      }\n    }\n  }\n}\n```\n\nIn the above example, the `product_name` source emits an explicit `null` bucket for documents without a `product` value. This bucket is placed last.\n\nYou can control the position of the `null` bucket using the optional `missing_order` parameter. If `missing_order` is `first` or `last`, the `null` bucket is placed in the respective first or last position. If `missing_order` is omitted or `default`, the source\u2019s `order` determines the bucket\u2019s position. If `order` is `asc` (ascending), the bucket is in the first position. If `order` is `desc` (descending), the bucket is in the last position.\n\n\n## Size [_size]\n\nThe `size` parameter can be set to define how many composite buckets should be returned. Each composite bucket is considered as a single bucket, so setting a size of 10 will return the first 10 composite buckets created from the value sources. The response contains the values for each composite bucket in an array containing the values extracted from each value source. Defaults to `10`.\n\n\n## Pagination [_pagination]\n\nIf the number of composite buckets is too high (or unknown) to be returned in a single response it is possible to split the retrieval in multiple requests. Since the composite buckets are flat by nature, the requested `size` is exactly the number of composite buckets that will be returned in the response (assuming that they are at least `size` composite buckets to return). If all composite buckets should be retrieved it is preferable to use a small size (`100` or `1000` for instance) and then use the `after` parameter to retrieve the next results. For example:\n\n$$$composite-aggregation-after-key-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"size\": 2,\n        \"sources\": [\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\" } } },\n          { \"product\": { \"terms\": { \"field\": \"product\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\n... returns:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"my_buckets\": {\n      \"after_key\": {\n        \"date\": 1494288000000,\n        \"product\": \"mad max\"\n      },\n      \"buckets\": [\n        {\n          \"key\": {\n            \"date\": 1494201600000,\n            \"product\": \"rocky\"\n          },\n          \"doc_count\": 1\n        },\n        {\n          \"key\": {\n            \"date\": 1494288000000,\n            \"product\": \"mad max\"\n          },\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n```\n\nTo get the next set of buckets, resend the same aggregation with the `after` parameter set to the `after_key` value returned in the response. For example, this request uses the `after_key` value provided in the previous response:\n\n$$$composite-aggregation-after-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"size\": 2,\n        \"sources\": [\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\", \"order\": \"desc\" } } },\n          { \"product\": { \"terms\": { \"field\": \"product\", \"order\": \"asc\" } } }\n        ],\n        \"after\": { \"date\": 1494288000000, \"product\": \"mad max\" } <1>\n      }\n    }\n  }\n}\n```\n\n1. Should restrict the aggregation to buckets that sort **after** the provided values.\n\n\n::::{note}\nThe `after_key` is **usually** the key to the last bucket returned in the response, but that isn\u2019t guaranteed. Always use the returned `after_key` instead of deriving it from the buckets.\n::::\n\n\n\n## Early termination [_early_termination]\n\nFor optimal performance the [index sort](/reference/elasticsearch/index-settings/sorting.md) should be set on the index so that it matches parts or fully the source order in the composite aggregation. For instance the following index sort:\n\n```console\nPUT my-index-000001\n{\n  \"settings\": {\n    \"index\": {\n      \"sort.field\": [ \"username\", \"timestamp\" ],   <1>\n      \"sort.order\": [ \"asc\", \"desc\" ]              <2>\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"username\": {\n        \"type\": \"keyword\",\n        \"doc_values\": true\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n```\n\n1. This index is sorted by `username` first then by `timestamp`.\n2. \u2026  in ascending order for the `username` field and in descending order for the `timestamp` field.1. could be used to optimize these composite aggregations:\n\n\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"user_name\": { \"terms\": { \"field\": \"user_name\" } } }     <1>\n        ]\n      }\n    }\n  }\n}\n```\n\n1. `user_name` is a prefix of the index sort and the order matches (`asc`).\n\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"user_name\": { \"terms\": { \"field\": \"user_name\" } } }, <1>\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\", \"order\": \"desc\" } } } <2>\n        ]\n      }\n    }\n  }\n}\n```\n\n1. `user_name` is a prefix of the index sort and the order matches (`asc`).\n2. `timestamp` matches also the prefix and the order matches (`desc`).\n\n\nIn order to optimize the early termination it is advised to set `track_total_hits` in the request to `false`. The number of total hits that match the request can be retrieved on the first request and it would be costly to compute this number on every page:\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"track_total_hits\": false,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"user_name\": { \"terms\": { \"field\": \"user_name\" } } },\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\", \"order\": \"desc\" } } }\n        ]\n      }\n    }\n  }\n}\n```\n\nNote that the order of the source is important, in the example below switching the `user_name` with the `timestamp` would deactivate the sort optimization since this configuration wouldn\u2019t match the index sort specification. If the order of sources do not matter for your use case you can follow these simple guidelines:\n\n* Put the fields with the highest cardinality first.\n* Make sure that the order of the field matches the order of the index sort.\n* Put multi-valued fields last since they cannot be used for early termination.\n\n::::{warning}\n[index sort](/reference/elasticsearch/index-settings/sorting.md) can slowdown indexing, it is very important to test index sorting with your specific use case and dataset to ensure that it matches your requirement. If it doesn\u2019t note that `composite` aggregations will also try to early terminate on non-sorted indices if the query matches all document (`match_all` query).\n::::\n\n\n\n## Sub-aggregations [_sub_aggregations]\n\nLike any `multi-bucket` aggregations the `composite` aggregation can hold sub-aggregations. These sub-aggregations can be used to compute other buckets or statistics on each composite bucket created by this parent aggregation. For instance the following example computes the average value of a field per composite bucket:\n\n$$$composite-aggregation-subaggregations-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"my_buckets\": {\n      \"composite\": {\n        \"sources\": [\n          { \"date\": { \"date_histogram\": { \"field\": \"timestamp\", \"calendar_interval\": \"1d\", \"order\": \"desc\" } } },\n          { \"product\": { \"terms\": { \"field\": \"product\" } } }\n        ]\n      },\n      \"aggregations\": {\n        \"the_avg\": {\n          \"avg\": { \"field\": \"price\" }\n        }\n      }\n    }\n  }\n}\n```\n\n... returns:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"my_buckets\": {\n      \"after_key\": {\n        \"date\": 1494201600000,\n        \"product\": \"rocky\"\n      },\n      \"buckets\": [\n        {\n          \"key\": {\n            \"date\": 1494460800000,\n            \"product\": \"apocalypse now\"\n          },\n          \"doc_count\": 1,\n          \"the_avg\": {\n            \"value\": 10.0\n          }\n        },\n        {\n          \"key\": {\n            \"date\": 1494374400000,\n            \"product\": \"mad max\"\n          },\n          \"doc_count\": 1,\n          \"the_avg\": {\n            \"value\": 27.0\n          }\n        },\n        {\n          \"key\": {\n            \"date\": 1494288000000,\n            \"product\": \"mad max\"\n          },\n          \"doc_count\": 2,\n          \"the_avg\": {\n            \"value\": 22.5\n          }\n        },\n        {\n          \"key\": {\n            \"date\": 1494201600000,\n            \"product\": \"rocky\"\n          },\n          \"doc_count\": 1,\n          \"the_avg\": {\n            \"value\": 10.0\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n\n## Pipeline aggregations [search-aggregations-bucket-composite-aggregation-pipeline-aggregations]\n\nThe composite agg is not currently compatible with pipeline aggregations, nor does it make sense in most cases. E.g. due to the paging nature of composite aggs, a single logical partition (one day for example) might be spread over multiple pages. Since pipeline aggregations are purely post-processing on the final list of buckets, running something like a derivative on a composite page could lead to inaccurate results as it is only taking into account a \"partial\" result on that page.\n\nPipeline aggs that are self contained to a single bucket (such as `bucket_selector`) might be supported in the future.",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-composite-aggregation.md"
            },
            {
              "navigation_title": "Filters",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-filters-aggregation.html"
              ],
              "content": "# Filters aggregation [search-aggregations-bucket-filters-aggregation]\n\n\nA multi-bucket aggregation where each bucket contains the documents that match a [query](/reference/query-languages/querydsl.md).\n\nExample:\n\n$$$filters-aggregation-example$$$\n\n```console\nPUT /logs/_bulk?refresh\n{ \"index\" : { \"_id\" : 1 } }\n{ \"body\" : \"warning: page could not be rendered\" }\n{ \"index\" : { \"_id\" : 2 } }\n{ \"body\" : \"authentication error\" }\n{ \"index\" : { \"_id\" : 3 } }\n{ \"body\" : \"warning: connection timed out\" }\n\nGET logs/_search\n{\n  \"size\": 0,\n  \"aggs\" : {\n    \"messages\" : {\n      \"filters\" : {\n        \"filters\" : {\n          \"errors\" :   { \"match\" : { \"body\" : \"error\"   }},\n          \"warnings\" : { \"match\" : { \"body\" : \"warning\" }}\n        }\n      }\n    }\n  }\n}\n```\n\nIn the above example, we analyze log messages. The aggregation will build two collection (buckets) of log messages - one for all those containing an error, and another for all those containing a warning.\n\nResponse:\n\n```console-result\n{\n  \"took\": 9,\n  \"timed_out\": false,\n  \"_shards\": ...,\n  \"hits\": ...,\n  \"aggregations\": {\n    \"messages\": {\n      \"buckets\": {\n        \"errors\": {\n          \"doc_count\": 1\n        },\n        \"warnings\": {\n          \"doc_count\": 2\n        }\n      }\n    }\n  }\n}\n```\n\n## Anonymous filters [anonymous-filters]\n\nThe filters field can also be provided as an array of filters, as in the following request:\n\n$$$filters-aggregation-anonymous-example$$$\n\n```console\nGET logs/_search\n{\n  \"size\": 0,\n  \"aggs\" : {\n    \"messages\" : {\n      \"filters\" : {\n        \"filters\" : [\n          { \"match\" : { \"body\" : \"error\"   }},\n          { \"match\" : { \"body\" : \"warning\" }}\n        ]\n      }\n    }\n  }\n}\n```\n\nThe filtered buckets are returned in the same order as provided in the request. The response for this example would be:\n\n```console-result\n{\n  \"took\": 4,\n  \"timed_out\": false,\n  \"_shards\": ...,\n  \"hits\": ...,\n  \"aggregations\": {\n    \"messages\": {\n      \"buckets\": [\n        {\n          \"doc_count\": 1\n        },\n        {\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n```\n\n\n## `Other` Bucket [other-bucket]\n\nThe `other_bucket` parameter can be set to add a bucket to the response which will contain all documents that do not match any of the given filters. The value of this parameter can be as follows:\n\n`false`\n:   Does not compute the `other` bucket\n\n`true`\n:   Returns the `other` bucket either in a bucket (named `_other_` by default) if named filters are being used, or as the last bucket if anonymous filters are being used\n\nThe `other_bucket_key` parameter can be used to set the key for the `other` bucket to a value other than the default `_other_`. Setting this parameter will implicitly set the `other_bucket` parameter to `true`.\n\nThe following snippet shows a response where the `other` bucket is requested to be named `other_messages`.\n\n$$$filters-aggregation-other-bucket-example$$$\n\n```console\nPUT logs/_doc/4?refresh\n{\n  \"body\": \"info: user Bob logged out\"\n}\n\nGET logs/_search\n{\n  \"size\": 0,\n  \"aggs\" : {\n    \"messages\" : {\n      \"filters\" : {\n        \"other_bucket_key\": \"other_messages\",\n        \"filters\" : {\n          \"errors\" :   { \"match\" : { \"body\" : \"error\"   }},\n          \"warnings\" : { \"match\" : { \"body\" : \"warning\" }}\n        }\n      }\n    }\n  }\n}\n```\n\nThe response would be something like the following:\n\n```console-result\n{\n  \"took\": 3,\n  \"timed_out\": false,\n  \"_shards\": ...,\n  \"hits\": ...,\n  \"aggregations\": {\n    \"messages\": {\n      \"buckets\": {\n        \"errors\": {\n          \"doc_count\": 1\n        },\n        \"warnings\": {\n          \"doc_count\": 2\n        },\n        \"other_messages\": {\n          \"doc_count\": 1\n        }\n      }\n    }\n  }\n}\n```\n\n\n## Non-keyed Response [non-keyed-response]\n\nBy default, the named filters aggregation returns the buckets as an object. But in some sorting cases, such as [bucket sort](/reference/aggregations/search-aggregations-pipeline-bucket-sort-aggregation.md), the JSON doesn\u2019t guarantee the order of elements in the object. You can use the `keyed` parameter to specify the buckets as an array of objects. The value of this parameter can be as follows:\n\n`true`\n:   (Default) Returns the buckets as an object\n\n`false`\n:   Returns the buckets as an array of objects\n\n::::{note}\nThis parameter is ignored by [Anonymous filters](#anonymous-filters).\n::::\n\n\nExample:\n\n$$$filters-aggregation-sortable-example$$$\n\n```console\nPOST /sales/_search?size=0&filter_path=aggregations\n{\n  \"aggs\": {\n    \"the_filter\": {\n      \"filters\": {\n        \"keyed\": false,\n        \"filters\": {\n          \"t-shirt\": { \"term\": { \"type\": \"t-shirt\" } },\n          \"hat\": { \"term\": { \"type\": \"hat\" } }\n        }\n      },\n      \"aggs\": {\n        \"avg_price\": { \"avg\": { \"field\": \"price\" } },\n        \"sort_by_avg_price\": {\n          \"bucket_sort\": { \"sort\": { \"avg_price\": \"asc\" } }\n        }\n      }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n  \"aggregations\": {\n    \"the_filter\": {\n      \"buckets\": [\n        {\n          \"key\": \"t-shirt\",\n          \"doc_count\": 3,\n          \"avg_price\": { \"value\": 128.33333333333334 }\n        },\n        {\n          \"key\": \"hat\",\n          \"doc_count\": 3,\n          \"avg_price\": { \"value\": 150.0 }\n        }\n      ]\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-filters-aggregation.md"
            },
            {
              "navigation_title": "Auto-interval date histogram",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-autodatehistogram-aggregation.html"
              ],
              "content": "# Auto-interval date histogram aggregation [search-aggregations-bucket-autodatehistogram-aggregation]\n\n\nA multi-bucket aggregation similar to the [Date histogram](/reference/aggregations/search-aggregations-bucket-datehistogram-aggregation.md) except instead of providing an interval to use as the width of each bucket, a target number of buckets is provided indicating the number of buckets needed and the interval of the buckets is automatically chosen to best achieve that target. The number of buckets returned will always be less than or equal to this target number.\n\nThe buckets field is optional, and will default to 10 buckets if not specified.\n\nRequesting a target of 10 buckets.\n\n$$$autodatehistogram-aggregation-example$$$\n\n```console\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"auto_date_histogram\": {\n        \"field\": \"date\",\n        \"buckets\": 10\n      }\n    }\n  }\n}\n```\n\n## Keys [_keys]\n\nInternally, a date is represented as a 64 bit number representing a timestamp in milliseconds-since-the-epoch. These timestamps are returned as the bucket `key`s. The `key_as_string` is the same timestamp converted to a formatted date string using the format specified with the `format` parameter:\n\n::::{tip}\nIf no `format` is specified, then it will use the first date [format](/reference/elasticsearch/mapping-reference/mapping-date-format.md) specified in the field mapping.\n::::\n\n\n$$$autodatehistogram-aggregation-format-example$$$\n\n```console\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"auto_date_histogram\": {\n        \"field\": \"date\",\n        \"buckets\": 5,\n        \"format\": \"yyyy-MM-dd\" <1>\n      }\n    }\n  }\n}\n```\n\n1. Supports expressive date [format pattern](/reference/aggregations/search-aggregations-bucket-daterange-aggregation.md#date-format-pattern)\n\n\nResponse:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"sales_over_time\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-01-01\",\n          \"key\": 1420070400000,\n          \"doc_count\": 3\n        },\n        {\n          \"key_as_string\": \"2015-02-01\",\n          \"key\": 1422748800000,\n          \"doc_count\": 2\n        },\n        {\n          \"key_as_string\": \"2015-03-01\",\n          \"key\": 1425168000000,\n          \"doc_count\": 2\n        }\n      ],\n      \"interval\": \"1M\"\n    }\n  }\n}\n```\n\n\n## Intervals [_intervals]\n\nThe interval of the returned buckets is selected based on the data collected by the aggregation so that the number of buckets returned is less than or equal to the number requested. The possible intervals returned are:\n\nseconds\n:   In multiples of 1, 5, 10 and 30\n\nminutes\n:   In multiples of 1, 5, 10 and 30\n\nhours\n:   In multiples of 1, 3 and 12\n\ndays\n:   In multiples of 1, and 7\n\nmonths\n:   In multiples of 1, and 3\n\nyears\n:   In multiples of 1, 5, 10, 20, 50 and 100\n\nIn the worst case, where the number of daily buckets are too many for the requested number of buckets, the number of buckets returned will be 1/7th of the number of buckets requested.\n\n\n## Time Zone [_time_zone]\n\nDate-times are stored in Elasticsearch in UTC. By default, all bucketing and rounding is also done in UTC. The `time_zone` parameter can be used to indicate that bucketing should use a different time zone.\n\nTime zones may either be specified as an ISO 8601 UTC offset (e.g. `+01:00` or `-08:00`)  or as a timezone id, an identifier used in the TZ database like `America/Los_Angeles`.\n\nConsider the following example:\n\n$$$autodatehistogram-aggregation-timezone-example$$$\n\n```console\nPUT my-index-000001/_doc/1?refresh\n{\n  \"date\": \"2015-10-01T00:30:00Z\"\n}\n\nPUT my-index-000001/_doc/2?refresh\n{\n  \"date\": \"2015-10-01T01:30:00Z\"\n}\n\nPUT my-index-000001/_doc/3?refresh\n{\n  \"date\": \"2015-10-01T02:30:00Z\"\n}\n\nGET my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"by_day\": {\n      \"auto_date_histogram\": {\n        \"field\":     \"date\",\n        \"buckets\" : 3\n      }\n    }\n  }\n}\n```\n\nUTC is used if no time zone is specified, three 1-hour buckets are returned starting at midnight UTC on 1 October 2015:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"by_day\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-10-01T00:00:00.000Z\",\n          \"key\": 1443657600000,\n          \"doc_count\": 1\n        },\n        {\n          \"key_as_string\": \"2015-10-01T01:00:00.000Z\",\n          \"key\": 1443661200000,\n          \"doc_count\": 1\n        },\n        {\n          \"key_as_string\": \"2015-10-01T02:00:00.000Z\",\n          \"key\": 1443664800000,\n          \"doc_count\": 1\n        }\n      ],\n      \"interval\": \"1h\"\n    }\n  }\n}\n```\n\nIf a `time_zone` of `-01:00` is specified, then midnight starts at one hour before midnight UTC:\n\n```console\nGET my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"by_day\": {\n      \"auto_date_histogram\": {\n        \"field\":     \"date\",\n        \"buckets\" : 3,\n        \"time_zone\": \"-01:00\"\n      }\n    }\n  }\n}\n```\n\nNow three 1-hour buckets are still returned but the first bucket starts at 11:00pm on 30 September 2015 since that is the local time for the bucket in the specified time zone.\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"by_day\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-09-30T23:00:00.000-01:00\", <1>\n          \"key\": 1443657600000,\n          \"doc_count\": 1\n        },\n        {\n          \"key_as_string\": \"2015-10-01T00:00:00.000-01:00\",\n          \"key\": 1443661200000,\n          \"doc_count\": 1\n        },\n        {\n          \"key_as_string\": \"2015-10-01T01:00:00.000-01:00\",\n          \"key\": 1443664800000,\n          \"doc_count\": 1\n        }\n      ],\n      \"interval\": \"1h\"\n    }\n  }\n}\n```\n\n1. The `key_as_string` value represents midnight on each day in the specified time zone.\n\n\n::::{warning}\nWhen using time zones that follow DST (daylight savings time) changes, buckets close to the moment when those changes happen can have slightly different sizes than neighbouring buckets. For example, consider a DST start in the `CET` time zone: on 27 March 2016 at 2am, clocks were turned forward 1 hour to 3am local time. If the result of the aggregation was daily buckets, the bucket covering that day will only hold data for 23 hours instead of the usual 24 hours for other buckets. The same is true for shorter intervals like e.g. 12h. Here, we will have only a 11h bucket on the morning of 27 March when the DST shift happens.\n::::\n\n\n\n## Minimum Interval parameter [_minimum_interval_parameter]\n\nThe `minimum_interval` allows the caller to specify the minimum rounding interval that should be used. This can make the collection process more efficient, as the aggregation will not attempt to round at any interval lower than `minimum_interval`.\n\nThe accepted units for `minimum_interval` are:\n\n* year\n* month\n* day\n* hour\n* minute\n* second\n\n$$$autodatehistogram-aggregation-minimum-interval-example$$$\n\n```console\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sale_date\": {\n      \"auto_date_histogram\": {\n        \"field\": \"date\",\n        \"buckets\": 10,\n        \"minimum_interval\": \"minute\"\n      }\n    }\n  }\n}\n```\n\n\n## Missing value [_missing_value]\n\nThe `missing` parameter defines how documents that are missing a value should be treated. By default they will be ignored but it is also possible to treat them as if they had a value.\n\n$$$autodatehistogram-aggregation-missing-example$$$\n\n```console\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sale_date\": {\n      \"auto_date_histogram\": {\n        \"field\": \"date\",\n        \"buckets\": 10,\n        \"missing\": \"2000/01/01\" <1>\n      }\n    }\n  }\n}\n```\n\n1. Documents without a value in the `publish_date` field will fall into the same bucket as documents that have the value `2000-01-01`.",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-autodatehistogram-aggregation.md"
            }
          ],
          "tool_call_id": "call_Y7Yl0wVN7xWev5PA6BxUTOjd",
          "metadata": null,
          "timestamp": "2025-10-03T04:07:00.789340+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "In Elasticsearch, there are several methods available to order buckets in an aggregation:\n\n1. **Bucket Sort Aggregation**: This aggregation allows you to sort the buckets of a parent multi-bucket aggregation based on one or more specified fields. You can sort buckets by the bucket key, count, or values from sub-aggregations.\n\n   **Example** of a `bucket_sort` aggregation:\n   ```json\n   {\n     \"bucket_sort\": {\n       \"sort\": [\n         { \"total_sales\": { \"order\": \"desc\" } }\n       ],\n       \"size\": 3\n     }\n   }\n   ```\n   This example sorts the buckets based on a sub-aggregation called `total_sales` in descending order, limiting the result to the top 3 buckets. You can read more about this in the [Bucket Sort Aggregation documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-bucket-sort-aggregation.html).\n\n2. **Terms Aggregation Ordering**: Within a terms aggregation, you can specify the order in which buckets should be returned. Buckets can be ordered by:\n   - **Count**: By default, the terms aggregation orders buckets by descending document count. You can also specify ascending order, but this is generally discouraged.\n   - **Key Value**: Buckets can be sorted based on the term values themselves in alphabetical or numerical order.\n\n   **Example** of ordering by term value:\n   ```json\n   {\n     \"aggs\": {\n       \"genres\": {\n         \"terms\": {\n           \"field\": \"genre\",\n           \"order\": { \"_key\": \"asc\" }\n         }\n       }\n     }\n   }\n   ```\n   This example orders the buckets alphabetically by their terms.\n\n3. **Composite Aggregation**: The composite aggregation can also be ordered. It allows for sorting buckets by multiple fields simultaneously.\n\n   **Example** of a composite aggregation with ordering:\n   ```json\n   {\n     \"aggs\": {\n       \"my_buckets\": {\n         \"composite\": {\n           \"sources\": [\n             { \"category\": { \"terms\": { \"field\": \"category\" } } },\n             { \"date\": { \"date_histogram\": { \"field\": \"date\", \"calendar_interval\": \"day\" } } }\n           ],\n           \"order\": {\n              \"category\": \"asc\"\n           }\n         }\n       }\n     }\n   }\n   ```\n\nEach of these methods can help you control how the results of your aggregations are sorted and presented to suit your specific analysis needs.\n\nFor further reading, you can check the detailed documentation for [Bucket Sort Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-bucket-sort-aggregation.html), [Terms Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html), and [Composite Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-composite-aggregation.html).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 20318,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 627,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:07:01+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMRMLhJsVnwH0FiLdyvEij61E6l6v",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}