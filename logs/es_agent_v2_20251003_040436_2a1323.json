{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "What are the different execution mechanisms for terms aggregations in Elasticsearch?",
          "timestamp": "2025-10-03T04:04:34.219494+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"execution mechanisms for terms aggregations\"}",
          "tool_call_id": "call_rFXywSX5NsY5td7xdHVhREqe",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 214,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 18,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:04:34+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMRJy5rh45QkfR83l0LbO8AfE4uLQ",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "Multi Terms",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-multi-terms-aggregation.html"
              ],
              "content": "# Multi Terms aggregation [search-aggregations-bucket-multi-terms-aggregation]\n\n\nA multi-bucket value source based aggregation where buckets are dynamically built - one per unique set of values. The multi terms aggregation is very similar to the [`terms aggregation`](/reference/aggregations/search-aggregations-bucket-terms-aggregation.md#search-aggregations-bucket-terms-aggregation-order), however in most cases it will be slower than the terms aggregation and will consume more memory. Therefore, if the same set of fields is constantly used, it would be more efficient to index a combined key for this fields as a separate field and use the terms aggregation on this field.\n\nThe multi_term aggregations are the most useful when you need to sort by a number of document or a metric aggregation on a composite key and get top N results. If sorting is not required and all values are expected to be retrieved using nested terms aggregation or [`composite aggregations`](/reference/aggregations/search-aggregations-bucket-composite-aggregation.md) will be a faster and more memory efficient solution.\n\nExample:\n\n$$$multi-terms-aggregation-example$$$\n\n```console\nGET /products/_search\n{\n  \"aggs\": {\n    \"genres_and_products\": {\n      \"multi_terms\": {\n        \"terms\": [{\n          \"field\": \"genre\" <1>\n        }, {\n          \"field\": \"product\"\n        }]\n      }\n    }\n  }\n}\n```\n\n1. `multi_terms` aggregation can work with the same field types as a [`terms aggregation`](/reference/aggregations/search-aggregations-bucket-terms-aggregation.md#search-aggregations-bucket-terms-aggregation-order) and supports most of the terms aggregation parameters.\n\n\nResponse:\n\n```console-result\n{\n  ...\n  \"aggregations\" : {\n    \"genres_and_products\" : {\n      \"doc_count_error_upper_bound\" : 0,  <1>\n      \"sum_other_doc_count\" : 0,          <2>\n      \"buckets\" : [                       <3>\n        {\n          \"key\" : [                       <4>\n            \"rock\",\n            \"Product A\"\n          ],\n          \"key_as_string\" : \"rock|Product A\",\n          \"doc_count\" : 2\n        },\n        {\n          \"key\" : [\n            \"electronic\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"electronic|Product B\",\n          \"doc_count\" : 1\n        },\n        {\n          \"key\" : [\n            \"jazz\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"jazz|Product B\",\n          \"doc_count\" : 1\n        },\n        {\n          \"key\" : [\n            \"rock\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"rock|Product B\",\n          \"doc_count\" : 1\n        }\n      ]\n    }\n  }\n}\n```\n\n1. an upper bound of the error on the document counts for each term, see <<search-aggregations-bucket-multi-terms-aggregation-approximate-counts,below>\n2. when there are lots of unique terms, Elasticsearch only returns the top terms; this number is the sum of the document counts for all buckets that are not part of the response\n3. the list of the top buckets.\n4. the keys are arrays of values ordered the same ways as expression in the `terms` parameter of the aggregation\n\n\nBy default, the `multi_terms` aggregation will return the buckets for the top ten terms ordered by the `doc_count`. One can change this default behaviour by setting the `size` parameter.\n\n## Aggregation Parameters [search-aggregations-bucket-multi-terms-aggregation-parameters]\n\nThe following parameters are supported. See [`terms aggregation`](/reference/aggregations/search-aggregations-bucket-terms-aggregation.md#search-aggregations-bucket-terms-aggregation-order) for more detailed explanation of these parameters.\n\nsize\n:   Optional. Defines how many term buckets should be returned out of the overall terms list. Defaults to 10.\n\nshard_size\n:   Optional. The higher the requested `size` is, the more accurate the results will be, but also, the more expensive it will be to compute the final results. The default `shard_size` is `(size * 1.5 + 10)`.\n\nshow_term_doc_count_error\n:   Optional. Calculates the doc count error on per term basis. Defaults to `false`\n\norder\n:   Optional. Specifies the order of the buckets. Defaults to the number of documents per bucket. The bucket terms value is used as a tiebreaker for buckets with the same document count.\n\nmin_doc_count\n:   Optional. The minimal number of documents in a bucket for it to be returned. Defaults to 1.\n\nshard_min_doc_count\n:   Optional. The minimal number of documents in a bucket on each shard for it to be returned. Defaults to `min_doc_count`.\n\ncollect_mode\n:   Optional. Specifies the strategy for data collection. The `depth_first` or `breadth_first` modes are supported. Defaults to `breadth_first`.\n\n\n## Script [search-aggregations-bucket-multi-terms-aggregation-script]\n\nGenerating the terms using a script:\n\n$$$multi-terms-aggregation-runtime-field-example$$$\n\n```console\nGET /products/_search\n{\n  \"runtime_mappings\": {\n    \"genre.length\": {\n      \"type\": \"long\",\n      \"script\": \"emit(doc['genre'].value.length())\"\n    }\n  },\n  \"aggs\": {\n    \"genres_and_products\": {\n      \"multi_terms\": {\n        \"terms\": [\n          {\n            \"field\": \"genre.length\"\n          },\n          {\n            \"field\": \"product\"\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n  ...\n  \"aggregations\" : {\n    \"genres_and_products\" : {\n      \"doc_count_error_upper_bound\" : 0,\n      \"sum_other_doc_count\" : 0,\n      \"buckets\" : [\n        {\n          \"key\" : [\n            4,\n            \"Product A\"\n          ],\n          \"key_as_string\" : \"4|Product A\",\n          \"doc_count\" : 2\n        },\n        {\n          \"key\" : [\n            4,\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"4|Product B\",\n          \"doc_count\" : 2\n        },\n        {\n          \"key\" : [\n            10,\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"10|Product B\",\n          \"doc_count\" : 1\n        }\n      ]\n    }\n  }\n}\n```\n\n\n## Missing value [_missing_value_3]\n\nThe `missing` parameter defines how documents that are missing a value should be treated. By default if any of the key components are missing the entire document will be ignored but it is also possible to treat them as if they had a value by using the `missing` parameter.\n\n$$$multi-terms-aggregation-missing-example$$$\n\n```console\nGET /products/_search\n{\n  \"aggs\": {\n    \"genres_and_products\": {\n      \"multi_terms\": {\n        \"terms\": [\n          {\n            \"field\": \"genre\"\n          },\n          {\n            \"field\": \"product\",\n            \"missing\": \"Product Z\"\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n   ...\n   \"aggregations\" : {\n    \"genres_and_products\" : {\n      \"doc_count_error_upper_bound\" : 0,\n      \"sum_other_doc_count\" : 0,\n      \"buckets\" : [\n        {\n          \"key\" : [\n            \"rock\",\n            \"Product A\"\n          ],\n          \"key_as_string\" : \"rock|Product A\",\n          \"doc_count\" : 2\n        },\n        {\n          \"key\" : [\n            \"electronic\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"electronic|Product B\",\n          \"doc_count\" : 1\n        },\n        {\n          \"key\" : [\n            \"electronic\",\n            \"Product Z\"\n          ],\n          \"key_as_string\" : \"electronic|Product Z\",  <1>\n          \"doc_count\" : 1\n        },\n        {\n          \"key\" : [\n            \"jazz\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"jazz|Product B\",\n          \"doc_count\" : 1\n        },\n        {\n          \"key\" : [\n            \"rock\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"rock|Product B\",\n          \"doc_count\" : 1\n        }\n      ]\n    }\n  }\n}\n```\n\n1. Documents without a value in the `product` field will fall into the same bucket as documents that have the value `Product Z`.\n\n\n\n## Mixing field types [_mixing_field_types]\n\n::::{warning}\nWhen aggregating on multiple indices the type of the aggregated field may not be the same in all indices. Some types are compatible with each other (`integer` and `long` or `float` and `double`) but when the types are a mix of decimal and non-decimal number the terms aggregation will promote the non-decimal numbers to decimal numbers. This can result in a loss of precision in the bucket values.\n::::\n\n\n\n## Sub aggregation and sorting examples [_sub_aggregation_and_sorting_examples]\n\nAs most bucket aggregations the `multi_term` supports sub aggregations and ordering the buckets by metrics sub-aggregation:\n\n$$$multi-terms-aggregation-subaggregation-example$$$\n\n```console\nGET /products/_search\n{\n  \"aggs\": {\n    \"genres_and_products\": {\n      \"multi_terms\": {\n        \"terms\": [\n          {\n            \"field\": \"genre\"\n          },\n          {\n            \"field\": \"product\"\n          }\n        ],\n        \"order\": {\n          \"total_quantity\": \"desc\"\n        }\n      },\n      \"aggs\": {\n        \"total_quantity\": {\n          \"sum\": {\n            \"field\": \"quantity\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n```console-result\n{\n  ...\n  \"aggregations\" : {\n    \"genres_and_products\" : {\n      \"doc_count_error_upper_bound\" : 0,\n      \"sum_other_doc_count\" : 0,\n      \"buckets\" : [\n        {\n          \"key\" : [\n            \"jazz\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"jazz|Product B\",\n          \"doc_count\" : 1,\n          \"total_quantity\" : {\n            \"value\" : 10.0\n          }\n        },\n        {\n          \"key\" : [\n            \"rock\",\n            \"Product A\"\n          ],\n          \"key_as_string\" : \"rock|Product A\",\n          \"doc_count\" : 2,\n          \"total_quantity\" : {\n            \"value\" : 9.0\n          }\n        },\n        {\n          \"key\" : [\n            \"electronic\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"electronic|Product B\",\n          \"doc_count\" : 1,\n          \"total_quantity\" : {\n            \"value\" : 3.0\n          }\n        },\n        {\n          \"key\" : [\n            \"rock\",\n            \"Product B\"\n          ],\n          \"key_as_string\" : \"rock|Product B\",\n          \"doc_count\" : 1,\n          \"total_quantity\" : {\n            \"value\" : 1.0\n          }\n        }\n      ]\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-multi-terms-aggregation.md"
            },
            {
              "navigation_title": "Terms",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html"
              ],
              "content": "# Terms aggregation [search-aggregations-bucket-terms-aggregation]\n\n\nA multi-bucket value source based aggregation where buckets are dynamically built - one per unique value.\n\nExample:\n\n$$$terms-aggregation-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": { \"field\": \"genre\" }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"genres\": {\n      \"doc_count_error_upper_bound\": 0,   <1>\n      \"sum_other_doc_count\": 0,           <2>\n      \"buckets\": [                        <3>\n        {\n          \"key\": \"electronic\",\n          \"doc_count\": 6\n        },\n        {\n          \"key\": \"rock\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n```\n\n1. an upper bound of the error on the document counts for each term, see [below](#terms-agg-doc-count-error)\n2. when there are lots of unique terms, Elasticsearch only returns the top terms; this number is the sum of the document counts for all buckets that are not part of the response\n3. the list of the top buckets, the meaning of `top` being defined by the [order](#search-aggregations-bucket-terms-aggregation-order)\n\n\n$$$search-aggregations-bucket-terms-aggregation-types$$$\nThe `field` can be [Keyword](/reference/elasticsearch/mapping-reference/keyword.md), [Numeric](/reference/elasticsearch/mapping-reference/number.md), [`ip`](/reference/elasticsearch/mapping-reference/ip.md), [`boolean`](/reference/elasticsearch/mapping-reference/boolean.md), or [`binary`](/reference/elasticsearch/mapping-reference/binary.md).\n\n::::{note}\nBy default, you cannot run a `terms` aggregation on a `text` field. Use a `keyword` [sub-field](/reference/elasticsearch/mapping-reference/multi-fields.md) instead. Alternatively, you can enable [`fielddata`](/reference/elasticsearch/mapping-reference/text.md#fielddata-mapping-param) on the `text` field to create buckets for the field\u2019s [analyzed](docs-content://manage-data/data-store/text-analysis.md) terms. Enabling `fielddata` can significantly increase memory usage.\n::::\n\n\n## Size [search-aggregations-bucket-terms-aggregation-size]\n\nBy default, the `terms` aggregation returns the top ten terms with the most documents. Use the `size` parameter to return more terms, up to the [search.max_buckets](/reference/elasticsearch/configuration-reference/search-settings.md#search-settings-max-buckets) limit.\n\nIf your data contains 100 or 1000 unique terms, you can increase the `size` of the `terms` aggregation to return them all. If you have more unique terms and you need them all, use the [composite aggregation](/reference/aggregations/search-aggregations-bucket-composite-aggregation.md) instead.\n\nLarger values of `size` use more memory to compute and, push the whole aggregation close to the `max_buckets` limit. You\u2019ll know you\u2019ve gone too large if the request fails with a message about `max_buckets`.\n\n\n## Shard size [search-aggregations-bucket-terms-aggregation-shard-size]\n\nTo get more accurate results, the `terms` agg fetches more than the top `size` terms from each shard. It fetches the top `shard_size` terms, which defaults to `size * 1.5 + 10`.\n\nThis is to handle the case when one term has many documents on one shard but is just below the `size` threshold on all other shards. If each shard only returned `size` terms, the aggregation would return an partial doc count for the term. So `terms` returns more terms in an attempt to catch the missing terms. This helps, but it\u2019s still quite possible to return a partial doc count for a term. It just takes a term with more disparate per-shard doc counts.\n\nYou can increase `shard_size` to better account for these disparate doc counts and improve the accuracy of the selection of top terms. It is much cheaper to increase the `shard_size` than to increase the `size`. However, it still takes more bytes over the wire and waiting in memory on the coordinating node.\n\n::::{important}\nThis guidance only applies if you\u2019re using the `terms` aggregation\u2019s default sort `order`. If you\u2019re sorting by anything other than document count in descending order, see [Order](#search-aggregations-bucket-terms-aggregation-order).\n::::\n\n\n::::{note}\n`shard_size` cannot be smaller than `size` (as it doesn\u2019t make much sense). When it is, Elasticsearch will override it and reset it to be equal to `size`.\n::::\n\n\n\n## Document count error [terms-agg-doc-count-error]\n\nEven with a larger `shard_size` value, `doc_count` values for a `terms` aggregation may be approximate. As a result, any sub-aggregations on the `terms` aggregation may also be approximate.\n\n`sum_other_doc_count` is the number of documents that didn\u2019t make it into the the top `size` terms. If this is greater than `0`, you can be sure that the `terms` agg had to throw away some buckets, either because they didn\u2019t fit into `size` on the coordinating node or they didn\u2019t fit into `shard_size` on the data node.\n\n\n## Per bucket document count error [_per_bucket_document_count_error]\n\nIf you set the `show_term_doc_count_error` parameter to `true`, the `terms` aggregation will include `doc_count_error_upper_bound`, which is an upper bound to the error on the `doc_count` returned by each shard. It\u2019s the sum of the size of the largest bucket on each shard that didn\u2019t fit into `shard_size`.\n\nIn more concrete terms, imagine there is one bucket that is very large on one shard and just outside the `shard_size` on all the other shards. In that case, the `terms` agg will return the bucket because it is large, but it\u2019ll be missing data from many documents on the shards where the term fell below the `shard_size` threshold. `doc_count_error_upper_bound` is the maximum number of those missing documents.\n\n$$$terms-aggregation-doc-count-error-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"products\": {\n      \"terms\": {\n        \"field\": \"product\",\n        \"size\": 5,\n        \"show_term_doc_count_error\": true\n      }\n    }\n  }\n}\n```\n\nThese errors can only be calculated in this way when the terms are ordered by descending document count. When the aggregation is ordered by the terms values themselves (either ascending or descending) there is no error in the document count since if a shard does not return a particular term which appears in the results from another shard, it must not have that term in its index. When the aggregation is either sorted by a sub aggregation or in order of ascending document count, the error in the document counts cannot be determined and is given a value of -1 to indicate this.\n\n\n## Order [search-aggregations-bucket-terms-aggregation-order]\n\nBy default, the `terms` aggregation orders terms by descending document `_count`.  This produces a bounded [document count](#terms-agg-doc-count-error) error that {{es}} can report.\n\nYou can use the `order` parameter to specify a different sort order, but we don\u2019t recommend it.  It is extremely easy to create a terms ordering that will just return wrong results, and not obvious to see when you have done so. Change this only with caution.\n\n::::{warning}\nEspecially avoid using `\"order\": { \"_count\": \"asc\" }`. If you need to find rare terms, use the [`rare_terms`](/reference/aggregations/search-aggregations-bucket-rare-terms-aggregation.md) aggregation instead. Due to the way the `terms` aggregation [gets terms from shards](#search-aggregations-bucket-terms-aggregation-shard-size), sorting by ascending doc count often produces inaccurate results.\n::::\n\n\n### Ordering by the term value [_ordering_by_the_term_value]\n\nIn this case, the buckets are ordered by the actual term values, such as lexicographic order for keywords or numerically for numbers. This sorting is safe in both ascending and descending directions, and produces accurate results.\n\nExample of ordering the buckets alphabetically by their terms in an ascending manner:\n\n$$$terms-aggregation-asc-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"_key\": \"asc\" }\n      }\n    }\n  }\n}\n```\n\n\n### Ordering by a sub aggregation [_ordering_by_a_sub_aggregation]\n\n::::{warning}\nSorting by a sub aggregation generally produces incorrect ordering, due to the way the `terms` aggregation [gets results from shards](#search-aggregations-bucket-terms-aggregation-shard-size).\n::::\n\n\nThere are two cases when sub-aggregation ordering is safe and returns correct results: sorting by a maximum in descending order, or sorting by a minimum in ascending order. These approaches work because they align with the behavior of sub aggregations. That is, if you\u2019re looking for the largest maximum or the smallest minimum, the global answer (from combined shards) must be included in one of the local shard answers. Conversely, the smallest maximum and largest minimum wouldn\u2019t be accurately computed.\n\nNote also that in these cases, the ordering is correct but the doc counts and non-ordering sub aggregations may still have errors (and {{es}} does not calculate a bound for those errors).\n\nOrdering the buckets by single value metrics sub-aggregation (identified by the aggregation name):\n\n$$$terms-aggregation-subaggregation-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"max_play_count\": \"desc\" }\n      },\n      \"aggs\": {\n        \"max_play_count\": { \"max\": { \"field\": \"play_count\" } }\n      }\n    }\n  }\n}\n```\n\nOrdering the buckets by multi value metrics sub-aggregation (identified by the aggregation name):\n\n$$$terms-aggregation-multivalue-subaggregation-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"playback_stats.max\": \"desc\" }\n      },\n      \"aggs\": {\n        \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n      }\n    }\n  }\n}\n```\n\n::::{admonition} Pipeline aggs cannot be used for sorting\n:class: note\n\n[Pipeline aggregations](/reference/aggregations/pipeline.md) are run during the reduce phase after all other aggregations have already completed. For this reason, they cannot be used for ordering.\n\n::::\n\n\nIt is also possible to order the buckets based on a \"deeper\" aggregation in the hierarchy. This is supported as long as the aggregations path are of a single-bucket type, where the last aggregation in the path may either be a single-bucket one or a metrics one. If it\u2019s a single-bucket type, the order will be defined by the number of docs in the bucket (i.e. `doc_count`), in case it\u2019s a metrics one, the same rules as above apply (where the path must indicate the metric name to sort by in case of a multi-value metrics aggregation, and in case of a single-value metrics aggregation the sort will be applied on that value).\n\nThe path must be defined in the following form:\n\n```ebnf\nAGG_SEPARATOR       =  '>' ;\nMETRIC_SEPARATOR    =  '.' ;\nAGG_NAME            =  <the name of the aggregation> ;\nMETRIC              =  <the name of the metric (in case of multi-value metrics aggregation)> ;\nPATH                =  <AGG_NAME> [ <AGG_SEPARATOR>, <AGG_NAME> ]* [ <METRIC_SEPARATOR>, <METRIC> ] ;\n```\n\n$$$terms-aggregation-hierarchy-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"countries\": {\n      \"terms\": {\n        \"field\": \"artist.country\",\n        \"order\": { \"rock>playback_stats.avg\": \"desc\" }\n      },\n      \"aggs\": {\n        \"rock\": {\n          \"filter\": { \"term\": { \"genre\": \"rock\" } },\n          \"aggs\": {\n            \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe above will sort the artist\u2019s countries buckets based on the average play count among the rock songs.\n\nMultiple criteria can be used to order the buckets by providing an array of order criteria such as the following:\n\n$$$terms-aggregation-multicriteria-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"countries\": {\n      \"terms\": {\n        \"field\": \"artist.country\",\n        \"order\": [ { \"rock>playback_stats.avg\": \"desc\" }, { \"_count\": \"desc\" } ]\n      },\n      \"aggs\": {\n        \"rock\": {\n          \"filter\": { \"term\": { \"genre\": \"rock\" } },\n          \"aggs\": {\n            \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe above will sort the artist\u2019s countries buckets based on the average play count among the rock songs and then by their `doc_count` in descending order.\n\n::::{note}\nIn the event that two buckets share the same values for all order criteria the bucket\u2019s term value is used as a tie-breaker in ascending alphabetical order to prevent non-deterministic ordering of buckets.\n::::\n\n\n\n### Ordering by count ascending [_ordering_by_count_ascending]\n\nOrdering terms by ascending document `_count` produces an unbounded error that {{es}} can\u2019t accurately report. We therefore strongly recommend against using `\"order\": { \"_count\": \"asc\" }` as shown in the following example:\n\n$$$terms-aggregation-count-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"_count\": \"asc\" }\n      }\n    }\n  }\n}\n```\n\n\n\n## Minimum document count [_minimum_document_count_4]\n\nIt is possible to only return terms that match more than a configured number of hits using the `min_doc_count` option:\n\n$$$terms-aggregation-min-doc-count-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"min_doc_count\": 10\n      }\n    }\n  }\n}\n```\n\nThe above aggregation would only return tags which have been found in 10 hits or more. Default value is `1`.\n\nTerms are collected and ordered on a shard level and merged with the terms collected from other shards in a second step. However, the shard does not have the information about the global document count available. The decision if a term is added to a candidate list depends only on the order computed on the shard using local shard frequencies. The `min_doc_count` criterion is only applied after merging local terms statistics of all shards. In a way the decision to add the term as a candidate is made without being very *certain* about if the term will actually reach the required `min_doc_count`. This might cause many (globally) high frequent terms to be missing in the final result if low frequent terms populated the candidate lists. To avoid this, the `shard_size` parameter can be increased to allow more candidate terms on the shards. However, this increases memory consumption and network traffic.\n\n### `shard_min_doc_count` [search-aggregations-bucket-terms-shard-min-doc-count]\n\nThe parameter `shard_min_doc_count` regulates the *certainty* a shard has if the term should actually be added to the candidate list or not with respect to the `min_doc_count`. Terms will only be considered if their local shard frequency within the set is higher than the `shard_min_doc_count`. If your dictionary contains many low frequent terms and you are not interested in those (for example misspellings), then you can set the `shard_min_doc_count` parameter to filter out candidate terms on a shard level that will with a reasonable certainty not reach the required `min_doc_count` even after merging the local counts. `shard_min_doc_count` is set to `0` per default and has no effect unless you explicitly set it.\n\n::::{note}\nSetting `min_doc_count`=`0` will also return buckets for terms that didn\u2019t match any hit. However, some of the returned terms which have a document count of zero might only belong to deleted documents or documents from other types, so there is no warranty that a `match_all` query would find a positive document count for those terms.\n::::\n\n\n::::{warning}\nWhen NOT sorting on `doc_count` descending, high values of `min_doc_count` may return a number of buckets which is less than `size` because not enough data was gathered from the shards. Missing buckets can be back by increasing `shard_size`. Setting `shard_min_doc_count` too high will cause terms to be filtered out on a shard level. This value should be set much lower than `min_doc_count/#shards`.\n::::\n\n\n\n\n## Script [search-aggregations-bucket-terms-aggregation-script]\n\nUse a [runtime field](docs-content://manage-data/data-store/mapping/runtime-fields.md) if the data in your documents doesn\u2019t exactly match what you\u2019d like to aggregate. If, for example, \"anthologies\" need to be in a special category then you could run this:\n\n$$$terms-aggregation-script-example$$$\n\n```console\nGET /_search\n{\n  \"size\": 0,\n  \"runtime_mappings\": {\n    \"normalized_genre\": {\n      \"type\": \"keyword\",\n      \"script\": \"\"\"\n        String genre = doc['genre'].value;\n        if (doc['product'].value.startsWith('Anthology')) {\n          emit(genre + ' anthology');\n        } else {\n          emit(genre);\n        }\n      \"\"\"\n    }\n  },\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"normalized_genre\"\n      }\n    }\n  }\n}\n```\n\nWhich will look like:\n\n```console-result\n{\n  \"aggregations\": {\n    \"genres\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"electronic\",\n          \"doc_count\": 4\n        },\n        {\n          \"key\": \"rock\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"electronic anthology\",\n          \"doc_count\": 2\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  },\n  ...\n}\n```\n\nThis is a little slower because the runtime field has to access two fields instead of one and because there are some optimizations that work on non-runtime `keyword` fields that we have to give up for for runtime `keyword` fields. If you need the speed, you can index the `normalized_genre` field.\n\n\n## Filtering Values [_filtering_values_4]\n\nIt is possible to filter the values for which buckets will be created. This can be done using the `include` and `exclude` parameters which are based on regular expression strings or arrays of exact values. Additionally, `include` clauses can filter using `partition` expressions.\n\n### Filtering Values with regular expressions [_filtering_values_with_regular_expressions_2]\n\n$$$terms-aggregation-regex-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"include\": \".*sport.*\",\n        \"exclude\": \"water_.*\"\n      }\n    }\n  }\n}\n```\n\nIn the above example, buckets will be created for all the tags that has the word `sport` in them, except those starting with `water_` (so the tag `water_sports` will not be aggregated). The `include` regular expression will determine what values are \"allowed\" to be aggregated, while the `exclude` determines the values that should not be aggregated. When both are defined, the `exclude` has precedence, meaning, the `include` is evaluated first and only then the `exclude`.\n\nThe syntax is the same as [regexp queries](/reference/query-languages/query-dsl/regexp-syntax.md).\n\n\n### Filtering Values with exact values [_filtering_values_with_exact_values_2]\n\nFor matching based on exact values the `include` and `exclude` parameters can simply take an array of strings that represent the terms as they are found in the index:\n\n$$$terms-aggregation-exact-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"JapaneseCars\": {\n      \"terms\": {\n        \"field\": \"make\",\n        \"include\": [ \"mazda\", \"honda\" ]\n      }\n    },\n    \"ActiveCarManufacturers\": {\n      \"terms\": {\n        \"field\": \"make\",\n        \"exclude\": [ \"rover\", \"jensen\" ]\n      }\n    }\n  }\n}\n```\n\n\n### Filtering Values with partitions [_filtering_values_with_partitions]\n\nSometimes there are too many unique terms to process in a single request/response pair so it can be useful to break the analysis up into multiple requests. This can be achieved by grouping the field\u2019s values into a number of partitions at query-time and processing only one partition in each request. Consider this request which is looking for accounts that have not logged any access recently:\n\n$$$terms-aggregation-partitions-example$$$\n\n```console\nGET /_search\n{\n   \"size\": 0,\n   \"aggs\": {\n      \"expired_sessions\": {\n         \"terms\": {\n            \"field\": \"account_id\",\n            \"include\": {\n               \"partition\": 0,\n               \"num_partitions\": 20\n            },\n            \"size\": 10000,\n            \"order\": {\n               \"last_access\": \"asc\"\n            }\n         },\n         \"aggs\": {\n            \"last_access\": {\n               \"max\": {\n                  \"field\": \"access_date\"\n               }\n            }\n         }\n      }\n   }\n}\n```\n\nThis request is finding the last logged access date for a subset of customer accounts because we might want to expire some customer accounts who haven\u2019t been seen for a long while. The `num_partitions` setting has requested that the unique account_ids are organized evenly into twenty partitions (0 to 19). and the `partition` setting in this request filters to only consider account_ids falling into partition 0. Subsequent requests should ask for partitions 1 then 2 etc to complete the expired-account analysis.\n\nNote that the `size` setting for the number of results returned needs to be tuned with the `num_partitions`. For this particular account-expiration example the process for balancing values for `size` and `num_partitions` would be as follows:\n\n1. Use the `cardinality` aggregation to estimate the total number of unique account_id values\n2. Pick a value for `num_partitions` to break the number from 1) up into more manageable chunks\n3. Pick a `size` value for the number of responses we want from each partition\n4. Run a test request\n\nIf we have a circuit-breaker error we are trying to do too much in one request and must increase `num_partitions`. If the request was successful but the last account ID in the date-sorted test response was still an account we might want to expire then we may be missing accounts of interest and have set our numbers too low. We must either\n\n* increase the `size` parameter to return more results per partition (could be heavy on memory) or\n* increase the `num_partitions` to consider less accounts per request (could increase overall processing time as we need to make more requests)\n\nUltimately this is a balancing act between managing the Elasticsearch resources required to process a single request and the volume of requests that the client application must issue to complete a task.\n\n::::{warning}\nPartitions cannot be used together with an `exclude` parameter.\n::::\n\n\n\n\n## Multi-field terms aggregation [_multi_field_terms_aggregation]\n\nThe `terms` aggregation does not support collecting terms from multiple fields in the same document. The reason is that the `terms` agg doesn\u2019t collect the string term values themselves, but rather uses [global ordinals](#search-aggregations-bucket-terms-aggregation-execution-hint) to produce a list of all of the unique values in the field. Global ordinals results in an important performance boost which would not be possible across multiple fields.\n\nThere are three approaches that you can use to perform a `terms` agg across multiple fields:\n\n[Script](#search-aggregations-bucket-terms-aggregation-script)\n:   Use a script to retrieve terms from multiple fields. This disables the global ordinals optimization and will be slower than collecting terms from a single field, but it gives you the flexibility to implement this option at search time.\n\n[`copy_to` field](/reference/elasticsearch/mapping-reference/copy-to.md)\n:   If you know ahead of time that you want to collect the terms from two or more fields, then use `copy_to` in your mapping to create a new dedicated field at index time which contains the values from both fields. You can aggregate on this single field, which will benefit from the global ordinals optimization.\n\n[`multi_terms` aggregation](/reference/aggregations/search-aggregations-bucket-multi-terms-aggregation.md)\n:   Use multi_terms aggregation to combine terms from multiple fields into a compound key. This also disables the global ordinals and will be slower than collecting terms from a single field. It is faster but less flexible than using a script.\n\n\n## Collect mode [search-aggregations-bucket-terms-aggregation-collect]\n\nDeferring calculation of child aggregations\n\nFor fields with many unique terms and a small number of required results it can be more efficient to delay the calculation of child aggregations until the top parent-level aggs have been pruned. Ordinarily, all branches of the aggregation tree are expanded in one depth-first pass and only then any pruning occurs. In some scenarios this can be very wasteful and can hit memory constraints. An example problem scenario is querying a movie database for the 10 most popular actors and their 5 most common co-stars:\n\n$$$terms-aggregation-collect-mode-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"actors\": {\n      \"terms\": {\n        \"field\": \"actors\",\n        \"size\": 10\n      },\n      \"aggs\": {\n        \"costars\": {\n          \"terms\": {\n            \"field\": \"actors\",\n            \"size\": 5\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nEven though the number of actors may be comparatively small and we want only 50 result buckets there is a combinatorial explosion of buckets during calculation - a single actor can produce n\u00b2 buckets where n is the number of actors. The sane option would be to first determine the 10 most popular actors and only then examine the top co-stars for these 10 actors. This alternative strategy is what we call the `breadth_first` collection mode as opposed to the `depth_first` mode.\n\n::::{note}\nThe `breadth_first` is the default mode for fields with a cardinality bigger than the requested size or when the cardinality is unknown (numeric fields or scripts for instance). It is possible to override the default heuristic and to provide a collect mode directly in the request:\n::::\n\n\n$$$terms-aggregation-breadth-first-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"actors\": {\n      \"terms\": {\n        \"field\": \"actors\",\n        \"size\": 10,\n        \"collect_mode\": \"breadth_first\" <1>\n      },\n      \"aggs\": {\n        \"costars\": {\n          \"terms\": {\n            \"field\": \"actors\",\n            \"size\": 5\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n1. the possible values are `breadth_first` and `depth_first`\n\n\nWhen using `breadth_first` mode the set of documents that fall into the uppermost buckets are cached for subsequent replay so there is a memory overhead in doing this which is linear with the number of matching documents. Note that the `order` parameter can still be used to refer to data from a child aggregation when using the `breadth_first` setting - the parent aggregation understands that this child aggregation will need to be called first before any of the other child aggregations.\n\n::::{warning}\nNested aggregations such as `top_hits` which require access to score information under an aggregation that uses the `breadth_first` collection mode need to replay the query on the second pass but only for the documents belonging to the top buckets.\n::::\n\n\n\n## Execution hint [search-aggregations-bucket-terms-aggregation-execution-hint]\n\nThere are different mechanisms by which terms aggregations can be executed:\n\n* by using field values directly in order to aggregate data per-bucket (`map`)\n* by using global ordinals of the field and allocating one bucket per global ordinal (`global_ordinals`)\n\nElasticsearch tries to have sensible defaults so this is something that generally doesn\u2019t need to be configured.\n\n`global_ordinals` is the default option for `keyword` field, it uses global ordinals to allocates buckets dynamically so memory usage is linear to the number of values of the documents that are part of the aggregation scope.\n\n`map` should only be considered when very few documents match a query. Otherwise the ordinals-based execution mode is significantly faster. By default, `map` is only used when running an aggregation on scripts, since they don\u2019t have ordinals.\n\n$$$terms-aggregation-execution-hint-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"execution_hint\": \"map\" <1>\n      }\n    }\n  }\n}\n```\n\n1. The possible values are `map`, `global_ordinals`\n\n\nPlease note that Elasticsearch will ignore this execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.\n\n\n## Missing value [_missing_value_5]\n\nThe `missing` parameter defines how documents that are missing a value should be treated. By default they will be ignored but it is also possible to treat them as if they had a value.\n\n$$$terms-aggregation-missing-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"missing\": \"N/A\" <1>\n      }\n    }\n  }\n}\n```\n\n1. Documents without a value in the `tags` field will fall into the same bucket as documents that have the value `N/A`.\n\n\n\n## Mixing field types [_mixing_field_types_2]\n\n::::{warning}\nWhen aggregating on multiple indices the type of the aggregated field may not be the same in all indices. Some types are compatible with each other (`integer` and `long` or `float` and `double`) but when the types are a mix of decimal and non-decimal number the terms aggregation will promote the non-decimal numbers to decimal numbers. This can result in a loss of precision in the bucket values.\n::::\n\n\n\n### Troubleshooting [search-aggregations-bucket-terms-aggregation-troubleshooting]\n\n### Failed Trying to Format Bytes [_failed_trying_to_format_bytes]\n\nWhen running a terms aggregation (or other aggregation, but in practice usually terms) over multiple indices, you may get an error that starts with \"Failed trying to format bytes\u2026 \".  This is usually caused by two of the indices not having the same mapping type for the field being aggregated.\n\n**Use an explicit `value_type`** Although it\u2019s best to correct the mappings, you can work around this issue if the field is unmapped in one of the indices.  Setting the `value_type` parameter can resolve the issue by coercing the unmapped field into the correct type.\n\n$$$terms-aggregation-value_type-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"ip_addresses\": {\n      \"terms\": {\n        \"field\": \"destination_ip\",\n        \"missing\": \"0.0.0.0\",\n        \"value_type\": \"ip\"\n      }\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-terms-aggregation.md"
            },
            {
              "navigation_title": "Significant text",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significanttext-aggregation.html"
              ],
              "content": "# Significant text aggregation [search-aggregations-bucket-significanttext-aggregation]\n\n\nAn aggregation that returns interesting or unusual occurrences of free-text terms in a set. It is like the [significant terms](/reference/aggregations/search-aggregations-bucket-significantterms-aggregation.md) aggregation but differs in that:\n\n* It is specifically designed for use on type `text` fields\n* It does not require field data or doc-values\n* It re-analyzes text content on-the-fly meaning it can also filter duplicate sections of noisy text that otherwise tend to skew statistics.\n\n::::{warning}\nRe-analyzing *large* result sets will require a lot of time and memory. It is recommended that the significant_text aggregation is used as a child of either the [sampler](/reference/aggregations/search-aggregations-bucket-sampler-aggregation.md) or [diversified sampler](/reference/aggregations/search-aggregations-bucket-diversified-sampler-aggregation.md) aggregation to limit the analysis to a *small* selection of top-matching documents e.g. 200. This will typically improve speed, memory use and quality of results.\n::::\n\n\n* Suggesting \"H5N1\" when users search for \"bird flu\" to help expand queries\n* Suggesting keywords relating to stock symbol $ATI for use in an automated news classifier\n\nIn these cases the words being selected are not simply the most popular terms in results. The most popular words tend to be very boring (*and, of, the, we, I, they* \u2026 ). The significant words are the ones that have undergone a significant change in popularity measured between a *foreground* and *background* set. If the term \"H5N1\" only exists in 5 documents in a 10 million document index and yet is found in 4 of the 100 documents that make up a user\u2019s search results that is significant and probably very relevant to their search. 5/10,000,000 vs 4/100 is a big swing in frequency.\n\n## Basic use [_basic_use_2]\n\nIn the typical use case, the *foreground* set of interest is a selection of the top-matching search results for a query and the *background* set used for statistical comparisons is the index or indices from which the results were gathered.\n\nExample:\n\n$$$significanttext-aggregation-example$$$\n\n```console\nGET news/_search\n{\n  \"query\": {\n    \"match\": { \"content\": \"Bird flu\" }\n  },\n  \"aggregations\": {\n    \"my_sample\": {\n      \"sampler\": {\n        \"shard_size\": 100\n      },\n      \"aggregations\": {\n        \"keywords\": {\n          \"significant_text\": { \"field\": \"content\" }\n        }\n      }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n  \"took\": 9,\n  \"timed_out\": false,\n  \"_shards\": ...,\n  \"hits\": ...,\n    \"aggregations\" : {\n        \"my_sample\": {\n            \"doc_count\": 100,\n            \"keywords\" : {\n                \"doc_count\": 100,\n                \"buckets\" : [\n                    {\n                        \"key\": \"h5n1\",\n                        \"doc_count\": 4,\n                        \"score\": 4.71235374214817,\n                        \"bg_count\": 5\n                    }\n                    ...\n                ]\n            }\n        }\n    }\n}\n```\n\nThe results show that \"h5n1\" is one of several terms strongly associated with bird flu. It only occurs 5 times in our index as a whole (see the `bg_count`) and yet 4 of these were lucky enough to appear in our 100 document sample of \"bird flu\" results. That suggests a significant word and one which the user can potentially add to their search.\n\n\n## Dealing with noisy data using `filter_duplicate_text` [filter-duplicate-text-noisy-data]\n\nFree-text fields often contain a mix of original content and mechanical copies of text (cut-and-paste biographies, email reply chains, retweets, boilerplate headers/footers, page navigation menus, sidebar news links, copyright notices, standard disclaimers, addresses).\n\nIn real-world data these duplicate sections of text tend to feature heavily in `significant_text` results if they aren\u2019t filtered out. Filtering near-duplicate text is a difficult task at index-time but we can cleanse the data on-the-fly at query time using the `filter_duplicate_text` setting.\n\nFirst let\u2019s look at an unfiltered real-world example using the [Signal media dataset](https://research.signalmedia.co/newsir16/signal-dataset.md) of a million news articles covering a wide variety of news. Here are the raw significant text results for a search for the articles mentioning \"elasticsearch\":\n\n```js\n{\n  ...\n  \"aggregations\": {\n    \"sample\": {\n      \"doc_count\": 35,\n      \"keywords\": {\n        \"doc_count\": 35,\n        \"buckets\": [\n          {\n            \"key\": \"elasticsearch\",\n            \"doc_count\": 35,\n            \"score\": 28570.428571428572,\n            \"bg_count\": 35\n          },\n          ...\n          {\n            \"key\": \"currensee\",\n            \"doc_count\": 8,\n            \"score\": 6530.383673469388,\n            \"bg_count\": 8\n          },\n          ...\n          {\n            \"key\": \"pozmantier\",\n            \"doc_count\": 4,\n            \"score\": 3265.191836734694,\n            \"bg_count\": 4\n          },\n          ...\n\n}\n```\n\nThe uncleansed documents have thrown up some odd-looking terms that are, on the face of it, statistically correlated with appearances of our search term \"elasticsearch\" e.g. \"pozmantier\". We can drill down into examples of these documents to see why pozmantier is connected using this query:\n\n$$$significanttext-aggregation-pozmantier-example$$$\n\n```console\nGET news/_search\n{\n  \"query\": {\n    \"simple_query_string\": {\n      \"query\": \"+elasticsearch  +pozmantier\"\n    }\n  },\n  \"_source\": [\n    \"title\",\n    \"source\"\n  ],\n  \"highlight\": {\n    \"fields\": {\n      \"content\": {}\n    }\n  }\n}\n```\n\nThe results show a series of very similar news articles about a judging panel for a number of tech projects:\n\n```js\n{\n  ...\n  \"hits\": {\n    \"hits\": [\n      {\n        ...\n        \"_source\": {\n          \"source\": \"Presentation Master\",\n          \"title\": \"T.E.N. Announces Nominees for the 2015 ISE\u00ae North America Awards\"\n        },\n        \"highlight\": {\n          \"content\": [\n            \"City of San Diego Mike <em>Pozmantier</em>, Program Manager, Cyber Security Division, Department of\",\n            \" Janus, Janus <em>ElasticSearch</em> Security Visualization Engine \"\n          ]\n        }\n      },\n      {\n        ...\n        \"_source\": {\n          \"source\": \"RCL Advisors\",\n          \"title\": \"T.E.N. Announces Nominees for the 2015 ISE(R) North America Awards\"\n        },\n        \"highlight\": {\n          \"content\": [\n            \"Mike <em>Pozmantier</em>, Program Manager, Cyber Security Division, Department of Homeland Security S&T\",\n            \"Janus, Janus <em>ElasticSearch</em> Security Visualization Engine\"\n          ]\n        }\n      },\n      ...\n```\n\nMike Pozmantier was one of many judges on a panel and elasticsearch was used in one of many projects being judged.\n\nAs is typical, this lengthy press release was cut-and-paste by a variety of news sites and consequently any rare names, numbers or typos they contain become statistically correlated with our matching query.\n\nFortunately similar documents tend to rank similarly so as part of examining the stream of top-matching documents the significant_text aggregation can apply a filter to remove sequences of any 6 or more tokens that have already been seen. Let\u2019s try this same query now but with the `filter_duplicate_text` setting turned on:\n\n$$$significanttext-aggregation-filter-duplicate-text-example$$$\n\n```console\nGET news/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"elasticsearch\"\n    }\n  },\n  \"aggs\": {\n    \"sample\": {\n      \"sampler\": {\n        \"shard_size\": 100\n      },\n      \"aggs\": {\n        \"keywords\": {\n          \"significant_text\": {\n            \"field\": \"content\",\n            \"filter_duplicate_text\": true\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe results from analysing our deduplicated text are obviously of higher quality to anyone familiar with the elastic stack:\n\n```js\n{\n  ...\n  \"aggregations\": {\n    \"sample\": {\n      \"doc_count\": 35,\n      \"keywords\": {\n        \"doc_count\": 35,\n        \"buckets\": [\n          {\n            \"key\": \"elasticsearch\",\n            \"doc_count\": 22,\n            \"score\": 11288.001166180758,\n            \"bg_count\": 35\n          },\n          {\n            \"key\": \"logstash\",\n            \"doc_count\": 3,\n            \"score\": 1836.648979591837,\n            \"bg_count\": 4\n          },\n          {\n            \"key\": \"kibana\",\n            \"doc_count\": 3,\n            \"score\": 1469.3020408163263,\n            \"bg_count\": 5\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\nMr Pozmantier and other one-off associations with elasticsearch no longer appear in the aggregation results as a consequence of copy-and-paste operations or other forms of mechanical repetition.\n\nIf your duplicate or near-duplicate content is identifiable via a single-value indexed field  (perhaps a hash of the article\u2019s `title` text or an `original_press_release_url` field) then it would be more efficient to use a parent [diversified sampler](/reference/aggregations/search-aggregations-bucket-diversified-sampler-aggregation.md) aggregation to eliminate these documents from the sample set based on that single key. The less duplicate content you can feed into the significant_text aggregation up front the better in terms of performance.\n\n::::{admonition} How are the significance scores calculated?\nThe numbers returned for scores are primarily intended for ranking different suggestions sensibly rather than something easily understood by end users. The scores are derived from the doc frequencies in *foreground* and *background* sets. In brief, a term is considered significant if there is a noticeable difference in the frequency in which a term appears in the subset and in the background. The way the terms are ranked can be configured, see \"Parameters\" section.\n\n::::\n\n\n::::{admonition} Use the \"like this but not this\" pattern\nYou can spot mis-categorized content by first searching a structured field e.g. `category:adultMovie` and use significant_text on the text \"movie_description\" field. Take the suggested words (I\u2019ll leave them to your imagination) and then search for all movies NOT marked as category:adultMovie but containing these keywords. You now have a ranked list of badly-categorized movies that you should reclassify or at least remove from the \"familyFriendly\" category.\n\nThe significance score from each term can also provide a useful `boost` setting to sort matches. Using the `minimum_should_match` setting of the `terms` query with the keywords will help control the balance of precision/recall in the result set i.e a high setting would have a small number of relevant results packed full of keywords and a setting of \"1\" would produce a more exhaustive results set with all documents containing *any* keyword.\n\n::::\n\n\n\n## Limitations [_limitations_9]\n\n### No support for child aggregations [_no_support_for_child_aggregations]\n\nThe significant_text aggregation intentionally does not support the addition of child aggregations because:\n\n* It would come with a high memory cost\n* It isn\u2019t a generally useful feature and there is a workaround for those that need it\n\nThe volume of candidate terms is generally very high and these are pruned heavily before the final results are returned. Supporting child aggregations would generate additional churn and be inefficient. Clients can always take the heavily-trimmed set of results from a `significant_text` request and make a subsequent follow-up query using a `terms` aggregation with an `include` clause and child aggregations to perform further analysis of selected keywords in a more efficient fashion.\n\n\n### No support for nested objects [_no_support_for_nested_objects]\n\nThe significant_text aggregation currently also cannot be used with text fields in nested objects, because it works with the document JSON source. This makes this feature inefficient when matching nested docs from stored JSON given a matching Lucene docID.\n\n\n### Approximate counts [_approximate_counts_2]\n\nThe counts of how many documents contain a term provided in results are based on summing the samples returned from each shard and as such may be:\n\n* low if certain shards did not provide figures for a given term in their top sample\n* high when considering the background frequency as it may count occurrences found in deleted documents\n\nLike most design decisions, this is the basis of a trade-off in which we have chosen to provide fast performance at the cost of some (typically small) inaccuracies. However, the `size` and `shard size` settings covered in the next section provide tools to help control the accuracy levels.\n\n\n\n## Parameters [significanttext-aggregation-parameters]\n\n### Significance heuristics [_significance_heuristics]\n\nThis aggregation supports the same scoring heuristics (JLH, mutual_information, gnd, chi_square etc) as the [significant terms](/reference/aggregations/search-aggregations-bucket-significantterms-aggregation.md) aggregation\n\n\n### Size & Shard Size [sig-text-shard-size]\n\nThe `size` parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. If the number of unique terms is greater than `size`, the returned list can be slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned).\n\nTo ensure better accuracy a multiple of the final `size` is used as the number of terms to request from each shard (`2 * (size * 1.5 + 10)`). To take manual control of this setting the `shard_size` parameter can be used to control the volumes of candidate terms produced by each shard.\n\nLow-frequency terms can turn out to be the most interesting ones once all results are combined so the significant_terms aggregation can produce higher-quality results when the `shard_size` parameter is set to values significantly higher than the `size` setting. This ensures that a bigger volume of promising candidate terms are given a consolidated review by the reducing node before the final selection. Obviously large candidate term lists will cause extra network traffic and RAM usage so this is quality/cost trade off that needs to be balanced. If `shard_size` is set to -1 (the default) then `shard_size` will be automatically estimated based on the number of shards and the `size` parameter.\n\n::::{note}\n`shard_size` cannot be smaller than `size` (as it doesn\u2019t make much sense). When it is, elasticsearch will override it and reset it to be equal to `size`.\n::::\n\n\n\n### Minimum document count [_minimum_document_count_3]\n\nIt is possible to only return terms that match more than a configured number of hits using the `min_doc_count` option. The Default value is 3.\n\nTerms that score highly will be collected on a shard level and merged with the terms collected from other shards in a second step. However, the shard does not have the information about the global term frequencies available. The decision if a term is added to a candidate list depends only on the score computed on the shard using local shard frequencies, not the global frequencies of the word. The `min_doc_count` criterion is only applied after merging local terms statistics of all shards. In a way the decision to add the term as a candidate is made without being very *certain* about if the term will actually reach the required `min_doc_count`. This might cause many (globally) high frequent terms to be missing in the final result if low frequent but high scoring terms populated the candidate lists. To avoid this, the `shard_size` parameter can be increased to allow more candidate terms on the shards. However, this increases memory consumption and network traffic.\n\n#### `shard_min_doc_count` [search-aggregations-bucket-significanttext-shard-min-doc-count]\n\nThe parameter `shard_min_doc_count` regulates the *certainty* a shard has if the term should actually be added to the candidate list or not with respect to the `min_doc_count`. Terms will only be considered if their local shard frequency within the set is higher than the `shard_min_doc_count`. If your dictionary contains many low frequent terms and you are not interested in those (for example misspellings), then you can set the `shard_min_doc_count` parameter to filter out candidate terms on a shard level that will with a reasonable certainty not reach the required `min_doc_count` even after merging the local counts. `shard_min_doc_count` is set to `0` per default and has no effect unless you explicitly set it.\n\n::::{warning}\nSetting `min_doc_count` to `1` is generally not advised as it tends to return terms that are typos or other bizarre curiosities. Finding more than one instance of a term helps reinforce that, while still rare, the term was not the result of a one-off accident. The default value of 3 is used to provide a minimum weight-of-evidence. Setting `shard_min_doc_count` too high will cause significant candidate terms to be filtered out on a shard level. This value should be set much lower than `min_doc_count/#shards`.\n::::\n\n\n\n\n### Custom background context [_custom_background_context_2]\n\nThe default source of statistical information for background term frequencies is the entire index and this scope can be narrowed through the use of a `background_filter` to focus in on significant terms within a narrower context:\n\n$$$significanttext-aggregation-custom-background-example$$$\n\n```console\nGET news/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"madrid\"\n    }\n  },\n  \"aggs\": {\n    \"tags\": {\n      \"significant_text\": {\n        \"field\": \"content\",\n        \"background_filter\": {\n          \"term\": { \"content\": \"spain\" }\n        }\n      }\n    }\n  }\n}\n```\n\nThe above filter would help focus in on terms that were peculiar to the city of Madrid rather than revealing terms like \"Spanish\" that are unusual in the full index\u2019s worldwide context but commonplace in the subset of documents containing the word \"Spain\".\n\n::::{warning}\nUse of background filters will slow the query as each term\u2019s postings must be filtered to determine a frequency\n::::\n\n\n\n### Dealing with source and index mappings [_dealing_with_source_and_index_mappings]\n\nOrdinarily the indexed field name and the original JSON field being retrieved share the same name. However with more complex field mappings using features like `copy_to` the source JSON field(s) and the indexed field being aggregated can differ. In these cases it is possible to list the JSON _source fields from which text will be analyzed using the `source_fields` parameter:\n\n$$$significanttext-aggregation-mappings-example$$$\n\n```console\nGET news/_search\n{\n  \"query\": {\n    \"match\": {\n      \"custom_all\": \"elasticsearch\"\n    }\n  },\n  \"aggs\": {\n    \"tags\": {\n      \"significant_text\": {\n        \"field\": \"custom_all\",\n        \"source_fields\": [ \"content\", \"title\" ]\n      }\n    }\n  }\n}\n```\n\n\n### Filtering Values [_filtering_values_3]\n\nIt is possible (although rarely required) to filter the values for which buckets will be created. This can be done using the `include` and `exclude` parameters which are based on a regular expression string or arrays of exact terms. This functionality mirrors the features described in the [terms aggregation](/reference/aggregations/search-aggregations-bucket-terms-aggregation.md) documentation.",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-significanttext-aggregation.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html"
              ],
              "content": "# Aggregations\n\n:::{note}\nThis section provides detailed **reference information** for aggregations.\n\nRefer to [Aggregations](docs-content://explore-analyze/query-filter/aggregations.md) in the **Explore and analyze** section for overview, getting started and conceptual information.\n:::\n\nAggregations are a powerful framework that enables you to perform complex data analysis and summarization over indexed documents. They enable you to extract and compute statistics, trends, and patterns from large datasets.\n\n{{es}} organizes aggregations into three categories:\n\n* Metric aggregations that calculate metrics, such as a sum or average, from field values.\n* Bucket aggregations that group documents into buckets, also called bins, based on field values, ranges, or other criteria.\n* Pipeline aggregations that take input from other aggregations instead of documents or fields.",
              "filename": "elasticsearch-main/docs/reference/aggregations/index.md"
            },
            {
              "navigation_title": "Significant terms",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html"
              ],
              "content": "# Significant terms aggregation [search-aggregations-bucket-significantterms-aggregation]\n\n\nAn aggregation that returns interesting or unusual occurrences of terms in a set.\n\n* Suggesting \"H5N1\" when users search for \"bird flu\" in text\n* Identifying the merchant that is the \"common point of compromise\" from the transaction history of credit card owners reporting loss\n* Suggesting keywords relating to stock symbol $ATI for an automated news classifier\n* Spotting the fraudulent doctor who is diagnosing more than their fair share of whiplash injuries\n* Spotting the tire manufacturer who has a disproportionate number of blow-outs\n\nIn all these cases the terms being selected are not simply the most popular terms in a set. They are the terms that have undergone a significant change in popularity measured between a *foreground* and *background* set. If the term \"H5N1\" only exists in 5 documents in a 10 million document index and yet is found in 4 of the 100 documents that make up a user\u2019s search results that is significant and probably very relevant to their search. 5/10,000,000 vs 4/100 is a big swing in frequency.\n\n## Single-set analysis [_single_set_analysis]\n\nIn the simplest case, the *foreground* set of interest is the search results matched by a query and the *background* set used for statistical comparisons is the index or indices from which the results were gathered.\n\nExample:\n\n$$$significantterms-aggregation-example$$$\n\n```console\nGET /_search\n{\n  \"query\": {\n    \"terms\": { \"force\": [ \"British Transport Police\" ] }\n  },\n  \"aggregations\": {\n    \"significant_crime_types\": {\n      \"significant_terms\": { \"field\": \"crime_type\" }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n  ...\n  \"aggregations\": {\n    \"significant_crime_types\": {\n      \"doc_count\": 47347,\n      \"bg_count\": 5064554,\n      \"buckets\": [\n        {\n          \"key\": \"Bicycle theft\",\n          \"doc_count\": 3640,\n          \"score\": 0.371235374214817,\n          \"bg_count\": 66799\n        }\n              ...\n      ]\n    }\n  }\n}\n```\n\nWhen querying an index of all crimes from all police forces, what these results show is that the British Transport Police force stand out as a force dealing with a disproportionately large number of bicycle thefts. Ordinarily, bicycle thefts represent only 1% of crimes (66799/5064554) but for the British Transport Police, who handle crime on railways and stations, 7% of crimes (3640/47347) is a bike theft. This is a significant seven-fold increase in frequency and so this anomaly was highlighted as the top crime type.\n\nThe problem with using a query to spot anomalies is it only gives us one subset to use for comparisons. To discover all the other police forces' anomalies we would have to repeat the query for each of the different forces.\n\nThis can be a tedious way to look for unusual patterns in an index.\n\n\n## Multi-set analysis [_multi_set_analysis]\n\nA simpler way to perform analysis across multiple categories is to use a parent-level aggregation to segment the data ready for analysis.\n\nExample using a parent aggregation for segmentation:\n\n$$$significantterms-aggregation-multiset--example$$$\n\n```console\nGET /_search\n{\n  \"aggregations\": {\n    \"forces\": {\n      \"terms\": { \"field\": \"force\" },\n      \"aggregations\": {\n        \"significant_crime_types\": {\n          \"significant_terms\": { \"field\": \"crime_type\" }\n        }\n      }\n    }\n  }\n}\n```\n\nResponse:\n\n```console-result\n{\n ...\n \"aggregations\": {\n    \"forces\": {\n        \"doc_count_error_upper_bound\": 1375,\n        \"sum_other_doc_count\": 7879845,\n        \"buckets\": [\n            {\n                \"key\": \"Metropolitan Police Service\",\n                \"doc_count\": 894038,\n                \"significant_crime_types\": {\n                    \"doc_count\": 894038,\n                    \"bg_count\": 5064554,\n                    \"buckets\": [\n                        {\n                            \"key\": \"Robbery\",\n                            \"doc_count\": 27617,\n                            \"score\": 0.0599,\n                            \"bg_count\": 53182\n                        }\n                        ...\n                    ]\n                }\n            },\n            {\n                \"key\": \"British Transport Police\",\n                \"doc_count\": 47347,\n                \"significant_crime_types\": {\n                    \"doc_count\": 47347,\n                    \"bg_count\": 5064554,\n                    \"buckets\": [\n                        {\n                            \"key\": \"Bicycle theft\",\n                            \"doc_count\": 3640,\n                            \"score\": 0.371,\n                            \"bg_count\": 66799\n                        }\n                        ...\n                    ]\n                }\n            }\n        ]\n    }\n  }\n}\n```\n\nNow we have anomaly detection for each of the police forces using a single request.\n\nWe can use other forms of top-level aggregations to segment our data, for example segmenting by geographic area to identify unusual hot-spots of a particular crime type:\n\n$$$significantterms-aggregation-hotspot-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"hotspots\": {\n      \"geohash_grid\": {\n        \"field\": \"location\",\n        \"precision\": 5\n      },\n      \"aggs\": {\n        \"significant_crime_types\": {\n          \"significant_terms\": { \"field\": \"crime_type\" }\n        }\n      }\n    }\n  }\n}\n```\n\nThis example uses the `geohash_grid` aggregation to create result buckets that represent geographic areas, and inside each bucket we can identify anomalous levels of a crime type in these tightly-focused areas e.g.\n\n* Airports exhibit unusual numbers of weapon confiscations\n* Universities show uplifts of bicycle thefts\n\nAt a higher geohash_grid zoom-level with larger coverage areas we would start to see where an entire police-force may be tackling an unusual volume of a particular crime type.\n\nObviously a time-based top-level segmentation would help identify current trends for each point in time where a simple `terms` aggregation would typically show the very popular \"constants\" that persist across all time slots.\n\n::::{admonition} How are the scores calculated?\nThe numbers returned for scores are primarily intended for ranking different suggestions sensibly rather than something easily understood by end users. The scores are derived from the doc frequencies in *foreground* and *background* sets. In brief, a term is considered significant if there is a noticeable difference in the frequency in which a term appears in the subset and in the background. The way the terms are ranked can be configured, see \"Parameters\" section.\n\n::::\n\n\n\n## Use on free-text fields [_use_on_free_text_fields]\n\nThe significant_terms aggregation can be used effectively on tokenized free-text fields to suggest:\n\n* keywords for refining end-user searches\n* keywords for use in percolator queries\n\n::::{warning}\nPicking a free-text field as the subject of a significant terms analysis can be expensive! It will attempt to load every unique word into RAM. It is recommended to only use this on smaller indices.\n::::\n\n\n::::{admonition} Use the \"like this but not this\" pattern\nYou can spot mis-categorized content by first searching a structured field e.g. `category:adultMovie` and use significant_terms on the free-text \"movie_description\" field. Take the suggested words (I\u2019ll leave them to your imagination) and then search for all movies NOT marked as category:adultMovie but containing these keywords. You now have a ranked list of badly-categorized movies that you should reclassify or at least remove from the \"familyFriendly\" category.\n\nThe significance score from each term can also provide a useful `boost` setting to sort matches. Using the `minimum_should_match` setting of the `terms` query with the keywords will help control the balance of precision/recall in the result set i.e a high setting would have a small number of relevant results packed full of keywords and a setting of \"1\" would produce a more exhaustive results set with all documents containing *any* keyword.\n\n::::\n\n\n::::{tip}\nFree-text significant_terms are much more easily understood when viewed in context. Take the results of `significant_terms` suggestions from a free-text field and use them in a `terms` query on the same field with a `highlight` clause to present users with example snippets of documents. When the terms are presented unstemmed, highlighted, with the right case, in the right order and with some context, their significance/meaning is more readily apparent.\n\n::::\n\n\n\n## Custom background sets [_custom_background_sets]\n\nOrdinarily, the foreground set of documents is \"diffed\" against a background set of all the documents in your index. However, sometimes it may prove useful to use a narrower background set as the basis for comparisons. For example, a query on documents relating to \"Madrid\" in an index with content from all over the world might reveal that \"Spanish\" was a significant term. This may be true but if you want some more focused terms you could use a `background_filter` on the term *spain* to establish a narrower set of documents as context. With this as a background \"Spanish\" would now be seen as commonplace and therefore not as significant as words like \"capital\" that relate more strongly with Madrid. Note that using a background filter will slow things down - each term\u2019s background frequency must now be derived on-the-fly from filtering posting lists rather than reading the index\u2019s pre-computed count for a term.\n\n\n## Limitations [_limitations_8]\n\n### Significant terms must be indexed values [_significant_terms_must_be_indexed_values]\n\nUnlike the terms aggregation it is currently not possible to use script-generated terms for counting purposes. Because of the way the significant_terms aggregation must consider both *foreground* and *background* frequencies it would be prohibitively expensive to use a script on the entire index to obtain background frequencies for comparisons. Also DocValues are not supported as sources of term data for similar reasons.\n\n\n### No analysis of floating point fields [_no_analysis_of_floating_point_fields]\n\nFloating point fields are currently not supported as the subject of significant_terms analysis. While integer or long fields can be used to represent concepts like bank account numbers or category numbers which can be interesting to track, floating point fields are usually used to represent quantities of something. As such, individual floating point terms are not useful for this form of frequency analysis.\n\n\n### Use as a parent aggregation [_use_as_a_parent_aggregation]\n\nIf there is the equivalent of a `match_all` query or no query criteria providing a subset of the index the significant_terms aggregation should not be used as the top-most aggregation - in this scenario the *foreground* set is exactly the same as the *background* set and so there is no difference in document frequencies to observe and from which to make sensible suggestions.\n\nAnother consideration is that the significant_terms aggregation produces many candidate results at shard level that are only later pruned on the reducing node once all statistics from all shards are merged. As a result, it can be inefficient and costly in terms of RAM to embed large child aggregations under a significant_terms aggregation that later discards many candidate terms. It is advisable in these cases to perform two searches - the first to provide a rationalized list of significant_terms and then add this shortlist of terms to a second query to go back and fetch the required child aggregations.\n\n\n### Approximate counts [_approximate_counts]\n\nThe counts of how many documents contain a term provided in results are based on summing the samples returned from each shard and as such may be:\n\n* low if certain shards did not provide figures for a given term in their top sample\n* high when considering the background frequency as it may count occurrences found in deleted documents\n\nLike most design decisions, this is the basis of a trade-off in which we have chosen to provide fast performance at the cost of some (typically small) inaccuracies. However, the `size` and `shard size` settings covered in the next section provide tools to help control the accuracy levels.\n\n\n\n## Parameters [significantterms-aggregation-parameters]\n\n### JLH score [_jlh_score]\n\nThe JLH score can be used as a significance score by adding the parameter\n\n```js\n   \"jlh\": {\n   }\n```\n\nThe scores are derived from the doc frequencies in *foreground* and *background* sets. The *absolute* change in popularity (foregroundPercent - backgroundPercent) would favor common terms whereas the *relative* change in popularity (foregroundPercent/ backgroundPercent) would favor rare terms. Rare vs common is essentially a precision vs recall balance and so the absolute and relative changes are multiplied to provide a sweet spot between precision and recall.\n\n\n### Mutual information [_mutual_information]\n\nMutual information as described in \"Information Retrieval\", Manning et al., Chapter 13.5.1 can be used as significance score by adding the parameter\n\n```js\n   \"mutual_information\": {\n        \"include_negatives\": true\n   }\n```\n\nMutual information does not differentiate between terms that are descriptive for the subset or for documents outside the subset. The significant terms therefore can contain terms that appear more or less frequent in the subset than outside the subset. To filter out the terms that appear less often in the subset than in documents outside the subset, `include_negatives` can be set to `false`.\n\nPer default, the assumption is that the documents in the bucket are also contained in the background. If instead you defined a custom background filter that represents a different set of documents that you want to compare to, set\n\n```js\n\"background_is_superset\": false\n```\n\n\n### Chi square [_chi_square]\n\nChi square as described in \"Information Retrieval\", Manning et al., Chapter 13.5.2 can be used as significance score by adding the parameter\n\n```js\n   \"chi_square\": {\n   }\n```\n\nChi square behaves like mutual information and can be configured with the same parameters `include_negatives` and `background_is_superset`.\n\n\n### Google normalized distance [_google_normalized_distance]\n\nGoogle normalized distance as described in [\"The Google Similarity Distance\", Cilibrasi and Vitanyi, 2007](https://arxiv.org/pdf/cs/0412098v3.pdf) can be used as significance score by adding the parameter\n\n```js\n   \"gnd\": {\n   }\n```\n\n`gnd` also accepts the `background_is_superset` parameter.\n\n\n### p-value score [p-value-score]\n\nThe p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. The p-value is calculated assuming that the foreground set and the background set are independent [Bernoulli trials](https://en.wikipedia.org/wiki/Bernoulli_trial), with the null hypothesis that the probabilities are the same.\n\n#### Example usage [_example_usage]\n\nThis example calculates the p-value score for terms `user_agent.version` given the foreground set of \"ended in failure\" versus \"NOT ended in failure\".\n\n`\"background_is_superset\": false` indicates that the background set does not contain the counts of the foreground set as they are filtered out.\n\n`\"normalize_above\": 1000` facilitates returning consistent significance results at various scales. `1000` indicates that term counts greater than `1000` are scaled down by a factor of `1000/term_count`.\n\n```console\nGET /_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        {\n          \"term\": {\n            \"event.outcome\": \"failure\"\n          }\n        },\n        {\n          \"range\": {\n            \"@timestamp\": {\n              \"gte\": \"2021-02-01\",\n              \"lt\": \"2021-02-04\"\n            }\n          }\n        },\n        {\n          \"term\": {\n            \"service.name\": {\n              \"value\": \"frontend-node\"\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"aggs\": {\n    \"failure_p_value\": {\n      \"significant_terms\": {\n        \"field\": \"user_agent.version\",\n        \"background_filter\": {\n          \"bool\": {\n            \"must_not\": [\n              {\n                \"term\": {\n                  \"event.outcome\": \"failure\"\n                }\n              }\n            ],\n            \"filter\": [\n              {\n                \"range\": {\n                  \"@timestamp\": {\n                    \"gte\": \"2021-02-01\",\n                    \"lt\": \"2021-02-04\"\n                  }\n                }\n              },\n              {\n                \"term\": {\n                  \"service.name\": {\n                    \"value\": \"frontend-node\"\n                  }\n                }\n              }\n            ]\n          }\n        },\n        \"p_value\": {\"background_is_superset\": false, \"normalize_above\": 1000}\n      }\n    }\n  }\n}\n```\n\n\n\n### Percentage [_percentage]\n\nA simple calculation of the number of documents in the foreground sample with a term divided by the number of documents in the background with the term. By default this produces a score greater than zero and less than one.\n\nThe benefit of this heuristic is that the scoring logic is simple to explain to anyone familiar with a \"per capita\" statistic. However, for fields with high cardinality there is a tendency for this heuristic to select the rarest terms such as typos that occur only once because they score 1/1 = 100%.\n\nIt would be hard for a seasoned boxer to win a championship if the prize was awarded purely on the basis of percentage of fights won - by these rules a newcomer with only one fight under their belt would be impossible to beat. Multiple observations are typically required to reinforce a view so it is recommended in these cases to set both `min_doc_count` and `shard_min_doc_count` to a higher value such as 10 in order to filter out the low-frequency terms that otherwise take precedence.\n\n```js\n   \"percentage\": {\n   }\n```\n\n\n### Which one is best? [_which_one_is_best]\n\nRoughly, `mutual_information` prefers high frequent terms even if they occur also frequently in the background. For example, in an analysis of natural language text this might lead to selection of stop words. `mutual_information` is unlikely to select very rare terms like misspellings. `gnd` prefers terms with a high co-occurrence and avoids selection of stopwords. It might be better suited for synonym detection. However, `gnd` has a tendency to select very rare terms that are, for example, a result of misspelling. `chi_square` and `jlh` are somewhat in-between.\n\nIt is hard to say which one of the different heuristics will be the best choice as it depends on what the significant terms are used for (see for example [Yang and Pedersen, \"A Comparative Study on Feature Selection in Text Categorization\", 1997](http://courses.ischool.berkeley.edu/i256/f06/papers/yang97comparative.pdf) for a study on using significant terms for feature selection for text classification).\n\nIf none of the above measures suits your usecase than another option is to implement a custom significance measure:\n\n\n### Scripted [_scripted]\n\nCustomized scores can be implemented via a script:\n\n```js\n      \"script_heuristic\": {\n              \"script\": {\n          \"lang\": \"painless\",\n          \"source\": \"params._subset_freq/(params._superset_freq - params._subset_freq + 1)\"\n        }\n            }\n```\n\nScripts can be inline (as in above example), indexed or stored on disk. For details on the options, see [script documentation](docs-content://explore-analyze/scripting.md).\n\nAvailable parameters in the script are\n\n`_subset_freq`\n:   Number of documents the term appears in the subset.\n\n`_superset_freq`\n:   Number of documents the term appears in the superset.\n\n`_subset_size`\n:   Number of documents in the subset.\n\n`_superset_size`\n:   Number of documents in the superset.\n\n\n### Size & Shard Size [sig-terms-shard-size]\n\nThe `size` parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. If the number of unique terms is greater than `size`, the returned list can be slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned).\n\nTo ensure better accuracy a multiple of the final `size` is used as the number of terms to request from each shard (`2 * (size * 1.5 + 10)`). To take manual control of this setting the `shard_size` parameter can be used to control the volumes of candidate terms produced by each shard.\n\nLow-frequency terms can turn out to be the most interesting ones once all results are combined so the significant_terms aggregation can produce higher-quality results when the `shard_size` parameter is set to values significantly higher than the `size` setting. This ensures that a bigger volume of promising candidate terms are given a consolidated review by the reducing node before the final selection. Obviously large candidate term lists will cause extra network traffic and RAM usage so this is quality/cost trade off that needs to be balanced. If `shard_size` is set to -1 (the default) then `shard_size` will be automatically estimated based on the number of shards and the `size` parameter.\n\n::::{note}\n`shard_size` cannot be smaller than `size` (as it doesn\u2019t make much sense). When it is, Elasticsearch will override it and reset it to be equal to `size`.\n::::\n\n\n\n### Minimum document count [_minimum_document_count_2]\n\nIt is possible to only return terms that match more than a configured number of hits using the `min_doc_count` option:\n\n$$$significantterms-aggregation-min-document-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"significant_terms\": {\n        \"field\": \"tag\",\n        \"min_doc_count\": 10\n      }\n    }\n  }\n}\n```\n\nThe above aggregation would only return tags which have been found in 10 hits or more. Default value is `3`.\n\nTerms that score highly will be collected on a shard level and merged with the terms collected from other shards in a second step. However, the shard does not have the information about the global term frequencies available. The decision if a term is added to a candidate list depends only on the score computed on the shard using local shard frequencies, not the global frequencies of the word. The `min_doc_count` criterion is only applied after merging local terms statistics of all shards. In a way the decision to add the term as a candidate is made without being very *certain* about if the term will actually reach the required `min_doc_count`. This might cause many (globally) high frequent terms to be missing in the final result if low frequent but high scoring terms populated the candidate lists. To avoid this, the `shard_size` parameter can be increased to allow more candidate terms on the shards. However, this increases memory consumption and network traffic.\n\n\n### `shard_min_doc_count` [search-aggregations-bucket-significantterms-shard-min-doc-count]\n\nThe parameter `shard_min_doc_count` regulates the *certainty* a shard has if the term should actually be added to the candidate list or not with respect to the `min_doc_count`. Terms will only be considered if their local shard frequency within the set is higher than the `shard_min_doc_count`. If your dictionary contains many low frequent terms and you are not interested in those (for example misspellings), then you can set the `shard_min_doc_count` parameter to filter out candidate terms on a shard level that will with a reasonable certainty not reach the required `min_doc_count` even after merging the local counts. `shard_min_doc_count` is set to `0` per default and has no effect unless you explicitly set it.\n\n::::{warning}\nSetting `min_doc_count` to `1` is generally not advised as it tends to return terms that are typos or other bizarre curiosities. Finding more than one instance of a term helps reinforce that, while still rare, the term was not the result of a one-off accident. The default value of 3 is used to provide a minimum weight-of-evidence. Setting `shard_min_doc_count` too high will cause significant candidate terms to be filtered out on a shard level. This value should be set much lower than `min_doc_count/#shards`.\n::::\n\n\n\n### Custom background context [_custom_background_context]\n\nThe default source of statistical information for background term frequencies is the entire index and this scope can be narrowed through the use of a `background_filter` to focus in on significant terms within a narrower context:\n\n$$$significantterms-aggregation-custom-background-example$$$\n\n```console\nGET /_search\n{\n  \"query\": {\n    \"match\": {\n      \"city\": \"madrid\"\n    }\n  },\n  \"aggs\": {\n    \"tags\": {\n      \"significant_terms\": {\n        \"field\": \"tag\",\n        \"background_filter\": {\n          \"term\": { \"text\": \"spain\" }\n        }\n      }\n    }\n  }\n}\n```\n\nThe above filter would help focus in on terms that were peculiar to the city of Madrid rather than revealing terms like \"Spanish\" that are unusual in the full index\u2019s worldwide context but commonplace in the subset of documents containing the word \"Spain\".\n\n::::{warning}\nUse of background filters will slow the query as each term\u2019s postings must be filtered to determine a frequency\n::::\n\n\n\n### Filtering Values [_filtering_values_2]\n\nIt is possible (although rarely required) to filter the values for which buckets will be created. This can be done using the `include` and `exclude` parameters which are based on a regular expression string or arrays of exact terms. This functionality mirrors the features described in the [terms aggregation](/reference/aggregations/search-aggregations-bucket-terms-aggregation.md) documentation.\n\n\n\n## Collect mode [_collect_mode]\n\nTo avoid memory issues, the `significant_terms` aggregation always computes child aggregations in `breadth_first` mode. A description of the different collection modes can be found in the [terms aggregation](/reference/aggregations/search-aggregations-bucket-terms-aggregation.md#search-aggregations-bucket-terms-aggregation-collect) documentation.\n\n\n## Execution hint [_execution_hint_2]\n\nThere are different mechanisms by which terms aggregations can be executed:\n\n* by using field values directly in order to aggregate data per-bucket (`map`)\n* by using [global ordinals](/reference/elasticsearch/mapping-reference/eager-global-ordinals.md) of the field and allocating one bucket per global ordinal (`global_ordinals`)\n\nElasticsearch tries to have sensible defaults so this is something that generally doesn\u2019t need to be configured.\n\n`global_ordinals` is the default option for `keyword` field, it uses global ordinals to allocates buckets dynamically so memory usage is linear to the number of values of the documents that are part of the aggregation scope.\n\n`map` should only be considered when very few documents match a query. Otherwise the ordinals-based execution mode is significantly faster. By default, `map` is only used when running an aggregation on scripts, since they don\u2019t have ordinals.\n\n$$$significantterms-aggregation-execution-hint-example$$$\n\n```console\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"significant_terms\": {\n        \"field\": \"tags\",\n        \"execution_hint\": \"map\" <1>\n      }\n    }\n  }\n}\n```\n\n1. the possible values are `map`, `global_ordinals`\n\n\nPlease note that Elasticsearch will ignore this execution hint if it is not applicable.",
              "filename": "elasticsearch-main/docs/reference/aggregations/search-aggregations-bucket-significantterms-aggregation.md"
            }
          ],
          "tool_call_id": "call_rFXywSX5NsY5td7xdHVhREqe",
          "metadata": null,
          "timestamp": "2025-10-03T04:04:36.017382+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "In Elasticsearch, the **terms aggregation** has two main execution mechanisms:\n\n1. **Global Ordinals**: This is the default mechanism for aggregating on `keyword` fields. It allocates one bucket per global ordinal, allowing for efficient memory usage and faster performance. It dynamically allocates memory based on the unique values in the specified field and is linear with respect to the number of values being aggregated.\n\n2. **Map**: This approach directly uses field values to aggregate data per bucket. It is generally less efficient than the global ordinals method, especially for large datasets, and should only be used when very few documents match the query. By default, this method is utilized when running aggregations on scripts since they don't have ordinals.\n\nAdditionally, the aggregation can be configured via the `execution_hint` parameter to specify which mechanism to use.\n\nThese execution hints are generally specified as follows in the aggregation request:\n\n```json\n\"execution_hint\": \"map\"  // or \"global_ordinals\"\n```\n\nFor detailed configurations and examples, refer to the following sections from the Elasticsearch documentation:\n- [Terms Aggregation Execution Hint](https://github.com/elastic/elasticsearch/blob/main/docs/reference/aggregations/search-aggregations-bucket-terms-aggregation.md) (see the section on Execution Hint).\n- [Significant Terms Aggregation Execution Hint](https://github.com/elastic/elasticsearch/blob/main/docs/reference/aggregations/search-aggregations-bucket-significantterms-aggregation.md) (contains similar execution mechanisms).\n\nThese references can provide additional insights into the performance implications and appropriate scenarios for each execution method.",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 22390,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 325,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:04:36+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMRK0VzUh3B09stxIvhqK0CxGRgqg",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}