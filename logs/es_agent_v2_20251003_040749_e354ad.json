{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How do you calculate the L2 norm similarity between dense vectors in Elasticsearch?",
          "timestamp": "2025-10-03T04:07:47.812667+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"L2 norm similarity dense vectors Elasticsearch\"}",
          "tool_call_id": "call_nfLKQxxb2Qwxxln2nx1qSCla",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 216,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 19,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:07:48+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMRN6mE7JGUgNWrowm1ufzSCzcUxE",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "Dense vector",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html"
              ],
              "content": "# Dense vector field type [dense-vector]\n\nThe `dense_vector` field type stores dense vectors of numeric values. Dense vector fields are primarily used for [k-nearest neighbor (kNN) search](docs-content://deploy-manage/production-guidance/optimize-performance/approximate-knn-search.md).\n\nThe `dense_vector` type does not support aggregations or sorting.\n\nYou add a `dense_vector` field as an array of numeric values based on [`element_type`](#dense-vector-params) with `float` by default:\n\n```console\nPUT my-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3\n      },\n      \"my_text\" : {\n        \"type\" : \"keyword\"\n      }\n    }\n  }\n}\n\nPUT my-index/_doc/1\n{\n  \"my_text\" : \"text1\",\n  \"my_vector\" : [0.5, 10, 6]\n}\n\nPUT my-index/_doc/2\n{\n  \"my_text\" : \"text2\",\n  \"my_vector\" : [-0.5, 10, 10]\n}\n```\n\n::::{note}\nUnlike most other data types, dense vectors are always single-valued. It is not possible to store multiple values in one `dense_vector` field.\n::::\n\n## Index vectors for kNN search [index-vectors-knn-search]\n\nA *k-nearest neighbor* (kNN) search finds the *k* nearest vectors to a query vector, as measured by a similarity metric.\n\nDense vector fields can be used to rank documents in [`script_score` queries](/reference/query-languages/query-dsl/query-dsl-script-score-query.md). This lets you perform a brute-force kNN search by scanning all documents and ranking them by similarity.\n\nIn many cases, a brute-force kNN search is not efficient enough. For this reason, the `dense_vector` type supports indexing vectors into a specialized data structure to support fast kNN retrieval through the [`knn` option](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-search) in the search API\n\nUnmapped array fields of float elements with size between 128 and 4096 are dynamically mapped as `dense_vector` with a default similariy of `cosine`. You can override the default similarity by explicitly mapping the field as `dense_vector` with the desired similarity.\n\nIndexing is enabled by default for dense vector fields and indexed as `bbq_hnsw` if dimensions are greater than or equal to 384, otherwise they are indexed as `int8_hnsw`. {applies_to}`stack: ga 9.1`\n\n:::{note}\nIn {{stack}} 9.0, dense vector fields are always indexed as `int8_hnsw`.\n:::\n\n\nWhen indexing is enabled, you can define the vector similarity to use in kNN search:\n\n```console\nPUT my-index-2\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"similarity\": \"dot_product\"\n      }\n    }\n  }\n}\n```\n\n::::{note}\nIndexing vectors for approximate kNN search is an expensive process. It can take substantial time to ingest documents that contain vector fields with `index` enabled. See [k-nearest neighbor (kNN) search](docs-content://deploy-manage/production-guidance/optimize-performance/approximate-knn-search.md) to learn more about the memory requirements.\n::::\n\nYou can disable indexing by setting the `index` parameter to `false`:\n\n```console\nPUT my-index-2\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": false\n      }\n    }\n  }\n}\n```\n\n{{es}} uses the [HNSW algorithm](https://arxiv.org/abs/1603.09320) to support efficient kNN search. Like most kNN algorithms, HNSW is an approximate method that sacrifices result accuracy for improved speed.\n\n## Accessing `dense_vector` fields in search responses\n```{applies_to}\nstack: ga 9.2\nserverless: ga\n```\n\nBy default, `dense_vector` fields are **not included in `_source`** in responses from the `_search`, `_msearch`, `_get`, and `_mget` APIs.\nThis helps reduce response size and improve performance, especially in scenarios where vectors are used solely for similarity scoring and not required in the output.\n\nTo retrieve vector values explicitly, you can use:\n\n* The `fields` option to request specific vector fields directly:\n\n  ```console\n  POST my-index-2/_search\n  {\n    \"fields\": [\"my_vector\"]\n  }\n  ```\n\n- The `_source.exclude_vectors` flag to re-enable vector inclusion in `_source` responses:\n\n  ```console\n  POST my-index-2/_search\n  {\n    \"_source\": {\n      \"exclude_vectors\": false\n    }\n  }\n  ```\n\n### Storage behavior and `_source`\n\nBy default, `dense_vector` fields are **not stored in `_source`** on disk. This is also controlled by the index setting `index.mapping.exclude_source_vectors`.\nThis setting is enabled by default for newly created indices and can only be set at index creation time.\n\nWhen enabled:\n\n* `dense_vector` fields are removed from `_source` and the rest of the `_source` is stored as usual.\n* If a request includes `_source` and vector values are needed (e.g., during recovery or reindex), the vectors are rehydrated from their internal format.\n\nThis setting is compatible with synthetic `_source`, where the entire `_source` document is reconstructed from columnar storage. In full synthetic mode, no `_source` is stored on disk, and all fields \u2014 including vectors \u2014 are rebuilt when needed.\n\n### Rehydration and precision\n\nWhen vector values are rehydrated (e.g., for reindex, recovery, or explicit `_source` requests), they are restored from their internal format. Internally, vectors are stored at float precision, so if they were originally indexed as higher-precision types (e.g., `double` or `long`), the rehydrated values will have reduced precision. This lossy representation is intended to save space while preserving search quality.\n\n### Storing original vectors in `_source`\n\nIf you want to preserve the original vector values exactly as they were provided, you can re-enable vector storage in `_source`:\n\n```console\nPUT my-index-include-vectors\n{\n  \"settings\": {\n    \"index.mapping.exclude_source_vectors\": false\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\"\n      }\n    }\n  }\n}\n```\n\nWhen this setting is disabled:\n\n* `dense_vector` fields are stored as part of the `_source`, exactly as indexed.\n* The index will store both the original `_source` value and the internal representation used for vector search, resulting in increased storage usage.\n* Vectors are once again returned in `_source` by default in all relevant APIs, with no need to use `exclude_vectors` or `fields`.\n\nThis configuration is appropriate when full source fidelity is required, such as for auditing or round-tripping exact input values.\n\n## Automatically quantize vectors for kNN search [dense-vector-quantization]\n\nThe `dense_vector` type supports quantization to reduce the memory footprint required when [searching](docs-content://solutions/search/vector/knn.md#approximate-knn) `float` vectors. The three following quantization strategies are supported:\n\n* `int8` - Quantizes each dimension of the vector to 1-byte integers. This reduces the memory footprint by 75% (or 4x) at the cost of some accuracy.\n* `int4` - Quantizes each dimension of the vector to half-byte integers. This reduces the memory footprint by 87% (or 8x) at the cost of accuracy.\n* `bbq` - Better binary quantization which reduces each dimension to a single bit precision. This reduces the memory footprint by 96% (or 32x) at a larger cost of accuracy. Generally, oversampling during query time and reranking can help mitigate the accuracy loss.\n\nWhen using a quantized format, you may want to oversample and rescore the results to improve accuracy. See [oversampling and rescoring](docs-content://solutions/search/vector/knn.md#dense-vector-knn-search-rescoring) for more information.\n\nTo use a quantized index, you can set your index type to `int8_hnsw`, `int4_hnsw`, or `bbq_hnsw`. When indexing `float` vectors, the current default index type is `bbq_hnsw` for vectors with greater than or equal to 384 dimensions, otherwise it's `int8_hnsw`.\n\n:::{note}\nIn {{stack}} 9.0, dense vector fields are always indexed as `int8_hnsw`.\n:::\n\nQuantized vectors can use [oversampling and rescoring](docs-content://solutions/search/vector/knn.md#dense-vector-knn-search-rescoring) to improve accuracy on approximate kNN search results.\n\n::::{note}\nQuantization will continue to keep the raw float vector values on disk for reranking, reindexing, and quantization improvements over the lifetime of the data. This means disk usage will increase by ~25% for `int8`, ~12.5% for `int4`, and ~3.1% for `bbq` due to the overhead of storing the quantized and raw vectors.\n::::\n\n::::{note}\n`int4` quantization requires an even number of vector dimensions.\n::::\n\n::::{note}\n`bbq` quantization only supports vector dimensions that are greater than 64.\n::::\n\nHere is an example of how to create a byte-quantized index:\n\n```console\nPUT my-byte-quantized-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": true,\n        \"index_options\": {\n          \"type\": \"int8_hnsw\"\n        }\n      }\n    }\n  }\n}\n```\n\nHere is an example of how to create a half-byte-quantized index:\n\n```console\nPUT my-byte-quantized-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 4,\n        \"index\": true,\n        \"index_options\": {\n          \"type\": \"int4_hnsw\"\n        }\n      }\n    }\n  }\n}\n```\n\nHere is an example of how to create a binary quantized index:\n\n```console\nPUT my-byte-quantized-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 64,\n        \"index\": true,\n        \"index_options\": {\n          \"type\": \"bbq_hnsw\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Parameters for dense vector fields [dense-vector-params]\n\nThe following mapping parameters are accepted:\n\n$$$dense-vector-element-type$$$\n\n`element_type`\n:   (Optional, string) The data type used to encode vectors. The supported data types are `float` (default), `byte`, and `bit`.\n\n::::{dropdown} Valid values for element_type\n`float`\n:   indexes a 4-byte floating-point value per dimension. This is the default value.\n\n`byte`\n:   indexes a 1-byte integer value per dimension.\n\n`bit`\n:   indexes a single bit per dimension. Useful for very high-dimensional vectors or models that specifically support bit vectors. NOTE: when using `bit`, the number of dimensions must be a multiple of 8 and must represent the number of bits.\n\n::::\n\n`dims`\n:   (Optional, integer) Number of vector dimensions. Can\u2019t exceed `4096`. If `dims` is not specified, it will be set to the length of the first vector added to the field.\n\n`index`\n:   (Optional, Boolean) If `true`, you can search this field using the [knn query](/reference/query-languages/query-dsl/query-dsl-knn-query.md) or [knn in _search](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-search) . Defaults to `true`.\n\n$$$dense-vector-similarity$$$\n\n`similarity`\n:   (Optional[\u00b9](#footnote-1), string) The vector similarity metric to use in kNN search. Documents are ranked by their vector field\u2019s similarity to the query vector. The `_score` of each document will be derived from the similarity, in a way that ensures scores are positive and that a larger score corresponds to a higher ranking. Defaults to `l2_norm` when `element_type: bit` otherwise defaults to `cosine`.\n\n    \u00b9 $$$footnote-1$$$ This parameter can only be specified when `index` is `true`.\n\n    ::::{note}\n    `bit` vectors only support `l2_norm` as their similarity metric.\n    ::::\n\n::::{dropdown} Valid values for similarity\n`l2_norm`\n:   Computes similarity based on the L2 distance (also known as Euclidean distance) between the vectors. The document `_score` is computed as `1 / (1 + l2_norm(query, vector)^2)`.\n\n    For `bit` vectors, instead of using `l2_norm`, the `hamming` distance between the vectors is used. The `_score` transformation is `(numBits - hamming(a, b)) / numBits`\n\n`dot_product`\n:   Computes the dot product of two unit vectors. This option provides an optimized way to perform cosine similarity. The constraints and computed score are defined by `element_type`.\n\n    When `element_type` is `float`, all vectors must be unit length, including both document and query vectors. The document `_score` is computed as `(1 + dot_product(query, vector)) / 2`.\n\n    When `element_type` is `byte`, all vectors must have the same length including both document and query vectors or results will be inaccurate. The document `_score` is computed as `0.5 + (dot_product(query, vector) / (32768 * dims))` where `dims` is the number of dimensions per vector.\n\n`cosine`\n:   Computes the cosine similarity. During indexing {{es}} automatically normalizes vectors with `cosine` similarity to unit length. This allows to internally use `dot_product` for computing similarity, which is more efficient. Original un-normalized vectors can be still accessed through scripts. The document `_score` is computed as `(1 + cosine(query, vector)) / 2`. The `cosine` similarity does not allow vectors with zero magnitude, since cosine is not defined in this case.\n\n`max_inner_product`\n:   Computes the maximum inner product of two vectors. This is similar to `dot_product`, but doesn\u2019t require vectors to be normalized. This means that each vector\u2019s magnitude can significantly effect the score. The document `_score` is adjusted to prevent negative values. For `max_inner_product` values `< 0`, the `_score` is `1 / (1 + -1 * max_inner_product(query, vector))`. For non-negative `max_inner_product` results the `_score` is calculated `max_inner_product(query, vector) + 1`.\n\n::::\n\n::::{note}\nAlthough they are conceptually related, the `similarity` parameter is different from [`text`](/reference/elasticsearch/mapping-reference/text.md) field [`similarity`](/reference/elasticsearch/mapping-reference/similarity.md) and accepts a distinct set of options.\n::::\n\n$$$dense-vector-index-options$$$\n\n`index_options`\n:   (Optional[\u00b2](#footnote-2), object) An optional section that configures the kNN indexing algorithm. The HNSW algorithm has two internal parameters that influence how the data structure is built. These can be adjusted to improve the accuracy of results, at the expense of slower indexing speed.\n\n    \u00b2 $$$footnote-2$$$ This parameter can only be specified when `index` is `true`.\n\n::::{dropdown} Properties of index_options\n`type`\n:   (Required, string) The type of kNN algorithm to use. Can be either any of:\n    * `hnsw` - This utilizes the [HNSW algorithm](https://arxiv.org/abs/1603.09320) for scalable approximate kNN search. This supports all `element_type` values.\n    * `int8_hnsw` - The default index type for some float vectors:\n      * {applies_to}`stack: ga 9.1` Default for float vectors with less than 384 dimensions.\n      * {applies_to}`stack: ga 9.0` Default for float all vectors.\n      This utilizes the [HNSW algorithm](https://arxiv.org/abs/1603.09320) in addition to automatically scalar quantization for scalable approximate kNN search with `element_type` of `float`. This can reduce the memory footprint by 4x at the cost of some accuracy. See [Automatically quantize vectors for kNN search](#dense-vector-quantization).\n    * `int4_hnsw` - This utilizes the [HNSW algorithm](https://arxiv.org/abs/1603.09320) in addition to automatically scalar quantization for scalable approximate kNN search with `element_type` of `float`. This can reduce the memory footprint by 8x at the cost of some accuracy. See [Automatically quantize vectors for kNN search](#dense-vector-quantization).\n    * `bbq_hnsw` - This utilizes the [HNSW algorithm](https://arxiv.org/abs/1603.09320) in addition to automatically binary quantization for scalable approximate kNN search with `element_type` of `float`. This can reduce the memory footprint by 32x at the cost of accuracy. See [Automatically quantize vectors for kNN search](#dense-vector-quantization).\n\n      {applies_to}`stack: ga 9.1` `bbq_hnsw` is the default index type for float vectors with greater than or equal to 384 dimensions.\n    * `flat` - This utilizes a brute-force search algorithm for exact kNN search. This supports all `element_type` values.\n    * `int8_flat` - This utilizes a brute-force search algorithm in addition to automatic scalar quantization. Only supports `element_type` of `float`.\n    * `int4_flat` - This utilizes a brute-force search algorithm in addition to automatic half-byte scalar quantization. Only supports `element_type` of `float`.\n    * `bbq_flat` - This utilizes a brute-force search algorithm in addition to automatic binary quantization. Only supports `element_type` of `float`.\n    * {applies_to}`stack: ga 9.2` `bbq_disk` - This utilizes a variant of [k-means clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering) in addition to automatic binary quantization to partition vectors and search subspaces rather than an entire graph structure as in with HNSW. Only supports `element_type` of `float`.  This combines the benefits of BBQ quantization with partitioning to further reduces the required memory overhead when compared with HNSW and can effectively be run at the smallest possible RAM and heap sizes when HNSW would otherwise cause swapping and grind to a halt.  DiskBBQ largely scales linearlly with the total RAM.  And search performance is enhanced at scale as a subset of the total vector space is loaded.\n\n`m`\n:   (Optional, integer) The number of neighbors each node will be connected to in the HNSW graph. Defaults to `16`. Only applicable to `hnsw`, `int8_hnsw`, `int4_hnsw` and `bbq_hnsw` index types.\n\n`ef_construction`\n:   (Optional, integer) The number of candidates to track while assembling the list of nearest neighbors for each new node. Defaults to `100`. Only applicable to `hnsw`, `int8_hnsw`, `int4_hnsw` and `bbq_hnsw` index types.\n\n`confidence_interval`\n:   (Optional, float) Only applicable to `int8_hnsw`, `int4_hnsw`, `int8_flat`, and `int4_flat` index types. The confidence interval to use when quantizing the vectors. Can be any value between and including `0.90` and `1.0` or exactly `0`. When the value is `0`, this indicates that dynamic quantiles should be calculated for optimized quantization. When between `0.90` and `1.0`, this value restricts the values used when calculating the quantization thresholds. For example, a value of `0.95` will only use the middle 95% of the values when calculating the quantization thresholds (e.g. the highest and lowest 2.5% of values will be ignored). Defaults to `1/(dims + 1)` for `int8` quantized vectors and `0` for `int4` for dynamic quantile calculation.\n\n`default_visit_percentage` {applies_to}`stack: ga 9.2`\n:   (Optional, integer) Only applicable to `bbq_disk`. Must be between 0 and 100.  0 will default to using `num_candidates` for calculating the percent visited. Increasing `default_visit_percentage` tends to improve the accuracy of the final results. Defaults to ~1% per shard for every 1 million vectors.\n\n`cluster_size` {applies_to}`stack: ga 9.2`\n:   (Optional, integer) Only applicable to `bbq_disk`.  The number of vectors per cluster.  Smaller cluster sizes increases accuracy at the cost of performance. Defaults to `384`. Must be a value between `64` and `65536`.\n\n`rescore_vector` {applies_to}`stack: preview 9.0, ga 9.1`\n:   (Optional, object) An optional section that configures automatic vector rescoring on knn queries for the given field. Only applicable to quantized index types.\n:::::{dropdown} Properties of rescore_vector\n`oversample`\n:   (required, float) The amount to oversample the search results by. This value should be one of the following:\n    * Greater than `1.0` and less than `10.0`\n    * Exactly `0` to indicate no oversampling and rescoring should occur {applies_to}`stack: ga 9.1`\n    :   The higher the value, the more vectors will be gathered and rescored with the raw values per shard.\n    :   In case a knn query specifies a `rescore_vector` parameter, the query `rescore_vector` parameter will be used instead.\n    :   See [oversampling and rescoring quantized vectors](docs-content://solutions/search/vector/knn.md#dense-vector-knn-search-rescoring) for details.\n:::::\n::::\n\n\n\n## Synthetic `_source` [dense-vector-synthetic-source]\n\n`dense_vector` fields support [synthetic `_source`](/reference/elasticsearch/mapping-reference/mapping-source-field.md#synthetic-source) .\n\n\n## Indexing and searching bit vectors [dense-vector-index-bit]\n\nWhen using `element_type: bit`, this will treat all vectors as bit vectors. Bit vectors utilize only a single bit per dimension and are internally encoded as bytes. This can be useful for very high-dimensional vectors or models.\n\nWhen using `bit`, the number of dimensions must be a multiple of 8 and must represent the number of bits. Additionally, with `bit` vectors, the typical vector similarity values are effectively all scored the same, e.g. with `hamming` distance.\n\nLet\u2019s compare two `byte[]` arrays, each representing 40 individual bits.\n\n`[-127, 0, 1, 42, 127]` in bits `1000000100000000000000010010101001111111` `[127, -127, 0, 1, 42]` in bits `0111111110000001000000000000000100101010`\n\nWhen comparing these two bit, vectors, we first take the [`hamming` distance](https://en.wikipedia.org/wiki/Hamming_distance).\n\n`xor` result:\n\n```\n1000000100000000000000010010101001111111\n^\n0111111110000001000000000000000100101010\n=\n1111111010000001000000010010101101010101\n```\n\nThen, we gather the count of `1` bits in the `xor` result: `18`. To scale for scoring, we subtract from the total number of bits and divide by the total number of bits: `(40 - 18) / 40 = 0.55`. This would be the `_score` betwee these two vectors.\n\nHere is an example of indexing and searching bit vectors:\n\n```console\nPUT my-bit-vectors\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 40, <1>\n        \"element_type\": \"bit\"\n      }\n    }\n  }\n}\n```\n\n1. The number of dimensions that represents the number of bits\n\n\n```console\nPOST /my-bit-vectors/_bulk?refresh\n{\"index\": {\"_id\" : \"1\"}}\n{\"my_vector\": [127, -127, 0, 1, 42]} <1>\n{\"index\": {\"_id\" : \"2\"}}\n{\"my_vector\": \"8100012a7f\"} <2>\n```\n\n1. 5 bytes representing the 40 bit dimensioned vector\n2. A hexidecimal string representing the 40 bit dimensioned vector\n\n\nThen, when searching, you can use the `knn` query to search for similar bit vectors:\n\n```console\nPOST /my-bit-vectors/_search?filter_path=hits.hits\n{\n  \"query\": {\n    \"knn\": {\n      \"query_vector\": [127, -127, 0, 1, 42],\n      \"field\": \"my_vector\"\n    }\n  }\n}\n```\n\n```console-result\n{\n    \"hits\": {\n        \"hits\": [\n            {\n                \"_index\": \"my-bit-vectors\",\n                \"_id\": \"1\",\n                \"_score\": 1.0,\n                \"_source\": {\n                    \"my_vector\": [\n                        127,\n                        -127,\n                        0,\n                        1,\n                        42\n                    ]\n                }\n            },\n            {\n                \"_index\": \"my-bit-vectors\",\n                \"_id\": \"2\",\n                \"_score\": 0.55,\n                \"_source\": {\n                    \"my_vector\": \"8100012a7f\"\n                }\n            }\n        ]\n    }\n}\n```\n\n\n## Updatable field type [_updatable_field_type]\n\nTo better accommodate scaling and performance needs, updating the `type` setting in `index_options` is possible with the [Update Mapping API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-put-mapping), according to the following graph (jumps allowed):\n\n::::{tab-set}\n:::{tab-item} {{stack}} 9.1+\n```txt\nflat --> int8_flat --> int4_flat --> bbq_flat --> hnsw --> int8_hnsw --> int4_hnsw --> bbq_hnsw\n```\n:::\n:::{tab-item} {{stack}} 9.0\n```txt\nflat --> int8_flat --> int4_flat --> hnsw --> int8_hnsw --> int4_hnsw\n```\n:::\n::::\n\nFor updating all HNSW types (`hnsw`, `int8_hnsw`, `int4_hnsw`, `bbq_hnsw`) the number of connections `m` must either stay the same or increase. For the scalar quantized formats  `int8_flat`, `int4_flat`, `int8_hnsw` and `int4_hnsw` the `confidence_interval` must always be consistent (once defined, it cannot change).\n\nUpdating `type` in `index_options` will fail in all other scenarios.\n\nSwitching `types` won\u2019t re-index vectors that have already been indexed (they will keep using their original `type`), vectors being indexed after the change will use the new `type` instead.\n\nFor example, it\u2019s possible to define a dense vector field that utilizes the `flat` type (raw float32 arrays) for a first batch of data to be indexed.\n\n```console\nPUT my-index-000001\n{\n    \"mappings\": {\n        \"properties\": {\n            \"text_embedding\": {\n                \"type\": \"dense_vector\",\n                \"dims\": 384,\n                \"index_options\": {\n                    \"type\": \"flat\"\n                }\n            }\n        }\n    }\n}\n```\n\nChanging the `type` to `int4_hnsw` makes sure vectors indexed after the change will use an int4 scalar quantized representation and HNSW (e.g., for KNN queries). That includes new segments created by [merging](/reference/elasticsearch/index-settings/merge.md) previously created segments.\n\n```console\nPUT /my-index-000001/_mapping\n{\n    \"properties\": {\n        \"text_embedding\": {\n            \"type\": \"dense_vector\",\n            \"dims\": 384,\n            \"index_options\": {\n                \"type\": \"int4_hnsw\"\n            }\n        }\n    }\n}\n```\n\nVectors indexed before this change will keep using the `flat` type (raw float32 representation and brute force search for KNN queries).\n\nIn order to have all the vectors updated to the new type, either reindexing or force merging should be used.\n\nFor debugging purposes, it\u2019s possible to inspect how many segments (and docs) exist for each `type` with the [Index Segments API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-segments).",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/mapping-reference/dense-vector.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html"
              ],
              "navigation_title": "Similarity",
              "applies_to": {
                "stack": "all",
                "serverless": "all"
              },
              "content": "# Similarity settings [index-modules-similarity]\n\nA similarity (scoring / ranking model) defines how matching documents are scored. Similarity is per field, meaning that via the mapping one can define a different similarity per field.\n\nSimilarity is only applicable for text type and keyword type fields.\n\nConfiguring a custom similarity is considered an expert feature and the builtin similarities are most likely sufficient as is described in [`similarity`](/reference/elasticsearch/mapping-reference/similarity.md).\n\n\n## Configuring a similarity [configuration]\n\nMost existing or custom Similarities have configuration options which can be configured via the index settings as shown below. The index options can be provided when creating an index or updating index settings.\n\n```console\nPUT /index\n{\n  \"settings\": {\n    \"index\": {\n      \"similarity\": {\n        \"my_similarity\": {\n          \"type\": \"DFR\",\n          \"basic_model\": \"g\",\n          \"after_effect\": \"l\",\n          \"normalization\": \"h2\",\n          \"normalization.h2.c\": \"3.0\"\n        }\n      }\n    }\n  }\n}\n```\n\nHere we configure the DFR similarity so it can be referenced as `my_similarity` in mappings as is illustrate in the below example:\n\n```console\nPUT /index/_mapping\n{\n  \"properties\" : {\n    \"title\" : { \"type\" : \"text\", \"similarity\" : \"my_similarity\" }\n  }\n}\n```\n\n\n## Available similarities [_available_similarities]\n\n\n### BM25 similarity (**default**) [bm25]\n\nTF/IDF based similarity that has built-in tf normalization and is supposed to work better for short fields (like names). See [Okapi_BM25](https://en.wikipedia.org/wiki/Okapi_BM25) for more details. This similarity has the following options:\n\n`k1`\n:   Controls non-linear term frequency normalization (saturation). The default value is `1.2`.\n\n`b`\n:   Controls to what degree document length normalizes tf values. The default value is `0.75`.\n\n`discount_overlaps`\n:   Determines whether overlap tokens (Tokens with 0 position increment) are ignored when computing norm. By default this is true, meaning overlap tokens do not count when computing norms.\n\nType name: `BM25`\n\n\n### DFR similarity [dfr]\n\nSimilarity that implements the [divergence from randomness](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/DFRSimilarity.md) framework. This similarity has the following options:\n\n`basic_model`\n:   Possible values: [`g`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/BasicModelG.md), [`if`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/BasicModelIF.md), [`in`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/BasicModelIn.md) and [`ine`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/BasicModelIne.md).\n\n`after_effect`\n:   Possible values: [`b`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/AfterEffectB.md) and [`l`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/AfterEffectL.md).\n\n`normalization`\n:   Possible values: [`no`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/Normalization.NoNormalization.md), [`h1`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/NormalizationH1.md), [`h2`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/NormalizationH2.md), [`h3`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/NormalizationH3.md) and [`z`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/NormalizationZ.md).\n\nAll options but the first option need a normalization value.\n\nType name: `DFR`\n\n\n### DFI similarity [dfi]\n\nSimilarity that implements the [divergence from independence](https://trec.nist.gov/pubs/trec21/papers/irra.web.nb.pdf) model. This similarity has the following options:\n\n`independence_measure`\n:   Possible values [`standardized`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/IndependenceStandardized.md), [`saturated`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/IndependenceSaturated.md), [`chisquared`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/IndependenceChiSquared.md).\n\nWhen using this similarity, it is highly recommended **not** to remove stop words to get good relevance. Also beware that terms whose frequency is less than the expected frequency will get a score equal to 0.\n\nType name: `DFI`\n\n\n### IB similarity. [ib]\n\n[Information based model](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/IBSimilarity.md) . The algorithm is based on the concept that the information content in any symbolic *distribution* sequence is primarily determined by the repetitive usage of its basic elements. For written texts this challenge would correspond to comparing the writing styles of different authors. This similarity has the following options:\n\n`distribution`\n:   Possible values: [`ll`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/DistributionLL.md) and [`spl`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/DistributionSPL.md).\n\n`lambda`\n:   Possible values: [`df`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/LambdaDF.md) and [`ttf`](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/LambdaTTF.md).\n\n`normalization`\n:   Same as in `DFR` similarity.\n\nType name: `IB`\n\n\n### LM Dirichlet similarity. [lm_dirichlet]\n\n[LM Dirichlet similarity](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.md) . This similarity has the following options:\n\n`mu`\n:   Default to `2000`.\n\nThe scoring formula in the paper assigns negative scores to terms that have fewer occurrences than predicted by the language model, which is illegal to Lucene, so such terms get a score of 0.\n\nType name: `LMDirichlet`\n\n\n### LM Jelinek Mercer similarity. [lm_jelinek_mercer]\n\n[LM Jelinek Mercer similarity](https://lucene.apache.org/core/10_0_0/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.md) . The algorithm attempts to capture important patterns in the text, while leaving out noise. This similarity has the following options:\n\n`lambda`\n:   The optimal value depends on both the collection and the query. The optimal value is around `0.1` for title queries and `0.7` for long queries. Default to `0.1`. When value approaches `0`, documents that match more query terms will be ranked higher than those that match fewer terms.\n\nType name: `LMJelinekMercer`\n\n\n### Scripted similarity [scripted_similarity]\n\nA similarity that allows you to use a script in order to specify how scores should be computed. For instance, the below example shows how to reimplement TF-IDF:\n\n```console\nPUT /index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"similarity\": {\n      \"scripted_tfidf\": {\n        \"type\": \"scripted\",\n        \"script\": {\n          \"source\": \"double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"field\": {\n        \"type\": \"text\",\n        \"similarity\": \"scripted_tfidf\"\n      }\n    }\n  }\n}\n\nPUT /index/_doc/1\n{\n  \"field\": \"foo bar foo\"\n}\n\nPUT /index/_doc/2\n{\n  \"field\": \"bar baz\"\n}\n\nPOST /index/_refresh\n\nGET /index/_search?explain=true\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"foo^1.7\",\n      \"default_field\": \"field\"\n    }\n  }\n}\n```\n\nWhich yields:\n\n```console-result\n{\n  \"took\": 12,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 1.9508477,\n    \"hits\": [\n      {\n        \"_shard\": \"[index][0]\",\n        \"_node\": \"OzrdjxNtQGaqs4DmioFw9A\",\n        \"_index\": \"index\",\n        \"_id\": \"1\",\n        \"_score\": 1.9508477,\n        \"_source\": {\n          \"field\": \"foo bar foo\"\n        },\n        \"_explanation\": {\n          \"value\": 1.9508477,\n          \"description\": \"weight(field:foo in 0) [PerFieldSimilarity], result of:\",\n          \"details\": [\n            {\n              \"value\": 1.9508477,\n              \"description\": \"score from ScriptedSimilarity(weightScript=[null], script=[Script{type=inline, lang='painless', idOrCode='double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;', options={}, params={}}]) computed from:\",\n              \"details\": [\n                {\n                  \"value\": 1.0,\n                  \"description\": \"weight\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 1.7,\n                  \"description\": \"query.boost\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 2,\n                  \"description\": \"field.docCount\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 4,\n                  \"description\": \"field.sumDocFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 5,\n                  \"description\": \"field.sumTotalTermFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 1,\n                  \"description\": \"term.docFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 2,\n                  \"description\": \"term.totalTermFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 2.0,\n                  \"description\": \"doc.freq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 3,\n                  \"description\": \"doc.length\",\n                  \"details\": []\n                }\n              ]\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n```\n\n::::{warning}\nWhile scripted similarities provide a lot of flexibility, there is a set of rules that they need to satisfy. Failing to do so could make Elasticsearch silently return wrong top hits or fail with internal errors at search time:\n::::\n\n\n* Returned scores must be positive.\n* All other variables remaining equal, scores must not decrease when `doc.freq` increases.\n* All other variables remaining equal, scores must not increase when `doc.length` increases.\n\nYou might have noticed that a significant part of the above script depends on statistics that are the same for every document. It is possible to make the above slightly more efficient by providing an `weight_script` which will compute the document-independent part of the score and will be available under the `weight` variable. When no `weight_script` is provided, `weight` is equal to `1`. The `weight_script` has access to the same variables as the `script` except `doc` since it is supposed to compute a document-independent contribution to the score.\n\nThe below configuration will give the same tf-idf scores but is slightly more efficient:\n\n```console\nPUT /index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"similarity\": {\n      \"scripted_tfidf\": {\n        \"type\": \"scripted\",\n        \"weight_script\": {\n          \"source\": \"double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; return query.boost * idf;\"\n        },\n        \"script\": {\n          \"source\": \"double tf = Math.sqrt(doc.freq); double norm = 1/Math.sqrt(doc.length); return weight * tf * norm;\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"field\": {\n        \"type\": \"text\",\n        \"similarity\": \"scripted_tfidf\"\n      }\n    }\n  }\n}\n```\n\nType name: `scripted`\n\n\n### Default Similarity [default-base]\n\nBy default, Elasticsearch will use whatever similarity is configured as `default`.\n\nYou can change the default similarity for all fields in an index when it is [created](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-create):\n\n```console\nPUT /index\n{\n  \"settings\": {\n    \"index\": {\n      \"similarity\": {\n        \"default\": {\n          \"type\": \"boolean\"\n        }\n      }\n    }\n  }\n}\n```\n\nIf you want to change the default similarity after creating the index you must [close](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-open) your index, send the following request and [open](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-open) it again afterwards:\n\n```console\nPOST /index/_close\n\nPUT /index/_settings\n{\n  \"index\": {\n    \"similarity\": {\n      \"default\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nPOST /index/_open\n```",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/index-settings/similarity.md"
            },
            {
              "navigation_title": "Sparse vector",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-sparse-vector-query.html"
              ],
              "content": "# Sparse vector query [query-dsl-sparse-vector-query]\n\n\nThe sparse vector query executes a query consisting of sparse vectors, such as built by a learned sparse retrieval model. This can be achieved with one of two strategies:\n\n* Using an {{nlp}} model to convert query text into a list of token-weight pairs\n* Sending in precalculated token-weight pairs as query vectors\n\nThese token-weight pairs are then used in a query against a [sparse vector](/reference/elasticsearch/mapping-reference/sparse-vector.md) or a [semantic_text](/reference/elasticsearch/mapping-reference/semantic-text.md) field with a compatible sparse inference model. At query time, query vectors are calculated using the same inference model that was used to create the tokens. When querying, these query vectors are ORed together with their respective weights, which means scoring is effectively a [dot product](/reference/query-languages/query-dsl/query-dsl-script-score-query.md#vector-functions-dot-product) calculation between stored dimensions and query dimensions.\n\nFor example, a stored vector `{\"feature_0\": 0.12, \"feature_1\": 1.2, \"feature_2\": 3.0}` with query vector `{\"feature_0\": 2.5, \"feature_2\": 0.2}` would score the document `_score = 0.12*2.5 + 3.0*0.2 = 0.9`\n\n\n## Example request using an {{nlp}} model [sparse-vector-query-ex-request]\n\n```console\nGET _search\n{\n   \"query\":{\n      \"sparse_vector\": {\n        \"field\": \"ml.tokens\",\n        \"inference_id\": \"the inference ID to produce the token weights\",\n        \"query\": \"the query string\"\n      }\n   }\n}\n```\n\n\n## Example request using precomputed vectors [_example_request_using_precomputed_vectors]\n\n```console\nGET _search\n{\n   \"query\":{\n      \"sparse_vector\": {\n        \"field\": \"ml.tokens\",\n        \"query_vector\": { \"token1\": 0.5, \"token2\": 0.3, \"token3\": 0.2 }\n      }\n   }\n}\n```\n\n\n## Top level parameters for `sparse_vector` [sparse-vector-field-params]\n\n`field`\n:   (Required, string) The name of the field that contains the token-weight pairs to be searched against.\n\n`inference_id`\n:   (Optional, string) The [inference ID](https://www.elastic.co/docs/api/doc/elasticsearch/group/endpoint-inference) to use to convert the query text into token-weight pairs. It must be the same inference ID that was used to create the tokens from the input text. Only one of `inference_id` and `query_vector` is allowed. If `inference_id` is specified, `query` must also be specified. If all queried fields are of type [semantic_text](/reference/elasticsearch/mapping-reference/semantic-text.md), the inference ID associated with the `semantic_text` field will be inferred. You can reference a `deployment_id` of a {{ml}} trained model deployment as an `inference_id`. For example, if you download and deploy the ELSER model in the {{ml-cap}} trained models UI in {{kib}}, you can use the `deployment_id` of that deployment as the `inference_id`.\n\n`query`\n:   (Optional, string) The query text you want to use for search. If `inference_id` is specified, `query` must also be specified. If `query_vector` is specified, `query` must not be specified.\n\n`query_vector`\n:   (Optional, dictionary) A dictionary of token-weight pairs representing the precomputed query vector to search. Searching using this query vector will bypass additional inference. Only one of `inference_id` and `query_vector` is allowed.\n\n`prune` {applies_to}`stack: preview 9.0, ga 9.1`\n:   (Optional, boolean) Whether to perform pruning, omitting the non-significant tokens from the query to improve query performance. If `prune` is true but the `pruning_config` is not specified, pruning will occur but default values will be used. Default: false.\n\n`pruning_config` {applies_to}`stack: preview 9.0, ga 9.1`\n:   (Optional, object) Optional pruning configuration. If enabled, this will omit non-significant tokens from the query in order to improve query performance. This is only used if `prune` is set to `true`. If `prune` is set to `true` but `pruning_config` is not specified, default values will be used.\n\n    Parameters for `pruning_config` are:\n\n    `tokens_freq_ratio_threshold`\n    :   (Optional, integer) Tokens whose frequency is more than `tokens_freq_ratio_threshold` times the average frequency of all tokens in the specified field are considered outliers and pruned. This value must between 1 and 100. Default: `5`.\n\n    `tokens_weight_threshold`\n    :   (Optional, float) Tokens whose weight is less than `tokens_weight_threshold` are considered insignificant and pruned. This value must be between 0 and 1. Default: `0.4`.\n\n    `only_score_pruned_tokens`\n    :   (Optional, boolean) If `true` we only input pruned tokens into scoring, and discard non-pruned tokens. It is strongly recommended to set this to `false` for the main query, but this can be set to `true` for a rescore query to get more relevant results. Default: `false`.\n\n    ::::{note}\n    The default values for `tokens_freq_ratio_threshold` and `tokens_weight_threshold` were chosen based on tests using ELSERv2 that provided the optimal results.\n    ::::\n\nWhen token pruning is applied, non-significant tokens will be pruned from the query.\nNon-significant tokens can be defined as tokens that meet both of the following criteria:\n* The token appears much more frequently than most tokens, indicating that it is a very common word and may not benefit the overall search results much.\n* The weight/score is so low that the token is likely not very relevant to the original term\n\nBoth the token frequency threshold and weight threshold must show the token is non-significant in order for the token to be pruned.\nThis ensures that:\n* The tokens that are kept are frequent enough and have significant scoring.\n* Very infrequent tokens that may not have as high of a score are removed.\n\n## Example ELSER query [sparse-vector-query-example]\n\nThe following is an example of the `sparse_vector` query that references the ELSER model to perform semantic search. For a more detailed description of how to perform semantic search by using ELSER and the `sparse_vector` query, refer to [this tutorial](docs-content://solutions/search/semantic-search/semantic-search-elser-ingest-pipelines.md).\n\n```console\nGET my-index/_search\n{\n   \"query\":{\n      \"sparse_vector\": {\n         \"field\": \"ml.tokens\",\n         \"inference_id\": \"my-elser-model\",\n         \"query\": \"How is the weather in Jamaica?\"\n      }\n   }\n}\n```\n\nMultiple `sparse_vector` queries can be combined with each other or other query types. This can be achieved by wrapping them in [boolean query clauses](/reference/query-languages/query-dsl/query-dsl-bool-query.md) and using linear boosting:\n\n```console\nGET my-index/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"sparse_vector\": {\n            \"field\": \"ml.inference.title_expanded.predicted_value\",\n            \"inference_id\": \"my-elser-model\",\n            \"query\": \"How is the weather in Jamaica?\",\n            \"boost\": 1\n          }\n        },\n        {\n          \"sparse_vector\": {\n            \"field\": \"ml.inference.description_expanded.predicted_value\",\n            \"inference_id\": \"my-elser-model\",\n            \"query\": \"How is the weather in Jamaica?\",\n            \"boost\": 1\n          }\n        },\n        {\n          \"multi_match\": {\n            \"query\": \"How is the weather in Jamaica?\",\n            \"fields\": [\n              \"title\",\n              \"description\"\n            ],\n            \"boost\": 4\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nThis can also be achieved using [reciprocal rank fusion (RRF)](/reference/elasticsearch/rest-apis/reciprocal-rank-fusion.md), through an [`rrf` retriever](/reference/elasticsearch/rest-apis/retrievers/rrf-retriever.md) with multiple [`standard` retrievers](/reference/elasticsearch/rest-apis/retrievers/standard-retriever.md).\n\n```console\nGET my-index/_search\n{\n  \"retriever\": {\n    \"rrf\": {\n      \"retrievers\": [\n        {\n          \"standard\": {\n            \"query\": {\n              \"multi_match\": {\n                \"query\": \"How is the weather in Jamaica?\",\n                \"fields\": [\n                  \"title\",\n                  \"description\"\n                ]\n              }\n            }\n          }\n        },\n        {\n          \"standard\": {\n            \"query\": {\n              \"sparse_vector\": {\n                \"field\": \"ml.inference.title_expanded.predicted_value\",\n                \"inference_id\": \"my-elser-model\",\n                \"query\": \"How is the weather in Jamaica?\",\n                \"boost\": 1\n              }\n            }\n          }\n        },\n        {\n          \"standard\": {\n            \"query\": {\n              \"sparse_vector\": {\n                \"field\": \"ml.inference.description_expanded.predicted_value\",\n                \"inference_id\": \"my-elser-model\",\n                \"query\": \"How is the weather in Jamaica?\",\n                \"boost\": 1\n              }\n            }\n          }\n        }\n      ],\n      \"window_size\": 10,\n      \"rank_constant\": 20\n    }\n  }\n}\n```\n\n\n## Example ELSER query with pruning configuration and rescore [sparse-vector-query-with-pruning-config-and-rescore-example]\n\nThe following is an extension to the above example that adds a pruning configuration to the `sparse_vector` query. The pruning configuration identifies non-significant tokens to prune from the query in order to improve query performance.\n\nToken pruning happens at the shard level. While this should result in the same tokens being labeled as insignificant across shards, this is not guaranteed based on the composition of each shard. Therefore, if you are running `sparse_vector` with a `pruning_config` on a multi-shard index, we strongly recommend adding a [Rescore filtered search results](/reference/elasticsearch/rest-apis/rescore-search-results.md#rescore) function with the tokens that were originally pruned from the query. This will help mitigate any shard-level inconsistency with pruned tokens and provide better relevance overall.\n\n```console\nGET my-index/_search\n{\n   \"query\":{\n      \"sparse_vector\":{\n         \"field\": \"ml.tokens\",\n         \"inference_id\": \"my-elser-model\",\n         \"query\":\"How is the weather in Jamaica?\",\n         \"prune\": true,\n         \"pruning_config\": {\n           \"tokens_freq_ratio_threshold\": 5,\n           \"tokens_weight_threshold\": 0.4,\n           \"only_score_pruned_tokens\": false\n         }\n      }\n   },\n   \"rescore\": {\n      \"window_size\": 100,\n      \"query\": {\n         \"rescore_query\": {\n            \"sparse_vector\": {\n               \"field\": \"ml.tokens\",\n               \"inference_id\": \"my-elser-model\",\n               \"query\": \"How is the weather in Jamaica?\",\n               \"prune\": true,\n               \"pruning_config\": {\n                   \"tokens_freq_ratio_threshold\": 5,\n                   \"tokens_weight_threshold\": 0.4,\n                   \"only_score_pruned_tokens\": true\n               }\n            }\n         }\n      }\n   }\n}\n```\n\n::::{note}\nWhen performing [cross-cluster search](docs-content://solutions/search/cross-cluster-search.md), inference is performed on the local cluster.\n::::",
              "filename": "elasticsearch-main/docs/reference/query-languages/query-dsl/query-dsl-sparse-vector-query.md"
            },
            {
              "navigation_title": "Rank Vectors",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/rank-vectors.html"
              ],
              "content": "# Rank Vectors [rank-vectors]\n\n\n::::{warning}\nThis functionality is in technical preview and may be changed or removed in a future release. Elastic will work to fix any issues, but features in technical preview are not subject to the support SLA of official GA features.\n::::\n\n\nThe `rank_vectors` field type enables late-interaction dense vector scoring in Elasticsearch. The number of vectors per field can vary, but they must all share the same number of dimensions and element type.\n\nThe purpose of vectors stored in this field is second order ranking documents with max-sim similarity.\n\nHere is a simple example of using this field with `float` elements.\n\n```console\nPUT my-rank-vectors-float\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"rank_vectors\"\n      }\n    }\n  }\n}\n\nPUT my-rank-vectors-float/_doc/1\n{\n  \"my_vector\" : [[0.5, 10, 6], [-0.5, 10, 10]]\n}\n```\n\nIn addition to the `float` element type, `byte` and `bit` element types are also supported.\n\nHere is an example of using this field with `byte` elements.\n\n```console\nPUT my-rank-vectors-byte\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"rank_vectors\",\n        \"element_type\": \"byte\"\n      }\n    }\n  }\n}\n\nPUT my-rank-vectors-byte/_doc/1\n{\n  \"my_vector\" : [[1, 2, 3], [4, 5, 6]]\n}\n```\n\nHere is an example of using this field with `bit` elements.\n\n```console\nPUT my-rank-vectors-bit\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"rank_vectors\",\n        \"element_type\": \"bit\"\n      }\n    }\n  }\n}\n\nPOST /my-rank-vectors-bit/_bulk?refresh\n{\"index\": {\"_id\" : \"1\"}}\n{\"my_vector\": [127, -127, 0, 1, 42]}\n{\"index\": {\"_id\" : \"2\"}}\n{\"my_vector\": \"8100012a7f\"}\n```\n\n## Parameters for rank vectors fields [rank-vectors-params]\n\nThe `rank_vectors` field type supports the following parameters:\n\n$$$rank-vectors-element-type$$$\n\n`element_type`\n:   (Optional, string) The data type used to encode vectors. The supported data types are `float` (default), `byte`, and `bit`.\n\n::::{dropdown} Valid values for element_type\n`float`\n:   indexes a 4-byte floating-point value per dimension. This is the default value.\n\n`byte`\n:   indexes a 1-byte integer value per dimension.\n\n`bit`\n:   indexes a single bit per dimension. Useful for very high-dimensional vectors or models that specifically support bit vectors. NOTE: when using `bit`, the number of dimensions must be a multiple of 8 and must represent the number of bits.\n\n::::\n\n\n`dims`\n:   (Optional, integer) Number of vector dimensions. Can\u2019t exceed `4096`. If `dims` is not specified, it will be set to the length of the first vector added to the field.\n\n## Accessing `dense_vector` fields in search responses\n```{applies_to}\nstack: ga 9.2\nserverless: ga\n```\n\nBy default, `dense_vector` fields are **not included in `_source`** in responses from the `_search`, `_msearch`, `_get`, and `_mget` APIs.\nThis helps reduce response size and improve performance, especially in scenarios where vectors are used solely for similarity scoring and not required in the output.\n\nTo retrieve vector values explicitly, you can use:\n\n* The `fields` option to request specific vector fields directly:\n\n```console\nPOST my-index-2/_search\n{\n  \"fields\": [\"my_vector\"]\n}\n```\n\n- The `_source.exclude_vectors` flag to re-enable vector inclusion in `_source` responses:\n\n```console\nPOST my-index-2/_search\n{\n  \"_source\": {\n    \"exclude_vectors\": false\n  }\n}\n```\n\n### Storage behavior and `_source`\n\nBy default, `rank_vectors` fields are not stored in `_source` on disk. This is also controlled by the index setting `index.mapping.exclude_source_vectors`.\nThis setting is enabled by default for newly created indices and can only be set at index creation time.\n\nWhen enabled:\n\n* `rank_vectors` fields are removed from `_source` and the rest of the `_source` is stored as usual.\n* If a request includes `_source` and vector values are needed (e.g., during recovery or reindex), the vectors are rehydrated from their internal format.\n\nThis setting is compatible with synthetic `_source`, where the entire `_source` document is reconstructed from columnar storage. In full synthetic mode, no `_source` is stored on disk, and all fields \u2014 including vectors \u2014 are rebuilt when needed.\n\n### Rehydration and precision\n\nWhen vector values are rehydrated (e.g., for reindex, recovery, or explicit `_source` requests), they are restored from their internal format. Internally, vectors are stored at float precision, so if they were originally indexed as higher-precision types (e.g., `double` or `long`), the rehydrated values will have reduced precision. This lossy representation is intended to save space while preserving search quality.\n\n### Storing original vectors in `_source`\n\nIf you want to preserve the original vector values exactly as they were provided, you can re-enable vector storage in `_source`:\n\n```console\nPUT my-index-include-vectors\n{\n  \"settings\": {\n    \"index.mapping.exclude_source_vectors\": false\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"rank_vectors\",\n        \"dims\": 128\n      }\n    }\n  }\n}\n```\n\nWhen this setting is disabled:\n\n* `rank_vectors` fields are stored as part of the `_source`, exactly as indexed.\n* The index will store both the original `_source` value and the internal representation used for vector search, resulting in increased storage usage.\n* Vectors are once again returned in `_source` by default in all relevant APIs, with no need to use `exclude_vectors` or `fields`.\n\nThis configuration is appropriate when full source fidelity is required, such as for auditing or round-tripping exact input values.\n\n## Scoring with rank vectors [rank-vectors-scoring]\n\nRank vectors can be accessed and used in [`script_score` queries](/reference/query-languages/query-dsl/query-dsl-script-score-query.md).\n\nFor example, the following query scores documents based on the maxSim similarity between the query vector and the vectors stored in the `my_vector` field:\n\n```console\nGET my-rank-vectors-float/_search\n{\n  \"query\": {\n    \"script_score\": {\n      \"query\": {\n        \"match_all\": {}\n      },\n      \"script\": {\n        \"source\": \"maxSimDotProduct(params.query_vector, 'my_vector')\",\n        \"params\": {\n          \"query_vector\": [[0.5, 10, 6], [-0.5, 10, 10]]\n        }\n      }\n    }\n  }\n}\n```\n\nAdditionally, asymmetric similarity functions can be used to score against `bit` vectors. For example, the following query scores documents based on the maxSimDotProduct similarity between a floating point query vector and bit vectors stored in the `my_vector` field:\n\n```console\nPUT my-rank-vectors-bit\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"rank_vectors\",\n        \"element_type\": \"bit\"\n      }\n    }\n  }\n}\n\nPOST /my-rank-vectors-bit/_bulk?refresh\n{\"index\": {\"_id\" : \"1\"}}\n{\"my_vector\": [127, -127, 0, 1, 42]}\n{\"index\": {\"_id\" : \"2\"}}\n{\"my_vector\": \"8100012a7f\"}\n\nGET my-rank-vectors-bit/_search\n{\n  \"query\": {\n    \"script_score\": {\n      \"query\": {\n        \"match_all\": {}\n      },\n      \"script\": {\n        \"source\": \"maxSimDotProduct(params.query_vector, 'my_vector')\",\n        \"params\": {\n          \"query_vector\": [\n            [0.35, 0.77, 0.95, 0.15, 0.11, 0.08, 0.58, 0.06, 0.44, 0.52, 0.21,\n       0.62, 0.65, 0.16, 0.64, 0.39, 0.93, 0.06, 0.93, 0.31, 0.92, 0.0,\n       0.66, 0.86, 0.92, 0.03, 0.81, 0.31, 0.2 , 0.92, 0.95, 0.64, 0.19,\n       0.26, 0.77, 0.64, 0.78, 0.32, 0.97, 0.84]\n           ] <1>\n        }\n      }\n    }\n  }\n}\n```\n\n1. Note that the query vector has 40 elements, matching the number of bits in the bit vectors.",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/mapping-reference/rank-vectors.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-similarity-context.html"
              ],
              "products": [
                {
                  "id": "painless"
                }
              ],
              "content": "# Similarity context [painless-similarity-context]\n\nUse a Painless script to create a [similarity](/reference/elasticsearch/index-settings/similarity.md) equation for scoring documents in a query.\n\n**Variables**\n\n`weight` (`float`, read-only)\n:   The weight as calculated by a [weight script](/reference/scripting-languages/painless/painless-weight-context.md)\n\n`query.boost` (`float`, read-only)\n:   The boost value if provided by the query. If this is not provided the value is `1.0f`.\n\n`field.docCount` (`long`, read-only)\n:   The number of documents that have a value for the current field.\n\n`field.sumDocFreq` (`long`, read-only)\n:   The sum of all terms that exist for the current field. If this is not available the value is `-1`.\n\n`field.sumTotalTermFreq` (`long`, read-only)\n:   The sum of occurrences in the index for all the terms that exist in the current field. If this is not available the value is `-1`.\n\n`term.docFreq` (`long`, read-only)\n:   The number of documents that contain the current term in the index.\n\n`term.totalTermFreq` (`long`, read-only)\n:   The total occurrences of the current term in the index.\n\n`doc.length` (`long`, read-only)\n:   The number of tokens the current document has in the current field. This is decoded from the stored [norms](/reference/elasticsearch/mapping-reference/norms.md) and may be approximate for long fields\n\n`doc.freq` (`long`, read-only)\n:   The number of occurrences of the current term in the current document for the current field.\n\nNote that the `query`, `field`, and `term` variables are also available to the [weight context](/reference/scripting-languages/painless/painless-weight-context.md). They are more efficiently used there, as they are constant for all documents.\n\nFor queries that contain multiple terms, the script is called once for each term with that term\u2019s calculated weight, and the results are summed. Note that some terms might have a `doc.freq` value of `0` on a document, for example if a query uses synonyms.\n\n**Return**\n\n`double`\n:   The similarity score for the current document.\n\n**API**\n\nThe standard [Painless API](https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-api-reference-shared.html) is available.",
              "filename": "elasticsearch-main/docs/reference/scripting-languages/painless/painless-similarity-context.md"
            }
          ],
          "tool_call_id": "call_nfLKQxxb2Qwxxln2nx1qSCla",
          "metadata": null,
          "timestamp": "2025-10-03T04:07:48.991197+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "To calculate the L2 norm similarity (also known as the Euclidean distance) between dense vectors in Elasticsearch, you can use the `dense_vector` field type combined with a similarity metric in your mappings.\n\nHere's how to set this up:\n\n1. **Define the `dense_vector` Field:**\n   When creating an index, you can map a field as `dense_vector` and specify the number of dimensions (`dims`). The similarity is defined with the `similarity` property.\n\n   **Example:**\n   ```json\n   PUT my-index\n   {\n     \"mappings\": {\n       \"properties\": {\n         \"my_vector\": {\n           \"type\": \"dense_vector\",\n           \"dims\": 3,\n           \"similarity\": \"l2_norm\" // Setting similarity to L2 norm\n         }\n       }\n     }\n   }\n   ```\n\n2. **Understanding L2 Norm Similarity:**\n   The L2 norm computes the similarity based on the Euclidean distance between the vectors. Elasticsearch uses the following scoring formula:\n   \\[\n   \\text{{_score}} = \\frac{1}{{1 + \\text{{l2_norm}}(\\text{{query}}, \\text{{vector}})^2}}\n   \\]\n   This means that a smaller L2 norm (indicating closer similarity) results in a higher score for the document.\n\n3. **Querying with L2 Norm Similarity:**\n   When querying, you would typically use a KNN (k-nearest neighbor) query along with your query vector.\n\n   **Example Query:**\n   ```json\n   POST my-index/_search\n   {\n     \"query\": {\n       \"knn\": {\n         \"field\": \"my_vector\",\n         \"query_vector\": [0.5, 10, 6],\n         \"k\": 3\n       }\n     }\n   }\n   ```\n\nThis specifies that you want to retrieve the top K vectors that are closest to the provided query vector according to the L2 norm similarity metric.\n\n### References:\n- For more details on `dense_vector`, refer to [Dense Vector documentation](https://github.com/elastic/elasticsearch/blob/main/docs/reference/elasticsearch/mapping-reference/dense-vector.md).\n- For information on available similarity metrics, check the [Similarity settings documentation](https://github.com/elastic/elasticsearch/blob/main/docs/reference/elasticsearch/index-settings/similarity.md).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 16707,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 489,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T04:07:49+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMRN7S7WJ3GVZ8KPs9cnEJS0FdteU",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}