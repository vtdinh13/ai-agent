{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How are ECS fields related to OpenTelemetry in the context of the Elastic Search processing functions?",
          "timestamp": "2025-10-03T06:00:13.140465+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"ECS fields OpenTelemetry processing functions\"}",
          "tool_call_id": "call_KGIlhkwqBsJE3InFYC7lNMge",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 219,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 19,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T06:00:13+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMT7tJCkBsSoucVl445pizBmEnsvR",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "Normalize for Stream",
              "applies_to": {
                "stack": "preview 9.1"
              },
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/normalize-for-stream-processor.html"
              ],
              "content": "# Normalize-for-Stream processor [normalize-for-stream-processor]\n\n\nDetects whether a document is OpenTelemetry-compliant and if not -\nnormalizes it as described below. If used in combination with the OTel-related\nmappings such as the ones defined in `logs-otel@template`, the resulting\ndocument can be queried seamlessly by clients that expect either [ECS](https://www.elastic.co/guide/en/ecs/current/index.html) or OpenTelemetry-[Semantic-Conventions](https://github.com/open-telemetry/semantic-conventions) formats.\n\n::::{note}\nThis processor is in tech preview and is not available in our serverless offering.\n::::\n\n## Detecting OpenTelemetry compliance\n\nThe processor detects OpenTelemetry compliance by checking the following fields:\n* `resource` exists as a key and the value is a map\n* `resource` either doesn't contain an `attributes` field, or contains an `attributes` field of type map\n* `scope` is either missing or a map\n* `attributes` is either missing or a map\n* `body` is either missing or a map\n* `body` either doesn't contain a `text` field, or contains a `text` field of type `String`\n* `body` either doesn't contain a `structured` field, or contains a `structured` field that is not of type `String`\n\nIf all of these conditions are met, the document is considered OpenTelemetry-compliant and is not modified by the processor.\n\n## Normalization\n\nIf the document is not OpenTelemetry-compliant, the processor normalizes it as follows:\n* Specific ECS fields are renamed to have their corresponding OpenTelemetry Semantic Conventions attribute names. These include the following:\n\n  | ECS Field   | Semantic Conventions Attribute |\n  |-------------|--------------------------------|\n  | `span.id`   | `span_id`                      |\n  | `trace.id`  | `trace_id`                     |\n  | `message`   | `body.text`                    |\n  | `log.level` | `severity_text`                |\n\n  The processor first looks for the nested form of the ECS field and if such does not exist, it looks for a top-level field with the dotted field name.\n* Other specific ECS fields that describe resources and have corresponding counterparts in the OpenTelemetry Semantic Conventions are moved to the `resource.attribtues` map. Fields that are considered resource attributes are such that conform to the following conditions:\n    * They are ECS fields that have corresponding counterparts (either with\n      the same name or with a different name) in OpenTelemetry Semantic Conventions.\n    * The corresponding OpenTelemetry attribute is defined in\n      [Semantic Conventions](https://github.com/open-telemetry/semantic-conventions/tree/main/model)\n      within a group that is defined as `type: enitity`.\n* All other fields, except for `@timestamp`, are moved to the `attributes` map.\n* All non-array entries of the `attributes` and `resource.attributes` maps are flattened. Flattening means that nested objects are merged into their parent object, and the keys are concatenated with a dot. See examples below.\n\n## Examples\n\nIf an OpenTelemetry-compliant document is detected, the processor does nothing. For example, the following document will stay unchanged:\n\n```json\n{\n  \"resource\": {\n    \"attributes\": {\n      \"service.name\": \"my-service\"\n    }\n  },\n  \"scope\": {\n    \"name\": \"my-library\",\n    \"version\": \"1.0.0\"\n  },\n  \"attributes\": {\n    \"http.method\": \"GET\"\n  },\n  \"body\": {\n    \"text\": \"Hello, world!\"\n  }\n}\n```\n\nIf a non-OpenTelemetry-compliant document is detected, the processor normalizes it. For example, the following document:\n\n```json\n{\n  \"@timestamp\": \"2023-10-01T12:00:00Z\",\n  \"service\": {\n    \"name\": \"my-service\",\n    \"version\": \"1.0.0\",\n    \"environment\": \"production\",\n    \"language\": {\n      \"name\": \"python\",\n      \"version\": \"3.8\"\n    }\n  },\n  \"log\": {\n    \"level\": \"INFO\"\n  },\n  \"message\": \"Hello, world!\",\n  \"http\": {\n    \"method\": \"GET\",\n    \"url\": {\n      \"path\": \"/api/v1/resource\"\n    },\n    \"headers\": [\n      {\n        \"name\": \"Authorization\",\n        \"value\": \"Bearer token\"\n      },\n      {\n        \"name\": \"User-Agent\",\n        \"value\": \"my-client/1.0\"\n      }\n    ]\n  },\n  \"span\" : {\n    \"id\": \"1234567890abcdef\"\n  },\n  \"span.id\": \"abcdef1234567890\",\n  \"trace.id\": \"abcdef1234567890abcdef1234567890\"\n}\n```\nwill be normalized into the following form:\n\n```json\n{\n  \"@timestamp\": \"2023-10-01T12:00:00Z\",\n  \"resource\": {\n    \"attributes\": {\n      \"service.name\": \"my-service\",\n      \"service.version\": \"1.0.0\",\n      \"service.environment\": \"production\"\n    }\n  },\n  \"attributes\": {\n    \"service.language.name\": \"python\",\n    \"service.language.version\": \"3.8\",\n    \"http.method\": \"GET\",\n    \"http.url.path\": \"/api/v1/resource\",\n    \"http.headers\": [\n      {\n        \"name\": \"Authorization\",\n        \"value\": \"Bearer token\"\n      },\n      {\n        \"name\": \"User-Agent\",\n        \"value\": \"my-client/1.0\"\n      }\n    ]\n  },\n  \"body\": {\n    \"text\": \"Hello, world!\"\n  },\n  \"span_id\": \"1234567890abcdef\",\n  \"trace_id\": \"abcdef1234567890abcdef1234567890\"\n}\n```\n## Structured `message` field\n\nIf the `message` field in the ingested document is structured as a JSON, the\nprocessor will determine whether it is in ECS format or not, based on the\nexistence or absence of the `@timestamp` field. If the `@timestamp` field is\npresent, the `message` field will be considered to be in ECS format, and its\ncontents will be merged into the root of the document and then normalized as\ndescribed above. The `@timestamp` from the `message` field will override the\nroot `@timestamp` field in the resulting document.\nIf the `@timestamp` field is absent, the `message` field will be moved to\nthe `body.structured` field as is, without any further normalization.\n\nFor example, if the `message` field is an ECS-JSON, as follows:\n\n```json\n{\n  \"@timestamp\": \"2023-10-01T12:00:00Z\",\n  \"message\": \"{\\\"@timestamp\\\":\\\"2023-10-01T12:01:00Z\\\",\\\"log.level\\\":\\\"INFO\\\",\\\"service.name\\\":\\\"my-service\\\",\\\"message\\\":\\\"The actual log message\\\",\\\"http\\\":{\\\"method\\\":\\\"GET\\\",\\\"url\\\":{\\\"path\\\":\\\"/api/v1/resource\\\"}}}\"\n\n}\n```\nit will be normalized into the following form:\n\n```json\n{\n  \"@timestamp\": \"2023-10-01T12:01:00Z\",\n  \"severity_text\": \"INFO\",\n  \"body\": {\n    \"text\": \"The actual log message\"\n  },\n  \"resource\": {\n    \"attributes\": {\n      \"service.name\": \"my-service\"\n    }\n  },\n  \"attributes\": {\n    \"http.method\": \"GET\",\n    \"http.url.path\": \"/api/v1/resource\"\n  }\n}\n```\n\nHowever, if the `message` field is not recognized as ECS format, as follows:\n\n```json\n{\n  \"@timestamp\": \"2023-10-01T12:00:00Z\",\n  \"log\": {\n    \"level\": \"INFO\"\n  },\n  \"service\": {\n    \"name\": \"my-service\"\n  },\n  \"tags\": [\"user-action\", \"api-call\"],\n  \"message\": \"{\\\"root_cause\\\":\\\"Network error\\\",\\\"http\\\":{\\\"method\\\":\\\"GET\\\",\\\"url\\\":{\\\"path\\\":\\\"/api/v1/resource\\\"}}}\"\n}\n```\nit will be normalized into the following form:\n\n```json\n{\n  \"@timestamp\": \"2023-10-01T12:00:00Z\",\n  \"severity_text\": \"INFO\",\n  \"resource\": {\n    \"attributes\": {\n      \"service.name\": \"my-service\"\n    }\n  },\n  \"attributes\": {\n    \"tags\": [\"user-action\", \"api-call\"]\n  },\n  \"body\": {\n    \"structured\": {\n      \"root_cause\": \"Network error\",\n      \"http\": {\n        \"method\": \"GET\",\n        \"url\": {\n          \"path\": \"/api/v1/resource\"\n        }\n      }\n    }\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/normalize-for-stream.md"
            },
            {
              "applies_to": {
                "stack": null,
                "serverless": null
              },
              "navigation_title": "{{esql}}",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-language.html",
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql.html",
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-getting-started.html",
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-using.html",
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-examples.html"
              ],
              "products": [
                {
                  "id": "elasticsearch"
                }
              ],
              "content": "# {{esql}} reference [esql-language]\n\n**Elasticsearch Query Language ({{esql}})** is a piped query language for filtering, transforming, and analyzing data.\n\n## What's {{esql}}? [_the_esql_compute_engine]\n\nYou can author {{esql}} queries to find specific events, perform statistical analysis, and create visualizations. It supports a wide range of commands, functions, and operators to perform various data operations, such as filter, aggregation, time-series analysis, and more. It initially supported a subset of the features available in Query DSL, but it is rapidly evolving with every {{serverless-full}} and Stack release.\n\n{{esql}} is designed to be easy to read and write, making it accessible for users with varying levels of technical expertise. It is particularly useful for data analysts, security professionals, and developers who need to work with large datasets in Elasticsearch.\n\n## How does it work? [search-analyze-data-esql]\n\n{{esql}} uses pipes (`|`) to manipulate and transform data in a step-by-step fashion. This approach allows you to compose a series of operations, where the output of one operation becomes the input for the next, enabling complex data transformations and analysis.\n\nHere's a simple example of an {{esql}} query:\n\n```esql\nFROM sample_data\n| SORT @timestamp DESC\n| LIMIT 3\n```\n\nNote that each line in the query represents a step in the data processing pipeline:\n- The `FROM` clause specifies the index or data stream to query\n- The `SORT` clause sorts the data by the `@timestamp` field in descending order\n- The `LIMIT` clause restricts the output to the top 3 results\n\n### User interfaces\n\nYou can interact with {{esql}} in two ways:\n\n- **Programmatic access**: Use {{esql}} syntax with the {{es}} `_query` endpoint.\n  - Refer to [](esql/esql-rest.md)\n\n- **Interactive interfaces**: Work with {{esql}} through Elastic user interfaces including Kibana Discover, Dashboards, Dev Tools, and analysis tools in Elastic Security and Observability.\n  - Refer to [Using {{esql}} in {{kib}}](docs-content://explore-analyze/query-filter/languages/esql-kibana.md).",
              "filename": "elasticsearch-main/docs/reference/query-languages/esql.md"
            },
            {
              "applies_to": {
                "stack": null,
                "serverless": null
              },
              "navigation_title": "Metadata fields",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-metadata-fields.html"
              ],
              "content": "# {{esql}} metadata fields [esql-metadata-fields]\n\n{{esql}} can access [metadata fields](/reference/elasticsearch/mapping-reference/document-metadata-fields.md).\n\nTo access these fields, use the `METADATA` directive with the [`FROM`](/reference/query-languages/esql/commands/from.md) source command. For example:\n\n```esql\nFROM index METADATA _index, _id\n```\n\n## Available metadata fields\n\nThe following metadata fields are available in {{esql}}:\n\n| Metadata field | Type | Description |\n|---------------|------|-------------|\n| [`_id`](/reference/elasticsearch/mapping-reference/mapping-id-field.md) | [keyword](/reference/elasticsearch/mapping-reference/keyword.md) | Unique document ID. |\n| [`_ignored`](/reference/elasticsearch/mapping-reference/mapping-ignored-field.md) | [keyword](/reference/elasticsearch/mapping-reference/keyword.md) | Names every field in a document that was ignored when the document was indexed. |\n| [`_index`](/reference/elasticsearch/mapping-reference/mapping-index-field.md) | [keyword](/reference/elasticsearch/mapping-reference/keyword.md) | Index name. |\n| `_index_mode` | [keyword](/reference/elasticsearch/mapping-reference/keyword.md) | [Index mode](/reference/elasticsearch/index-settings/index-modules.md#index-mode-setting). For example: `standard`, `lookup`, or `logsdb`. |\n| `_score` | [`float`](/reference/elasticsearch/mapping-reference/number.md) | Query relevance score (when enabled). Scores are updated when using [full text search functions](/reference/query-languages/esql/functions-operators/search-functions.md). |\n| [`_source`](/reference/elasticsearch/mapping-reference/mapping-source-field.md) | Special `_source` type | Original JSON document body passed at index time (or a reconstructed version if [synthetic `_source`](/reference/elasticsearch/mapping-reference/mapping-source-field.md#synthetic-source) is enabled). |\n| `_version` | [`long`](/reference/elasticsearch/mapping-reference/number.md) | Document version number |\n\n## Usage and limitations\n\n- Metadata fields are only available when the data source is an index\n- The `_source` type is not supported by functions\n- Only the `FROM` command supports the `METADATA` directive\n- Once enabled, metadata fields work like regular index fields\n\n## Examples\n\n### Basic metadata usage\n\nOnce enabled, metadata fields are available to subsequent processing commands, just like other index fields:\n\n```esql\nFROM ul_logs, apps METADATA _index, _version\n| WHERE id IN (13, 14) AND _version == 1\n| EVAL key = CONCAT(_index, \"_\", TO_STR(id))\n| SORT id, _index\n| KEEP id, _index, _version, key\n```\n\n| id:long | _index:keyword | _version:long | key:keyword |\n| --- | --- | --- | --- |\n| 13 | apps | 1 | apps_13 |\n| 13 | ul_logs | 1 | ul_logs_13 |\n| 14 | apps | 1 | apps_14 |\n| 14 | ul_logs | 1 | ul_logs_14 |\n\n### Metadata fields and aggregations\n\nSimilar to index fields, once an aggregation is performed, a metadata field will no longer be accessible to subsequent commands, unless used as a grouping field:\n\n```esql\nFROM employees METADATA _index, _id\n| STATS max = MAX(emp_no) BY _index\n```\n\n| max:integer | _index:keyword |\n| --- | --- |\n| 10100 | employees |\n\n### Sort results by search score\n\n```esql\nFROM products METADATA _score\n| WHERE MATCH(description, \"wireless headphones\")\n| SORT _score DESC\n| KEEP name, description, _score\n```\n\n:::{tip}\nRefer to [{{esql}} for search](docs-content://solutions/search/esql-for-search.md#esql-for-search-scoring) for more information on relevance scoring and how to use `_score` in your queries.\n:::",
              "filename": "elasticsearch-main/docs/reference/query-languages/esql/esql-metadata-fields.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-params.html"
              ],
              "content": "# Mapping parameters [mapping-params]\n\nThe following pages provide detailed explanations of the various mapping parameters that are used by [field mappings](/reference/elasticsearch/mapping-reference/field-data-types.md):\n\nThe following mapping parameters are common to some or all field data types:\n\n* [`analyzer`](/reference/elasticsearch/mapping-reference/analyzer.md)\n* [`coerce`](/reference/elasticsearch/mapping-reference/coerce.md)\n* [`copy_to`](/reference/elasticsearch/mapping-reference/copy-to.md)\n* [`doc_values`](/reference/elasticsearch/mapping-reference/doc-values.md)\n* [`dynamic`](/reference/elasticsearch/mapping-reference/dynamic.md)\n* [`eager_global_ordinals`](/reference/elasticsearch/mapping-reference/eager-global-ordinals.md)\n* [`enabled`](/reference/elasticsearch/mapping-reference/enabled.md)\n* [`fielddata`](/reference/elasticsearch/mapping-reference/text.md#fielddata-mapping-param)\n* [`fields`](/reference/elasticsearch/mapping-reference/multi-fields.md)\n* [`format`](/reference/elasticsearch/mapping-reference/mapping-date-format.md)\n* [`ignore_above`](/reference/elasticsearch/mapping-reference/ignore-above.md)\n* [`ignore_malformed`](/reference/elasticsearch/mapping-reference/ignore-malformed.md)\n* [`index_options`](/reference/elasticsearch/mapping-reference/index-options.md)\n* [`index_phrases`](/reference/elasticsearch/mapping-reference/index-phrases.md)\n* [`index_prefixes`](/reference/elasticsearch/mapping-reference/index-prefixes.md)\n* [`index`](/reference/elasticsearch/mapping-reference/mapping-index.md)\n* [`meta`](/reference/elasticsearch/mapping-reference/mapping-field-meta.md)\n* [`normalizer`](/reference/elasticsearch/mapping-reference/normalizer.md)\n* [`norms`](/reference/elasticsearch/mapping-reference/norms.md)\n* [`null_value`](/reference/elasticsearch/mapping-reference/null-value.md)\n* [`position_increment_gap`](/reference/elasticsearch/mapping-reference/position-increment-gap.md)\n* [`properties`](/reference/elasticsearch/mapping-reference/properties.md)\n* [`search_analyzer`](/reference/elasticsearch/mapping-reference/search-analyzer.md)\n* [`similarity`](/reference/elasticsearch/mapping-reference/similarity.md)\n* [`subobjects`](/reference/elasticsearch/mapping-reference/subobjects.md)\n* [`store`](/reference/elasticsearch/mapping-reference/mapping-store.md)\n* [`term_vector`](/reference/elasticsearch/mapping-reference/term-vector.md)",
              "filename": "elasticsearch-main/docs/reference/elasticsearch/mapping-reference/mapping-parameters.md"
            },
            {
              "navigation_title": "Redact",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/redact-processor.html"
              ],
              "content": "# Redact processor [redact-processor]\n\n\nThe Redact processor uses the Grok rules engine to obscure text in the input document matching the given Grok patterns. The processor can be used to obscure Personal Identifying Information (PII) by configuring it to detect known patterns such as email or IP addresses. Text that matches a Grok pattern is replaced with a configurable string such as `<EMAIL>` where an email address is matched or simply replace all matches with the text `<REDACTED>` if preferred.\n\n{{es}} comes packaged with a number of useful predefined [patterns](https://github.com/elastic/elasticsearch/blob/master/libs/grok/src/main/resources/patterns/ecs-v1) that can be conveniently referenced by the Redact processor. If one of those does not suit your needs, create a new pattern with a custom pattern definition. The Redact processor replaces every occurrence of a match. If there are multiple matches all will be replaced with the pattern name.\n\nThe Redact processor is compatible with [Elastic Common Schema (ECS)](ecs://reference/ecs-field-reference.md) patterns. Legacy Grok patterns are not supported.\n\n## Using the Redact processor in a pipeline [using-redact]\n\n$$$redact-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `field` | yes | - | The field to be redacted |\n| `patterns` | yes | - | A list of grok expressions to match and redact named captures with |\n| `pattern_definitions` | no | - | A map of pattern-name and pattern tuples defining custom patterns to be used by the processor. Patterns matching existing names will override the pre-existing definition |\n| `prefix` | no | < | Start a redacted section with this token |\n| `suffix` | no | > | End a redacted section with this token |\n| `ignore_missing` | no | `true` | If `true` and `field` does not exist or is `null`, the processor quietly exits without modifying the document |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n| `skip_if_unlicensed` | no | `false` | If `true` and the current license does not support running redact processors, then the processor quietly exits without modifying the document |\n| `trace_redact` | no | `false` | If `true` then ingest metadata `_ingest._redact._is_redacted` is set to `true` if the document has been redacted |\n\nIn this example the predefined `IP` Grok pattern is used to match and redact an IP addresses from the `message` text field. The pipeline is tested using the Simulate API.\n\n```console\nPOST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n    \"description\" : \"Hide my IP\",\n    \"processors\": [\n      {\n        \"redact\": {\n          \"field\": \"message\",\n          \"patterns\": [\"%{IP:client}\"]\n        }\n      }\n    ]\n  },\n  \"docs\":[\n    {\n      \"_source\": {\n        \"message\": \"55.3.244.1 GET /index.html 15824 0.043\"\n      }\n    }\n  ]\n}\n```\n\nThe document in the response still contains the `message` field but now the IP address `55.3.244.1` is replaced by the text `<client>`.\n\n```console-result\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_version\": \"-3\",\n        \"_source\": {\n          \"message\": \"<client> GET /index.html 15824 0.043\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2023-02-01T16:08:39.419056008Z\"\n        }\n      }\n    }\n  ]\n}\n```\n\nThe IP address is replaced with the word `client` because that is what is specified in the Grok pattern `%{IP:client}`. The `<` and `>` tokens which surround the pattern name are configurable using the `prefix` and `suffix` options.\n\nThe next example defines multiple patterns both of which are replaced with the word `REDACTED` and the prefix and suffix tokens are set to `*`\n\n```console\nPOST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n    \"description\": \"Hide my IP\",\n    \"processors\": [\n      {\n        \"redact\": {\n          \"field\": \"message\",\n          \"patterns\": [\n            \"%{IP:REDACTED}\",\n            \"%{EMAILADDRESS:REDACTED}\"\n          ],\n          \"prefix\": \"*\",\n          \"suffix\": \"*\"\n        }\n      }\n    ]\n  },\n  \"docs\": [\n    {\n      \"_source\": {\n        \"message\": \"55.3.244.1 GET /index.html 15824 0.043 test@elastic.co\"\n      }\n    }\n  ]\n}\n```\n\nIn the response both the IP `55.3.244.1` and email address `test@elastic.co` have been replaced by `*REDACTED*`.\n\n```console-result\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_version\": \"-3\",\n        \"_source\": {\n          \"message\": \"*REDACTED* GET /index.html 15824 0.043 *REDACTED*\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2023-02-01T16:53:14.560005377Z\"\n        }\n      }\n    }\n  ]\n}\n```\n\n\n## Custom patterns [redact-custom-patterns]\n\nIf one of the existing Grok [patterns](https://github.com/elastic/elasticsearch/blob/master/libs/grok/src/main/resources/patterns/ecs-v1) does not fit your requirements custom patterns can be added with the `pattern_definitions` option. New patterns definitions are composed of a pattern name and the pattern itself. The pattern may be a regular expression or reference existing Grok patterns.\n\nThis example defines the custom pattern `GITHUB_NAME` to match GitHub usernames. The pattern definition uses the existing `USERNAME` Grok [pattern](https://github.com/elastic/elasticsearch/blob/master/libs/grok/src/main/resources/patterns/ecs-v1/grok-patterns) prefixed by the literal `@`.\n\n::::{note}\nThe [Grok Debugger](docs-content://explore-analyze/query-filter/tools/grok-debugger.md) is a really useful tool for building custom patterns.\n::::\n\n\n```console\nPOST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n    \"processors\": [\n      {\n        \"redact\": {\n          \"field\": \"message\",\n          \"patterns\": [\n            \"%{GITHUB_NAME:GITHUB_NAME}\"\n          ],\n          \"pattern_definitions\": {\n            \"GITHUB_NAME\": \"@%{USERNAME}\"\n          }\n        }\n      }\n    ]\n  },\n  \"docs\": [\n    {\n      \"_source\": {\n        \"message\": \"@elastic-data-management the PR is ready for review\"\n      }\n    }\n  ]\n}\n```\n\nThe username is redacted in the response.\n\n```console-result\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_version\": \"-3\",\n        \"_source\": {\n          \"message\": \"<GITHUB_NAME> the PR is ready for review\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2023-02-01T16:53:14.560005377Z\"\n        }\n      }\n    }\n  ]\n}\n```\n\n\n## Grok watchdog [grok-watchdog-redact]\n\nThe watchdog interrupts expressions that take too long to execute. When interrupted, the Redact processor fails with an error. The same [settings](/reference/enrich-processor/grok-processor.md#grok-watchdog-options) that control the Grok Watchdog timeout also apply to the Redact processor.\n\n\n## Licensing [redact-licensing]\n\nThe `redact` processor is a commercial feature that requires an appropriate license. For more information, refer to [https://www.elastic.co/subscriptions](https://www.elastic.co/subscriptions).\n\nThe `skip_if_unlicensed` option can be set on a redact processor to control behavior when the cluster\u2019s license is not sufficient to run such a processor. `skip_if_unlicensed` defaults to `false`, and the redact processor will throw an exception if the cluster\u2019s license is not sufficient. If you set the `skip_if_unlicensed` option to `true`, however, then the redact processor not throw an exception (it will do nothing at all) in the case of an insufficient license.",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/redact-processor.md"
            }
          ],
          "tool_call_id": "call_KGIlhkwqBsJE3InFYC7lNMge",
          "metadata": null,
          "timestamp": "2025-10-03T06:00:17.683780+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "In the context of Elasticsearch processing functions, ECS (Elastic Common Schema) fields are closely related to OpenTelemetry through a specific processor known as the **Normalize-for-Stream processor**. This processor is designed to facilitate the integration of OpenTelemetry-compliant data into Elasticsearch by performing the following operations:\n\n1. **Detection of OpenTelemetry Compliance**: The processor recognizes whether a document adheres to OpenTelemetry standards by checking for the presence and structure of specific fields such as `resource`, `scope`, and `attributes`.\n\n2. **Normalization**: If a document does not comply with OpenTelemetry, it normalizes the data as follows:\n   - It renames certain ECS fields to align with OpenTelemetry's Semantic Conventions. For example:\n     - `span.id` becomes `span_id`\n     - `trace.id` becomes `trace_id`\n     - `message` is renamed to `body.text`\n     - `log.level` is changed to `severity_text`\n   - Other ECS fields that correspond to OpenTelemetry attributes are relocated into the `resource.attributes` map if they meet specified conditions.\n\n3. **Flattening of Attributes**: Non-array entries in the `attributes` and `resource.attributes` maps are flattened, merging nested objects into their parent objects and concatenating keys with a dot. \n\nThis approach facilitates seamless querying of documents that may have been formatted in either ECS or OpenTelemetry styles, thus enhancing interoperability between these two frameworks.\n\nFor more information, you can check the detailed specifications in the Normalize-for-Stream processor documentation: [Normalize-for-Stream Processor](https://github.com/elastic/elasticsearch/blob/main/elasticsearch-main/docs/reference/enrich-processor/normalize-for-stream.md).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 7051,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 342,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T06:00:18+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMT7ysQjZG0Df2K7DKTORYxTxyWTg",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}