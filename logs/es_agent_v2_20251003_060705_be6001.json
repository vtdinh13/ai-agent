{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How can I override default stopwords in an analyzer setting for a specific language in Elastic Search?",
          "timestamp": "2025-10-03T06:07:03.045847+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"override default stopwords analyzer settings language\"}",
          "tool_call_id": "call_BmMigVpbeKaKlZgGRFSSRVUX",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 220,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 19,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T06:07:03+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMTEV46LwJy8Dm9nerHQdwpFoMl6b",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "Stop",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-analyzer.html"
              ],
              "content": "# Stop analyzer [analysis-stop-analyzer]\n\n\nThe `stop` analyzer is the same as the [`simple` analyzer](/reference/text-analysis/analysis-simple-analyzer.md) but adds support for removing stop words. It defaults to using the `_english_` stop words.\n\n\n## Example output [_example_output_5]\n\n```console\nPOST _analyze\n{\n  \"analyzer\": \"stop\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n```\n\nThe above sentence would produce the following terms:\n\n```text\n[ quick, brown, foxes, jumped, over, lazy, dog, s, bone ]\n```\n\n\n## Configuration [_configuration_6]\n\nThe `stop` analyzer accepts the following parameters:\n\n`stopwords`\n:   A pre-defined stop words list like `_english_` or an array containing a list of stop words. Defaults to `_english_`.\n\n`stopwords_path`\n:   The path to a file containing stop words. This path is relative to the Elasticsearch `config` directory.\n\nSee the [Stop Token Filter](/reference/text-analysis/analysis-stop-tokenfilter.md) for more information about stop word configuration.\n\n\n## Example configuration [_example_configuration_5]\n\nIn this example, we configure the `stop` analyzer to use a specified list of words as stop words:\n\n```console\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_stop_analyzer\": {\n          \"type\": \"stop\",\n          \"stopwords\": [\"the\", \"over\"]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_stop_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n```\n\nThe above example produces the following terms:\n\n```text\n[ quick, brown, foxes, jumped, lazy, dog, s, bone ]\n```\n\n\n## Definition [_definition_5]\n\nIt consists of:\n\nTokenizer\n:   * [Lower Case Tokenizer](/reference/text-analysis/analysis-lowercase-tokenizer.md)\n\n\nToken filters\n:   * [Stop Token Filter](/reference/text-analysis/analysis-stop-tokenfilter.md)\n\n\nIf you need to customize the `stop` analyzer beyond the configuration parameters then you need to recreate it as a `custom` analyzer and modify it, usually by adding token filters. This would recreate the built-in `stop` analyzer and you can use it as a starting point for further customization:\n\n```console\nPUT /stop_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {\n        \"english_stop\": {\n          \"type\":       \"stop\",\n          \"stopwords\":  \"_english_\" <1>\n        }\n      },\n      \"analyzer\": {\n        \"rebuilt_stop\": {\n          \"tokenizer\": \"lowercase\",\n          \"filter\": [\n            \"english_stop\"          <2>\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\n1. The default stopwords can be overridden with the `stopwords` or `stopwords_path` parameters.\n2. You\u2019d add any token filters after `english_stop`.",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analysis-stop-analyzer.md"
            },
            {
              "navigation_title": "Stop",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html"
              ],
              "content": "# Stop token filter [analysis-stop-tokenfilter]\n\n\nRemoves [stop words](https://en.wikipedia.org/wiki/Stop_words) from a token stream.\n\nWhen not customized, the filter removes the following English stop words by default:\n\n`a`, `an`, `and`, `are`, `as`, `at`, `be`, `but`, `by`, `for`, `if`, `in`, `into`, `is`, `it`, `no`, `not`, `of`, `on`, `or`, `such`, `that`, `the`, `their`, `then`, `there`, `these`, `they`, `this`, `to`, `was`, `will`, `with`\n\nIn addition to English, the `stop` filter supports predefined [stop word lists for several languages](#analysis-stop-tokenfilter-stop-words-by-lang). You can also specify your own stop words as an array or file.\n\nThe `stop` filter uses Lucene\u2019s [StopFilter](https://lucene.apache.org/core/10_0_0/analysis/common/org/apache/lucene/analysis/core/StopFilter.md).\n\n## Example [analysis-stop-tokenfilter-analyze-ex]\n\nThe following analyze API request uses the `stop` filter to remove the stop words `a` and `the` from `a quick fox jumps over the lazy dog`:\n\n```console\nGET /_analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [ \"stop\" ],\n  \"text\": \"a quick fox jumps over the lazy dog\"\n}\n```\n\nThe filter produces the following tokens:\n\n```text\n[ quick, fox, jumps, over, lazy, dog ]\n```\n\n\n## Add to an analyzer [analysis-stop-tokenfilter-analyzer-ex]\n\nThe following [create index API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-create) request uses the `stop` filter to configure a new [custom analyzer](docs-content://manage-data/data-store/text-analysis/create-custom-analyzer.md).\n\n```console\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"stop\" ]\n        }\n      }\n    }\n  }\n}\n```\n\n\n## Configurable parameters [analysis-stop-tokenfilter-configure-parms]\n\n`stopwords`\n:   (Optional, string or array of strings) Language value, such as `_arabic_` or `_thai_`. Defaults to [`_english_`](#english-stop-words).\n\nEach language value corresponds to a predefined list of stop words in Lucene. See [Stop words by language](#analysis-stop-tokenfilter-stop-words-by-lang) for supported language values and their stop words.\n\nAlso accepts an array of stop words.\n\nFor an empty list of stop words, use `_none_`.\n\n\n`stopwords_path`\n:   (Optional, string) Path to a file that contains a list of stop words to remove.\n\nThis path must be absolute or relative to the `config` location, and the file must be UTF-8 encoded. Each stop word in the file must be separated by a line break.\n\n\n`ignore_case`\n:   (Optional, Boolean) If `true`, stop word matching is case insensitive. For example, if `true`, a stop word of `the` matches and removes `The`, `THE`, or `the`. Defaults to `false`.\n\n`remove_trailing`\n:   (Optional, Boolean) If `true`, the last token of a stream is removed if it\u2019s a stop word. Defaults to `true`.\n\nThis parameter should be `false` when using the filter with a completion suggester. This would ensure a query like `green a` matches and suggests `green apple` while still removing other stop words. For more information about completion suggesters, refer to [](/reference/elasticsearch/rest-apis/search-suggesters.md)\n\n## Customize [analysis-stop-tokenfilter-customize]\n\nTo customize the `stop` filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters.\n\nFor example, the following request creates a custom case-insensitive `stop` filter that removes stop words from the [`_english_`](#english-stop-words) stop words list:\n\n```console\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"my_custom_stop_words_filter\" ]\n        }\n      },\n      \"filter\": {\n        \"my_custom_stop_words_filter\": {\n          \"type\": \"stop\",\n          \"ignore_case\": true\n        }\n      }\n    }\n  }\n}\n```\n\nYou can also specify your own list of stop words. For example, the following request creates a custom case-insensitive `stop` filter that removes only the stop words `and`, `is`, and `the`:\n\n```console\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"my_custom_stop_words_filter\" ]\n        }\n      },\n      \"filter\": {\n        \"my_custom_stop_words_filter\": {\n          \"type\": \"stop\",\n          \"ignore_case\": true,\n          \"stopwords\": [ \"and\", \"is\", \"the\" ]\n        }\n      }\n    }\n  }\n}\n```\n\n\n## Stop words by language [analysis-stop-tokenfilter-stop-words-by-lang]\n\nThe following list contains supported language values for the `stopwords` parameter and a link to their predefined stop words in Lucene.\n\n$$$arabic-stop-words$$$\n\n`_arabic_`\n:   [Arabic stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ar/stopwords.txt)\n\n$$$armenian-stop-words$$$\n\n`_armenian_`\n:   [Armenian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/hy/stopwords.txt)\n\n$$$basque-stop-words$$$\n\n`_basque_`\n:   [Basque stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/eu/stopwords.txt)\n\n$$$bengali-stop-words$$$\n\n`_bengali_`\n:   [Bengali stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/bn/stopwords.txt)\n\n$$$brazilian-stop-words$$$\n\n`_brazilian_` (Brazilian Portuguese)\n:   [Brazilian Portuguese stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/br/stopwords.txt)\n\n$$$bulgarian-stop-words$$$\n\n`_bulgarian_`\n:   [Bulgarian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/bg/stopwords.txt)\n\n$$$catalan-stop-words$$$\n\n`_catalan_`\n:   [Catalan stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ca/stopwords.txt)\n\n$$$cjk-stop-words$$$\n\n`_cjk_` (Chinese, Japanese, and Korean)\n:   [CJK stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/cjk/stopwords.txt)\n\n$$$czech-stop-words$$$\n\n`_czech_`\n:   [Czech stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/cz/stopwords.txt)\n\n$$$danish-stop-words$$$\n\n`_danish_`\n:   [Danish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/danish_stop.txt)\n\n$$$dutch-stop-words$$$\n\n`_dutch_`\n:   [Dutch stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/dutch_stop.txt)\n\n$$$english-stop-words$$$\n\n`_english_`\n:   [English stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L48)\n\n$$$estonian-stop-words$$$\n\n`_estonian_`\n:   [Estonian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/et/stopwords.txt)\n\n$$$finnish-stop-words$$$\n\n`_finnish_`\n:   [Finnish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/finnish_stop.txt)\n\n$$$french-stop-words$$$\n\n`_french_`\n:   [French stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/french_stop.txt)\n\n$$$galician-stop-words$$$\n\n`_galician_`\n:   [Galician stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/gl/stopwords.txt)\n\n$$$german-stop-words$$$\n\n`_german_`\n:   [German stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/german_stop.txt)\n\n$$$greek-stop-words$$$\n\n`_greek_`\n:   [Greek stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/el/stopwords.txt)\n\n$$$hindi-stop-words$$$\n\n`_hindi_`\n:   [Hindi stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/hi/stopwords.txt)\n\n$$$hungarian-stop-words$$$\n\n`_hungarian_`\n:   [Hungarian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/hungarian_stop.txt)\n\n$$$indonesian-stop-words$$$\n\n`_indonesian_`\n:   [Indonesian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/id/stopwords.txt)\n\n$$$irish-stop-words$$$\n\n`_irish_`\n:   [Irish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ga/stopwords.txt)\n\n$$$italian-stop-words$$$\n\n`_italian_`\n:   [Italian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/italian_stop.txt)\n\n$$$latvian-stop-words$$$\n\n`_latvian_`\n:   [Latvian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/lv/stopwords.txt)\n\n$$$lithuanian-stop-words$$$\n\n`_lithuanian_`\n:   [Lithuanian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/lt/stopwords.txt)\n\n$$$norwegian-stop-words$$$\n\n`_norwegian_`\n:   [Norwegian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/norwegian_stop.txt)\n\n$$$persian-stop-words$$$\n\n`_persian_`\n:   [Persian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/fa/stopwords.txt)\n\n$$$portuguese-stop-words$$$\n\n`_portuguese_`\n:   [Portuguese stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/portuguese_stop.txt)\n\n$$$romanian-stop-words$$$\n\n`_romanian_`\n:   [Romanian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ro/stopwords.txt)\n\n$$$russian-stop-words$$$\n\n`_russian_`\n:   [Russian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/russian_stop.txt)\n\n$$$serbian-stop-words$$$\n\n`_serbian_`\n:   [Serbian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/sr/stopwords.txt)\n\n$$$sorani-stop-words$$$\n\n`_sorani_`\n:   [Sorani stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ckb/stopwords.txt)\n\n$$$spanish-stop-words$$$\n\n`_spanish_`\n:   [Spanish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/spanish_stop.txt)\n\n$$$swedish-stop-words$$$\n\n`_swedish_`\n:   [Swedish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/swedish_stop.txt)\n\n$$$thai-stop-words$$$\n\n`_thai_`\n:   [Thai stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/th/stopwords.txt)\n\n$$$turkish-stop-words$$$\n\n`_turkish_`\n:   [Turkish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/tr/stopwords.txt)",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analysis-stop-tokenfilter.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-polish-stop.html"
              ],
              "content": "# polish_stop token filter [analysis-polish-stop]\n\nThe `polish_stop` token filter filters out Polish stopwords (`_polish_`), and any other custom stopwords specified by the user. This filter only supports the predefined `_polish_` stopwords list. If you want to use a different predefined list, then use the [`stop` token filter](/reference/text-analysis/analysis-stop-tokenfilter.md) instead.\n\n```console\nPUT /polish_stop_example\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"analyzer_with_stop\": {\n            \"tokenizer\": \"standard\",\n            \"filter\": [\n              \"lowercase\",\n              \"polish_stop\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"polish_stop\": {\n            \"type\": \"polish_stop\",\n            \"stopwords\": [\n              \"_polish_\",\n              \"je\u015b\u0107\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET polish_stop_example/_analyze\n{\n  \"analyzer\": \"analyzer_with_stop\",\n  \"text\": \"Gdzie kucharek sze\u015b\u0107, tam nie ma co je\u015b\u0107.\"\n}\n```\n\nThe above request returns:\n\n```console-result\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"kucharek\",\n      \"start_offset\" : 6,\n      \"end_offset\" : 14,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"sze\u015b\u0107\",\n      \"start_offset\" : 15,\n      \"end_offset\" : 20,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 2\n    }\n  ]\n}\n```",
              "filename": "elasticsearch-main/docs/reference/elasticsearch-plugins/analysis-polish-stop.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-kuromoji-stop.html"
              ],
              "content": "# ja_stop token filter [analysis-kuromoji-stop]\n\nThe `ja_stop` token filter filters out Japanese stopwords (`_japanese_`), and any other custom stopwords specified by the user. This filter only supports the predefined `_japanese_` stopwords list. If you want to use a different predefined list, then use the [`stop` token filter](/reference/text-analysis/analysis-stop-tokenfilter.md) instead.\n\n```console\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"analyzer_with_ja_stop\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"ja_stop\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"ja_stop\": {\n            \"type\": \"ja_stop\",\n            \"stopwords\": [\n              \"_japanese_\",\n              \"\u30b9\u30c8\u30c3\u30d7\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"analyzer_with_ja_stop\",\n  \"text\": \"\u30b9\u30c8\u30c3\u30d7\u306f\u6d88\u3048\u308b\"\n}\n```\n\nThe above request returns:\n\n```console-result\n{\n  \"tokens\" : [ {\n    \"token\" : \"\u6d88\u3048\u308b\",\n    \"start_offset\" : 5,\n    \"end_offset\" : 8,\n    \"type\" : \"word\",\n    \"position\" : 2\n  } ]\n}\n```",
              "filename": "elasticsearch-main/docs/reference/elasticsearch-plugins/analysis-kuromoji-stop.md"
            },
            {
              "navigation_title": "Standard",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html"
              ],
              "content": "# Standard analyzer [analysis-standard-analyzer]\n\n\nThe `standard` analyzer is the default analyzer which is used if none is specified. It provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in [Unicode Standard Annex #29](https://unicode.org/reports/tr29/)) and works well for most languages.\n\n\n## Example output [_example_output_4]\n\n```console\nPOST _analyze\n{\n  \"analyzer\": \"standard\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n```\n\nThe above sentence would produce the following terms:\n\n```text\n[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone ]\n```\n\n\n## Configuration [_configuration_5]\n\nThe `standard` analyzer accepts the following parameters:\n\n`max_token_length`\n:   The maximum token length. If a token is seen that exceeds this length then it is split at `max_token_length` intervals. Defaults to `255`.\n\n`stopwords`\n:   A pre-defined stop words list like `_english_` or an array containing a list of stop words. Defaults to `_none_`.\n\n`stopwords_path`\n:   The path to a file containing stop words.\n\nSee the [Stop Token Filter](/reference/text-analysis/analysis-stop-tokenfilter.md) for more information about stop word configuration.\n\n\n## Example configuration [_example_configuration_4]\n\nIn this example, we configure the `standard` analyzer to have a `max_token_length` of 5 (for demonstration purposes), and to use the pre-defined list of English stop words:\n\n```console\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_english_analyzer\": {\n          \"type\": \"standard\",\n          \"max_token_length\": 5,\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_english_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n```\n\nThe above example produces the following terms:\n\n```text\n[ 2, quick, brown, foxes, jumpe, d, over, lazy, dog's, bone ]\n```\n\n\n## Definition [_definition_4]\n\nThe `standard` analyzer consists of:\n\nTokenizer\n:   * [Standard Tokenizer](/reference/text-analysis/analysis-standard-tokenizer.md)\n\n\nToken Filters\n:   * [Lower Case Token Filter](/reference/text-analysis/analysis-lowercase-tokenfilter.md)\n* [Stop Token Filter](/reference/text-analysis/analysis-stop-tokenfilter.md) (disabled by default)\n\n\nIf you need to customize the `standard` analyzer beyond the configuration parameters then you need to recreate it as a `custom` analyzer and modify it, usually by adding token filters. This would recreate the built-in `standard` analyzer and you can use it as a starting point:\n\n```console\nPUT /standard_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_standard\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\"       <1>\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\n1. You\u2019d add any token filters after `lowercase`.",
              "filename": "elasticsearch-main/docs/reference/text-analysis/analysis-standard-analyzer.md"
            }
          ],
          "tool_call_id": "call_BmMigVpbeKaKlZgGRFSSRVUX",
          "metadata": null,
          "timestamp": "2025-10-03T06:07:05.387748+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "To override the default stopwords in an analyzer setting for a specific language in Elasticsearch, you can create a custom analyzer and modify the `stopwords` parameter. Here's a brief guide on how to do this:\n\n1. **Use the Stop Filter**: You can use the `stop` filter which allows you to specify stop words either as an array or as a predefined list for different languages (e.g., `_english_`, `_german_`, etc.).\n\n2. **Create a Custom Analyzer**:\n   - You can define a custom analyzer in your index settings.\n   - Within the custom analyzer, you can specify your own stop words or override the default stop words list.\n\n### Example Configuration\n\nHere\u2019s how to configure a custom analyzer to override the default stop words. For instance, using English stop words:\n\n```json\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": {\n          \"type\": \"stop\",\n          \"stopwords\": [\"the\", \"over\"]  // Your custom list of stop words\n        }\n      }\n    }\n  }\n}\n```\n\n### Analyzing Text\n\nOnce you have defined your custom analyzer, you can analyze text with it:\n\n```json\nPOST /my-index-000001/_analyze\n{\n  \"analyzer\": \"my_custom_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n```\n\n### Notes\n\n- The `stop` analyzer defaults to the `_english_` stop words if not customized.\n- You can also specify a list of stop words via `stopwords_path` if you want to manage them via a file.\n- For languages other than English, ensure to use the appropriate predefined stop word list (e.g., `_german_`, `_french_`, etc.).\n\nIn case you need to customize it further or if you want to know about stop words for other languages, you can refer to the [Stop Analyzer Documentation](https://github.com/elastic/elasticsearch/blob/main/docs/reference/text-analysis/analysis-stop-analyzer.md) and [Stop Token Filter Documentation](https://github.com/elastic/elasticsearch/blob/main/docs/reference/text-analysis/analysis-stop-tokenfilter.md).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 6293,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 469,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T06:07:05+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMTEXdEc6sUfCSlHmZYnoC7zMZxK1",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}