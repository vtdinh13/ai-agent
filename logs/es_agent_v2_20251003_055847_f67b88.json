{
  "agent_name": "es_agent_v2",
  "system_prompt": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "What is the purpose of the `if` parameter in the context of the ingest-circle-processor?",
          "timestamp": "2025-10-03T05:58:45.467240+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\":\"if parameter ingest-circle-processor\"}",
          "tool_call_id": "call_I0LFdmX5Q3TNZ4vhBoKibefQ",
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 221,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 18,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T05:58:46+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CMT6U1xfLffGmFGfnVjcV882CMqUC",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "navigation_title": "Circle",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-circle-processor.html"
              ],
              "content": "# Circle processor [ingest-circle-processor]\n\n\nConverts circle definitions of shapes to regular polygons which approximate them.\n\n$$$circle-processor-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `field` | yes | - | The field to interpret as a circle. Either a string in WKT format or a map for GeoJSON. |\n| `target_field` | no | `field` | The field to assign the polygon shape to, by default `field` is updated in-place |\n| `ignore_missing` | no | `false` | If `true` and `field` does not exist, the processor quietly exits without modifying the document |\n| `error_distance` | yes | - | The difference between the resulting inscribed distance from center to side and the circle\u2019s radius (measured in meters for `geo_shape`, unit-less for `shape`) |\n| `shape_type` | yes | - | Which field mapping type is to be used when processing the circle: `geo_shape` or `shape` |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n![error distance](images/error_distance.png \"\")\n\n```console\nPUT circles\n{\n  \"mappings\": {\n    \"properties\": {\n      \"circle\": {\n        \"type\": \"geo_shape\"\n      }\n    }\n  }\n}\n\nPUT _ingest/pipeline/polygonize_circles\n{\n  \"description\": \"translate circle to polygon\",\n  \"processors\": [\n    {\n      \"circle\": {\n        \"field\": \"circle\",\n        \"error_distance\": 28.0,\n        \"shape_type\": \"geo_shape\"\n      }\n    }\n  ]\n}\n```\n\nUsing the above pipeline, we can attempt to index a document into the `circles` index. The circle can be represented as either a WKT circle or a GeoJSON circle. The resulting polygon will be represented and indexed using the same format as the input circle. WKT will be translated to a WKT polygon, and GeoJSON circles will be translated to GeoJSON polygons.\n\n::::{important}\nCircles that contain a pole are not supported.\n::::\n\n\n## Example: Circle defined in Well Known Text [_example_circle_defined_in_well_known_text]\n\nIn this example a circle defined in WKT format is indexed\n\n```console\nPUT circles/_doc/1?pipeline=polygonize_circles\n{\n  \"circle\": \"CIRCLE (30 10 40)\"\n}\n\nGET circles/_doc/1\n```\n\nThe response from the above index request:\n\n```console-result\n{\n  \"found\": true,\n  \"_index\": \"circles\",\n  \"_id\": \"1\",\n  \"_version\": 1,\n  \"_seq_no\": 22,\n  \"_primary_term\": 1,\n  \"_source\": {\n    \"circle\": \"POLYGON ((30.000365257263184 10.0, 30.000111397193788 10.00034284530941, 29.999706043744222 10.000213571721195, 29.999706043744222 9.999786428278805, 30.000111397193788 9.99965715469059, 30.000365257263184 10.0))\"\n  }\n}\n```\n\n\n## Example: Circle defined in GeoJSON [_example_circle_defined_in_geojson]\n\nIn this example a circle defined in GeoJSON format is indexed\n\n```console\nPUT circles/_doc/2?pipeline=polygonize_circles\n{\n  \"circle\": {\n    \"type\": \"circle\",\n    \"radius\": \"40m\",\n    \"coordinates\": [30, 10]\n  }\n}\n\nGET circles/_doc/2\n```\n\nThe response from the above index request:\n\n```console-result\n{\n  \"found\": true,\n  \"_index\": \"circles\",\n  \"_id\": \"2\",\n  \"_version\": 1,\n  \"_seq_no\": 22,\n  \"_primary_term\": 1,\n  \"_source\": {\n    \"circle\": {\n      \"coordinates\": [\n        [\n          [30.000365257263184, 10.0],\n          [30.000111397193788, 10.00034284530941],\n          [29.999706043744222, 10.000213571721195],\n          [29.999706043744222, 9.999786428278805],\n          [30.000111397193788, 9.99965715469059],\n          [30.000365257263184, 10.0]\n        ]\n      ],\n      \"type\": \"Polygon\"\n    }\n  }\n}\n```\n\n\n## Notes on Accuracy [circle-processor-notes]\n\nAccuracy of the polygon that represents the circle is defined as `error_distance`. The smaller this difference is, the closer to a perfect circle the polygon is.\n\nBelow is a table that aims to help capture how the radius of the circle affects the resulting number of sides of the polygon given different inputs.\n\nThe minimum number of sides is `4` and the maximum is `1000`.\n\n$$$circle-processor-accuracy$$$\n\n| error_distance | radius in meters | number of sides of polygon |\n| --- | --- | --- |\n| 1.00 | 1.0 | 4 |\n| 1.00 | 10.0 | 14 |\n| 1.00 | 100.0 | 45 |\n| 1.00 | 1000.0 | 141 |\n| 1.00 | 10000.0 | 445 |\n| 1.00 | 100000.0 | 1000 |",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/ingest-circle-processor.md"
            },
            {
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-ingest.html"
              ],
              "products": [
                {
                  "id": "painless"
                }
              ],
              "content": "# Using ingest processors in Painless [painless-ingest]\n\nSome [ingest processors](/reference/enrich-processor/index.md) expose behavior through Painless methods that can be called in Painless scripts that execute in ingest pipelines.\n\n## Method usage [_method_usage]\n\nAll ingest methods available in Painless are scoped to the `Processors` namespace. For example:\n\n```console\nPOST /_ingest/pipeline/_simulate?verbose\n{\n  \"pipeline\": {\n    \"processors\": [\n      {\n        \"script\": {\n          \"lang\": \"painless\",\n          \"source\": \"\"\"\n            long bytes = Processors.bytes(ctx.size);\n            ctx.size_in_bytes = bytes;\n          \"\"\"\n        }\n      }\n    ]\n  },\n  \"docs\": [\n    {\n      \"_source\": {\n        \"size\": \"1kb\"\n      }\n    }\n  ]\n}\n```\n\n\n## Ingest methods reference [_ingest_methods_reference]\n\n### Byte conversion [_byte_conversion]\n\nUse the [bytes processor](/reference/enrich-processor/bytes-processor.md) to return the number of bytes in the human-readable byte value supplied in the `value` parameter.\n\n```painless\nlong bytes(String value);\n```\n\n\n### Lowercase conversion [_lowercase_conversion]\n\nUse the [lowercase processor](/reference/enrich-processor/lowercase-processor.md) to convert the supplied string in the `value` parameter to its lowercase equivalent.\n\n```painless\nString lowercase(String value);\n```\n\n\n### Uppercase conversion [_uppercase_conversion]\n\nUse the [uppercase processor](/reference/enrich-processor/uppercase-processor.md) to convert the supplied string in the `value` parameter to its uppercase equivalent.\n\n```painless\nString uppercase(String value);\n```\n\n\n### JSON parsing [_json_parsing]\n\nUse the [JSON processor](/reference/enrich-processor/json-processor.md) to convert JSON strings to structured JSON objects. The first `json` method accepts a map and a key. The processor converts the JSON string in the map as specified by the `key` parameter to structured JSON content. That content is added directly to the `map` object.\n\nThe second `json` method accepts a JSON string in the `value` parameter and returns a structured JSON object.\n\n```painless\nvoid json(Map<String, Object> map, String key);\nObject json(Object value);\n```\n\nYou can then add this object to the document through the context object:\n\n```painless\nObject json = Processors.json(ctx.inputJsonString);\nctx.structuredJson = json;\n```\n\n\n### URL decoding [_url_decoding]\n\nUse the [URL decode processor](/reference/enrich-processor/urldecode-processor.md) to URL-decode the string supplied in the `value` parameter.\n\n```painless\nString urlDecode(String value);\n```\n\n\n### URI decomposition [_uri_decomposition]\n\nUse the [URI parts processor](/reference/enrich-processor/uri-parts-processor.md) to decompose the URI string supplied in the `value` parameter. Returns a map of key-value pairs in which the key is the name of the URI component such as `domain` or `path` and the value is the corresponding value for that component.\n\n```painless\nString uriParts(String value);\n```\n\n\n### Network community ID [_network_community_id]\n\nUse the [community ID processor](/reference/enrich-processor/community-id-processor.md) to compute the network community ID for network flow data.\n\n```painless\nString communityId(String sourceIpAddrString, String destIpAddrString, Object ianaNumber, Object transport, Object sourcePort, Object destinationPort, Object icmpType, Object icmpCode, int seed)\nString communityId(String sourceIpAddrString, String destIpAddrString, Object ianaNumber, Object transport, Object sourcePort, Object destinationPort, Object icmpType, Object icmpCode)\n```",
              "filename": "elasticsearch-main/docs/reference/scripting-languages/painless/using-ingest-processors-in-painless.md"
            },
            {
              "navigation_title": "Terminate",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/terminate-processor.html"
              ],
              "content": "# Terminate processor [terminate-processor]\n\n\nTerminates the current ingest pipeline, causing no further processors to be run. This will normally be executed conditionally, using the `if` option.\n\nIf this pipeline is being called from another pipeline, the calling pipeline is **not** terminated.\n\n$$$terminate-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n```js\n{\n  \"description\" : \"terminates the current pipeline if the error field is present\",\n  \"terminate\": {\n    \"if\": \"ctx.error != null\"\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/terminate-processor.md"
            },
            {
              "navigation_title": "{{infer-cap}}",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-processor.html"
              ],
              "content": "# {{infer-cap}} processor [inference-processor]\n\nUses a pre-trained {{dfanalytics}} model or a model deployed for natural language processing tasks to infer against the data that is being ingested in the pipeline.\n\n$$$inference-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `model_id`  | yes | - | (String) An inference ID, a model deployment ID, a trained model ID or an alias. |\n| `input_output` | no | - | (List) Input fields for {{infer}} and output (destination) fields for the {{infer}} results. This option is incompatible with the `target_field` and `field_map` options. |\n| `target_field` | no | `ml.inference.<processor_tag>` | (String) Field added to incoming documents to contain results objects. |\n| `field_map` | no | If defined the model\u2019s default field map | (Object) Maps the document field names to the known field names of the model. This mapping takes precedence over any default mappings provided in the model configuration. |\n| `inference_config` | no | The default settings defined in the model | (Object) Contains the inference type and its options. |\n| `ignore_missing` | no | `false` | (Boolean) If `true` and any of the input fields defined in `input_ouput` are missing then those missing fields are quietly ignored, otherwise a missing field causes a failure. Only applies when using `input_output` configurations to explicitly list the input fields. |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n::::{important}\n\n* You cannot use the `input_output` field with the `target_field` and `field_map` fields. For NLP models, use the `input_output` option. For {{dfanalytics}} models, use the `target_field` and `field_map` option.\n* Each {{infer}} input field must be single strings, not arrays of strings.\n* The `input_field` is processed as is and ignores any [index mapping](docs-content://manage-data/data-store/mapping.md)'s [analyzers](docs-content://manage-data/data-store/text-analysis.md) at time of {{infer}} run.\n\n::::\n\n## Configuring input and output fields [inference-input-output-example]\n\nSelect the `content` field for inference and write the result to `content_embedding`.\n\n::::{important}\nIf the specified `output_field` already exists in the ingest document, it won\u2019t be overwritten. The {{infer}} results will be appended to the existing fields within `output_field`, which could lead to duplicate fields and potential errors. To avoid this, use an unique `output_field` field name that does not clash with any existing fields.\n::::\n\n\n```js\n{\n  \"inference\": {\n    \"model_id\": \"model_deployment_for_inference\",\n    \"input_output\": [\n        {\n            \"input_field\": \"content\",\n            \"output_field\": \"content_embedding\"\n        }\n    ]\n  }\n}\n```\n\n## Configuring multiple inputs [_configuring_multiple_inputs]\n\nThe `content` and `title` fields will be read from the incoming document and sent to the model for the inference. The inference output is written to `content_embedding` and `title_embedding` respectively.\n\n```js\n{\n  \"inference\": {\n    \"model_id\": \"model_deployment_for_inference\",\n    \"input_output\": [\n        {\n            \"input_field\": \"content\",\n            \"output_field\": \"content_embedding\"\n        },\n        {\n            \"input_field\": \"title\",\n            \"output_field\": \"title_embedding\"\n        }\n    ]\n  }\n}\n```\n\nSelecting the input fields with `input_output` is incompatible with the `target_field` and `field_map` options.\n\n{{dfanalytics-cap}} models must use the `target_field` to specify the root location results are written to and optionally a `field_map` to map field names in the input document to the model input fields.\n\n```js\n{\n  \"inference\": {\n    \"model_id\": \"model_deployment_for_inference\",\n    \"target_field\": \"FlightDelayMin_prediction_infer\",\n    \"field_map\": {\n      \"your_field\": \"my_field\"\n    },\n    \"inference_config\": { \"regression\": {} }\n  }\n}\n```\n\n## {{classification-cap}} configuration options [inference-processor-classification-opt]\n\nClassification configuration for inference.\n\n`num_top_classes`\n:   (Optional, integer) Specifies the number of top class predictions to return. Defaults to 0.\n\n`num_top_feature_importance_values`\n:   (Optional, integer) Specifies the maximum number of [{{feat-imp}}](docs-content://explore-analyze/machine-learning/data-frame-analytics/ml-feature-importance.md) values per document. Defaults to 0 which means no {{feat-imp}} calculation occurs.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`top_classes_results_field`\n:   (Optional, string) Specifies the field to which the top classes are written. Defaults to `top_classes`.\n\n`prediction_field_type`\n:   (Optional, string) Specifies the type of the predicted field to write. Valid values are: `string`, `number`, `boolean`. When `boolean` is provided `1.0` is transformed to `true` and `0.0` to `false`.\n\n## Fill mask configuration options [inference-processor-fill-mask-opt]\n\n`num_top_classes`\n:   (Optional, integer) Specifies the number of top class predictions to return. Defaults to 0.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    ::::{dropdown} Properties of deberta_v2\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## NER configuration options [inference-processor-ner-opt]\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    ::::{dropdown} Properties of deberta_v2\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## {{regression-cap}} configuration options [inference-processor-regression-opt]\n\nRegression configuration for inference.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`num_top_feature_importance_values`\n:   (Optional, integer) Specifies the maximum number of [{{feat-imp}}](docs-content://explore-analyze/machine-learning/data-frame-analytics/ml-feature-importance.md) values per document. By default, it is zero and no {{feat-imp}} calculation occurs.\n\n## Text classification configuration options [inference-processor-text-classification-opt]\n\n`classification_labels`\n:   (Optional, string) An array of classification labels.\n\n`num_top_classes`\n:   (Optional, integer) Specifies the number of top class predictions to return. Defaults to 0.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of deberta_v2\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    :::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## Text embedding configuration options [inference-processor-text-embedding-opt]\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    ::::{dropdown} Properties of deberta_v2\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## Text expansion configuration options [inference-processor-text-expansion-opt]\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n::::{dropdown} Properties of tokenization\n`bert`\n:   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of bert\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`deberta_v2`\n:   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of deberta_v2\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    :::::\n`roberta`\n:   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of roberta\n    `span`\n    (Optional, integer) When `truncate` is `none`, you can partition longer text sequences for inference. The value indicates how many tokens overlap between each subsequence.\n        The default value is `-1`, indicating no windowing or spanning occurs.\n    ::::{note}\n    When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate; there will be very little information in the second subsequence.\n    ::::\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n`mpnet`\n:   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n    :::::{dropdown} Properties of mpnet\n    `truncate`\n    (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n    * `none`: No truncation occurs; the inference request receives an error.\n    * `first`: Only the first sequence is truncated.\n    * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n    ::::{note}\n    For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n    ::::\n    :::::\n::::\n\n## Text similarity configuration options [inference-processor-text-similarity-opt]\n\n`text_similarity`\n:   (Object, optional) Text similarity takes an input sequence and compares it with another input sequence. This is commonly referred to as cross-encoding. This task is useful for ranking document text when comparing it to another provided text input.\n\n::::{dropdown} Properties of text_similarity inference\n`span_score_combination_function`\n:   (Optional, string) Identifies how to combine the resulting similarity score when a provided text passage is longer than `max_sequence_length` and must be automatically separated for multiple calls. This only is applicable when `truncate` is `none` and `span` is a non-negative number. The default value is `max`. Available options are:\n* `max`: The maximum score from all the spans is returned.\n* `mean`: The mean score over all the spans is returned.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n* `bert`: Use for BERT-style models\n* `deberta_v2`: Use for DeBERTa v2 and v3-style models\n* `mpnet`: Use for MPNet-style models\n* `roberta`: Use for RoBERTa-style and BART-style models\n* {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n* {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\nRefer to [Properties of `tokenizaton`](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-ml-put-trained-model) to review the properties of the `tokenization` object.\n::::\n\n## Zero shot classification configuration options [inference-processor-zero-shot-opt]\n\n`labels`\n:   (Optional, array) The labels to classify. Can be set at creation for default labels, and then updated during inference.\n\n`multi_label`\n:   (Optional, boolean) Indicates if more than one `true` label is possible given the input. This is useful when labeling text that could pertain to more than one of the input labels. Defaults to `false`.\n\n`results_field`\n:   (Optional, string) The field that is added to incoming documents to contain the inference prediction. Defaults to the `results_field` value of the {{dfanalytics-job}} that was used to train the model, which defaults to `<dependent_variable>_prediction`.\n\n`tokenization`\n:   (Optional, object) Indicates the tokenization to perform and the desired settings. The default tokenization configuration is `bert`. Valid tokenization values are\n\n    * `bert`: Use for BERT-style models\n    * `deberta_v2`: Use for DeBERTa v2 and v3-style models\n    * `mpnet`: Use for MPNet-style models\n    * `roberta`: Use for RoBERTa-style and BART-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `xlm_roberta`: Use for XLMRoBERTa-style models\n    * {applies_to}`stack: preview` {applies_to}`serverless: preview` `bert_ja`: Use for BERT-style models trained for the Japanese language.\n\n    ::::{dropdown} Properties of tokenization\n    `bert`\n    :   (Optional, object) BERT-style tokenization is to be performed with the enclosed settings.\n        :::::{dropdown} Properties of bert\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::{note}\n        For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n        ::::\n        :::::\n    `deberta_v2`\n    :   (Optional, object) DeBERTa-style tokenization is to be performed with the enclosed settings.\n        ::::{dropdown} Properties of deberta_v2\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::\n    `roberta`\n    :   (Optional, object) RoBERTa-style tokenization is to be performed with the enclosed settings.\n        :::::{dropdown} Properties of roberta\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::{note}\n        For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n        ::::\n        :::::\n    `mpnet`\n    :   (Optional, object) MPNet-style tokenization is to be performed with the enclosed settings.\n        :::::{dropdown} Properties of mpnet\n        `truncate`\n        (Optional, string) Indicates how tokens are truncated when they exceed `max_sequence_length`. The default value is `first`.\n        * `none`: No truncation occurs; the inference request receives an error.\n        * `first`: Only the first sequence is truncated.\n        * `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\n        ::::{note}\n        For `zero_shot_classification`, the hypothesis sequence is always the second sequence. Therefore, do not use `second` in this case.\n        ::::\n        :::::\n    ::::\n\n## {{infer-cap}} processor examples [inference-processor-config-example]\n\n```js\n\"inference\":{\n  \"model_id\": \"my_model_id\",\n  \"field_map\": {\n    \"original_fieldname\": \"expected_fieldname\"\n  },\n  \"inference_config\": {\n    \"regression\": {\n      \"results_field\": \"my_regression\"\n    }\n  }\n}\n```\n\nThis configuration specifies a `regression` inference and the results are written to the `my_regression` field contained in the `target_field` results object. The `field_map` configuration maps the field `original_fieldname` from the source document to the field expected by the model.\n\n```js\n\"inference\":{\n  \"model_id\":\"my_model_id\"\n  \"inference_config\": {\n    \"classification\": {\n      \"num_top_classes\": 2,\n      \"results_field\": \"prediction\",\n      \"top_classes_results_field\": \"probabilities\"\n    }\n  }\n}\n```\n\nThis configuration specifies a `classification` inference. The number of categories for which the predicted probabilities are reported is 2 (`num_top_classes`). The result is written to the `prediction` field and the top classes to the `probabilities` field. Both fields are contained in the `target_field` results object.\n\nFor an example that uses {{nlp}} trained models, refer to [Add NLP inference to ingest pipelines](docs-content://explore-analyze/machine-learning/nlp/ml-nlp-inference.md).\n\n\n### {{feat-imp-cap}} object mapping [inference-processor-feature-importance]\n\nTo get the full benefit of aggregating and searching for [{{feat-imp}}](docs-content://explore-analyze/machine-learning/data-frame-analytics/ml-feature-importance.md), update your index mapping of the {{feat-imp}} result field as you can see below:\n\n```js\n\"ml.inference.feature_importance\": {\n  \"type\": \"nested\",\n  \"dynamic\": true,\n  \"properties\": {\n    \"feature_name\": {\n      \"type\": \"keyword\"\n    },\n    \"importance\": {\n      \"type\": \"double\"\n    }\n  }\n}\n```\n\nThe mapping field name for {{feat-imp}} (in the example above, it is `ml.inference.feature_importance`) is compounded as follows:\n\n`<ml.inference.target_field>`.`<inference.tag>`.`feature_importance`\n\n* `<ml.inference.target_field>`: defaults to `ml.inference`.\n* `<inference.tag>`: if is not provided in the processor definition, then it is not part of the field path.\n\nFor example, if you provide a tag `foo` in the definition as you can see below:\n\n```js\n{\n  \"tag\": \"foo\",\n  ...\n}\n```\n\nThen, the {{feat-imp}} value is written to the `ml.inference.foo.feature_importance` field.\n\nYou can also specify the target field as follows:\n\n```js\n{\n  \"tag\": \"foo\",\n  \"target_field\": \"my_field\"\n}\n```\n\nIn this case, {{feat-imp}} is exposed in the `my_field.foo.feature_importance` field.\n\n\n### {{infer-cap}} processor examples [inference-processor-examples]\n\nThe following example uses an [{{infer}} endpoint](https://www.elastic.co/docs/api/doc/elasticsearch/group/endpoint-inference) in an {{infer}} processor named `query_helper_pipeline` to perform a chat completion task. The processor generates an {{es}} query from natural language input using a prompt designed for a completion task type. Refer to [this list](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put) for the {{infer}} service you use and check the corresponding examples of setting up an endpoint with the chat completion task type.\n\n```console\nPUT _ingest/pipeline/query_helper_pipeline\n{\n  \"processors\": [\n    {\n      \"script\": {\n        \"source\": \"ctx.prompt = 'Please generate an elasticsearch search query on index `articles_index` for the following natural language query. Dates are in the field `@timestamp`, document types are in the field `type` (options are `news`, `publication`), categories in the field `category` and can be multiple (options are `medicine`, `pharmaceuticals`, `technology`), and document names are in the field `title` which should use a fuzzy match. Ignore fields which cannot be determined from the natural language query context: ' + ctx.content\" <1>\n      }\n    },\n    {\n      \"inference\": {\n        \"model_id\": \"openai_chat_completions\", <2>\n        \"input_output\": {\n          \"input_field\": \"prompt\",\n          \"output_field\": \"query\"\n        }\n      }\n    },\n    {\n      \"remove\": {\n        \"field\": \"prompt\"\n      }\n    }\n  ]\n}\n```\n\n1. The `prompt` field contains the prompt used for the completion task, created with [Painless](docs-content://explore-analyze/scripting/modules-scripting-painless.md). `+ ctx.content` appends the natural language input to the prompt.\n2. The ID of the pre-configured {{infer}} endpoint, which utilizes the [`openai` service](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put) with the `completion` task type.\n\n\nThe following API request will simulate running a document through the ingest pipeline created previously:\n\n```console\nPOST _ingest/pipeline/query_helper_pipeline/_simulate\n{\n  \"docs\": [\n    {\n      \"_source\": {\n        \"content\": \"artificial intelligence in medicine articles published in the last 12 months\" <1>\n      }\n    }\n  ]\n}\n```\n\n1. The natural language query used to generate an {{es}} query within the prompt created by the {{infer}} processor.\n\n\n\n### Further readings [infer-proc-readings]\n\n* [Which job is the best for you? Using LLMs and semantic_text to match resumes to jobs](https://www.elastic.co/search-labs/blog/openwebcrawler-llms-semantic-text-resume-job-search)",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/inference-processor.md"
            },
            {
              "navigation_title": "Fail",
              "mapped_pages": [
                "https://www.elastic.co/guide/en/elasticsearch/reference/current/fail-processor.html"
              ],
              "content": "# Fail processor [fail-processor]\n\n\nRaises an exception. This is useful for when you expect a pipeline to fail and want to relay a specific message to the requester.\n\n$$$fail-options$$$\n\n| Name | Required | Default | Description |\n| --- | --- | --- | --- |\n| `message` | yes | - | The error message thrown by the processor. Supports [template snippets](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#template-snippets). |\n| `description` | no | - | Description of the processor. Useful for describing the purpose of the processor or its configuration. |\n| `if` | no | - | Conditionally execute the processor. See [Conditionally run a processor](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#conditionally-run-processor). |\n| `ignore_failure` | no | `false` | Ignore failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `on_failure` | no | - | Handle failures for the processor. See [Handling pipeline failures](docs-content://manage-data/ingest/transform-enrich/ingest-pipelines.md#handling-pipeline-failures). |\n| `tag` | no | - | Identifier for the processor. Useful for debugging and metrics. |\n\n```js\n{\n  \"fail\": {\n    \"if\" : \"ctx.tags.contains('production') != true\",\n    \"message\": \"The production tag is not present, found tags: {{{tags}}}\"\n  }\n}\n```",
              "filename": "elasticsearch-main/docs/reference/enrich-processor/fail-processor.md"
            }
          ],
          "tool_call_id": "call_I0LFdmX5Q3TNZ4vhBoKibefQ",
          "metadata": null,
          "timestamp": "2025-10-03T05:58:47.573079+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a helpful assistant that answers questions about documentation.  \n\nUse the search tool to find relevant information from the repository before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.\nReplace it with the full path to the GitHub repository:\n\"https://github.com/elastic/elasticsearch/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "content": "The `if` parameter in the context of the `ingest-circle-processor` is used to conditionally execute the processor based on the evaluation of a specified condition. If the condition evaluates to true, the processor will run; if it evaluates to false, the processor will be skipped, allowing for more dynamic and adaptable ingest pipelines. \n\nThis functionality can be particularly useful for scenarios where certain processing should only occur under specific circumstances, helping to control the flow of data processing in the pipeline.\n\nFor more details, you can refer to the documentation on the [ingest-circle-processor](https://github.com/elastic/elasticsearch/blob/main/docs/reference/enrich-processor/ingest-circle-processor.md).",
          "id": null,
          "part_kind": "text"
        }
      ],
      "usage": {
        "input_tokens": 13624,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 142,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-10-03T05:58:47+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "stop"
      },
      "provider_response_id": "chatcmpl-CMT6V0cGf6QyicETxbNKxJp2EOJAJ",
      "finish_reason": "stop"
    }
  ],
  "source": "ai-generated"
}